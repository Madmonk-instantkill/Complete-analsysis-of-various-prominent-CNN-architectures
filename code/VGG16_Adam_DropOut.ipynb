{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "id": "Pr0jCnvKRqA7"
   },
   "outputs": [],
   "source": [
    "from keras.callbacks import ModelCheckpoint, EarlyStopping\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Conv2D, MaxPool2D, Flatten, Dense,BatchNormalization,Activation,Dropout,LeakyReLU\n",
    "from tensorflow.keras.optimizers import SGD,Adam\n",
    "from keras.preprocessing.image import ImageDataGenerator\n",
    "from keras.utils import to_categorical\n",
    "from tensorflow.keras.datasets import cifar100\n",
    "\n",
    "\n",
    "\n",
    "(x_train, Y_train), (x_test, Y_test) = cifar100.load_data()\n",
    "#x_train = x_train.astype('float32') / 255\n",
    "#x_test = x_test.astype('float32') / 255\n",
    "\n",
    "y_train = to_categorical(Y_train)\n",
    "y_test = to_categorical(Y_test)\n",
    "\n",
    "aug_data=ImageDataGenerator(rotation_range=20,horizontal_flip=True,width_shift_range=0.1,shear_range = 0.2,height_shift_range=0.1,zoom_range=0.2,brightness_range = (0.5, 1.5))\n",
    "aug_data.fit(x_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "id": "OTROfWF9S82z",
    "outputId": "9ebeac07-e01b-4b2e-e792-0cdfd5faf81b"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:`period` argument is deprecated. Please use `save_freq` to specify the frequency in number of batches seen.\n"
     ]
    }
   ],
   "source": [
    "\n",
    "#import tensorflow as \n",
    "model = Sequential()\n",
    "\n",
    "# Creating first block- (2 Convolution + 1 Max pool)\n",
    "model.add(Conv2D(filters= 64, kernel_size= (3,3), strides= (1,1), padding='same', input_shape= (32, 32, 3)))\n",
    "model.add(LeakyReLU(alpha=0.1))\n",
    "model.add(Conv2D(filters= 64, kernel_size= (3,3), strides= (1,1), padding='same'))\n",
    "model.add(LeakyReLU(alpha=0.1))\n",
    "#model.add(BatchNormalization())\n",
    "model.add(MaxPool2D(pool_size= (2,2), strides=(1,1)))\n",
    "model.add(Dropout(0.5))\n",
    "\n",
    "# Creating second block- (2 Convolution + 1 Max pool)\n",
    "model.add(Conv2D(filters= 128, kernel_size= (3,3), strides= (1,1), padding='same'))\n",
    "model.add(LeakyReLU(alpha=0.1))\n",
    "model.add(Conv2D(filters= 128, kernel_size= (3,3), strides= (1,1), padding='same'))\n",
    "model.add(LeakyReLU(alpha=0.1))\n",
    "#model.add(BatchNormalization())\n",
    "model.add(MaxPool2D(pool_size= (2,2), strides=(1,1)))\n",
    "model.add(Dropout(0.5))\n",
    "\n",
    "\n",
    "# Creating third block- (3 Convolution + 1 Max pool)\n",
    "model.add(Conv2D(filters= 256, kernel_size= (3,3), strides= (1,1), padding='same'))\n",
    "model.add(LeakyReLU(alpha=0.1))\n",
    "model.add(Conv2D(filters= 256, kernel_size= (3,3), strides= (1,1), padding='same'))\n",
    "model.add(LeakyReLU(alpha=0.1))\n",
    "model.add(Conv2D(filters= 256, kernel_size= (3,3), strides= (1,1), padding='same'))\n",
    "model.add(LeakyReLU(alpha=0.1))\n",
    "#model.add(BatchNormalization())\n",
    "model.add(MaxPool2D(pool_size= (2,2), strides=(1,1)))\n",
    "model.add(Dropout(0.5))\n",
    "\n",
    "# Creating fourth block- (3 Convolution + 1 Max pool)\n",
    "model.add(Conv2D(filters= 512, kernel_size= (3,3), strides= (1,1), padding='same'))\n",
    "model.add(LeakyReLU(alpha=0.1))\n",
    "model.add(Conv2D(filters= 512, kernel_size= (3,3), strides= (1,1), padding='same'))\n",
    "model.add(LeakyReLU(alpha=0.1))\n",
    "model.add(Conv2D(filters= 512, kernel_size= (3,3), strides= (1,1), padding='same'))\n",
    "model.add(LeakyReLU(alpha=0.1))\n",
    "#model.add(Dropout(0.3))\n",
    "model.add(MaxPool2D(pool_size= (2,2), strides=(2,2)))\n",
    "model.add(Dropout(0.5))\n",
    "\n",
    "# Creating fifth block- (3 Convolution + 1 Max pool)\n",
    "model.add(Conv2D(filters= 512, kernel_size= (3,3), strides= (1,1), padding='same'))\n",
    "model.add(LeakyReLU(alpha=0.1))\n",
    "model.add(Conv2D(filters= 512, kernel_size= (3,3), strides= (1,1), padding='same'))\n",
    "model.add(LeakyReLU(alpha=0.1))\n",
    "model.add(Conv2D(filters= 512, kernel_size= (3,3), strides= (1,1), padding='same'))\n",
    "model.add(LeakyReLU(alpha=0.1))\n",
    "#model.add(Dropout(0.3))\n",
    "model.add(MaxPool2D(pool_size= (2,2), strides=(1,1)))\n",
    "model.add(Dropout(0.5))\n",
    "\n",
    "# Flattening the pooled image pixels\n",
    "model.add(Flatten())\n",
    "\n",
    "# Creating 2 Dense Layers\n",
    "model.add(Dense(units= 512))\n",
    "model.add(LeakyReLU(alpha=0.1))\n",
    "model.add(Dropout(0.5))\n",
    "#model.add(Dense(units= 512))\n",
    "#model.add(LeakyReLU(alpha=0.1))\n",
    "model.add(Dropout(0.5))\n",
    "#model.add(Dropout(0.3))\n",
    "# Creating an output layer\n",
    "model.add(Dense(units= 100, activation='softmax'))\n",
    "'''\n",
    "optimizer = tf.train.AdamOptimizer(learning_rate=0.1)\n",
    "gvs = optimizer.compute_gradients(cost)\n",
    "capped_gvs = [(tf.clip_by_value(grad, -1., 1.), var) for grad, var in gvs]\n",
    "train_op = optimizer.apply_gradients(capped_gvs)\n",
    "'''\n",
    "#adam=Adam(learning_rate=0.0001,clipnorm=1,name='adam')\n",
    "\n",
    "adam=Adam(learning_rate=0.0001,clipnorm=1,name='adam')\n",
    "\n",
    "\n",
    "model.compile(optimizer=adam,loss='categorical_crossentropy',metrics=['accuracy'])\n",
    "\n",
    "checkpoint = ModelCheckpoint(\"VGG_DropOut_SGD_weights_amit.hdf5\", monitor='val_accuracy', verbose=1, save_best_only=True, save_weights_only=True, mode='auto', period=1)\n",
    "early = EarlyStopping(monitor='val_accuracy', min_delta=0, patience=5, verbose=1, mode='auto',restore_best_weights=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "uKsfdcP3Dhi_"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "id": "mMVZvLpNTWzv",
    "outputId": "713a68ce-3e9a-4912-ca95-5515106df191"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100\n",
      "  2/391 [..............................] - ETA: 2:17 - loss: 34.2420 - accuracy: 0.0078  WARNING:tensorflow:Callbacks method `on_train_batch_end` is slow compared to the batch time (batch time: 0.2650s vs `on_train_batch_end` time: 0.4408s). Check your callbacks.\n",
      "391/391 [==============================] - ETA: 0s - loss: 4.7136 - accuracy: 0.0189\n",
      "Epoch 00001: val_accuracy improved from -inf to 0.03990, saving model to VGG_DropOut_SGD_weights_amit.hdf5\n",
      "391/391 [==============================] - 316s 808ms/step - loss: 4.7136 - accuracy: 0.0189 - val_loss: 4.4680 - val_accuracy: 0.0399\n",
      "Epoch 2/100\n",
      "391/391 [==============================] - ETA: 0s - loss: 4.2410 - accuracy: 0.0560\n",
      "Epoch 00002: val_accuracy improved from 0.03990 to 0.11240, saving model to VGG_DropOut_SGD_weights_amit.hdf5\n",
      "391/391 [==============================] - 318s 814ms/step - loss: 4.2410 - accuracy: 0.0560 - val_loss: 4.0943 - val_accuracy: 0.1124\n",
      "Epoch 3/100\n",
      "391/391 [==============================] - ETA: 0s - loss: 3.9685 - accuracy: 0.0976\n",
      "Epoch 00003: val_accuracy improved from 0.11240 to 0.14620, saving model to VGG_DropOut_SGD_weights_amit.hdf5\n",
      "391/391 [==============================] - 318s 814ms/step - loss: 3.9685 - accuracy: 0.0976 - val_loss: 3.7970 - val_accuracy: 0.1462\n",
      "Epoch 4/100\n",
      "391/391 [==============================] - ETA: 0s - loss: 3.7319 - accuracy: 0.1381\n",
      "Epoch 00004: val_accuracy did not improve from 0.14620\n",
      "391/391 [==============================] - 318s 812ms/step - loss: 3.7319 - accuracy: 0.1381 - val_loss: 3.7000 - val_accuracy: 0.1446\n",
      "Epoch 5/100\n",
      "391/391 [==============================] - ETA: 0s - loss: 3.5562 - accuracy: 0.1675\n",
      "Epoch 00005: val_accuracy did not improve from 0.14620\n",
      "391/391 [==============================] - 318s 812ms/step - loss: 3.5562 - accuracy: 0.1675 - val_loss: 3.8590 - val_accuracy: 0.1270\n",
      "Epoch 6/100\n",
      "391/391 [==============================] - ETA: 0s - loss: 3.3920 - accuracy: 0.1973\n",
      "Epoch 00006: val_accuracy improved from 0.14620 to 0.19870, saving model to VGG_DropOut_SGD_weights_amit.hdf5\n",
      "391/391 [==============================] - 318s 814ms/step - loss: 3.3920 - accuracy: 0.1973 - val_loss: 3.3492 - val_accuracy: 0.1987\n",
      "Epoch 7/100\n",
      "391/391 [==============================] - ETA: 0s - loss: 3.2655 - accuracy: 0.2192\n",
      "Epoch 00007: val_accuracy improved from 0.19870 to 0.27090, saving model to VGG_DropOut_SGD_weights_amit.hdf5\n",
      "391/391 [==============================] - 318s 813ms/step - loss: 3.2655 - accuracy: 0.2192 - val_loss: 2.9899 - val_accuracy: 0.2709\n",
      "Epoch 8/100\n",
      "391/391 [==============================] - ETA: 0s - loss: 3.1351 - accuracy: 0.2419\n",
      "Epoch 00008: val_accuracy did not improve from 0.27090\n",
      "391/391 [==============================] - 317s 811ms/step - loss: 3.1351 - accuracy: 0.2419 - val_loss: 2.9776 - val_accuracy: 0.2678\n",
      "Epoch 9/100\n",
      "391/391 [==============================] - ETA: 0s - loss: 3.0157 - accuracy: 0.2664\n",
      "Epoch 00009: val_accuracy did not improve from 0.27090\n",
      "391/391 [==============================] - 317s 811ms/step - loss: 3.0157 - accuracy: 0.2664 - val_loss: 3.0706 - val_accuracy: 0.2551\n",
      "Epoch 10/100\n",
      "391/391 [==============================] - ETA: 0s - loss: 2.9170 - accuracy: 0.2836\n",
      "Epoch 00010: val_accuracy improved from 0.27090 to 0.30010, saving model to VGG_DropOut_SGD_weights_amit.hdf5\n",
      "391/391 [==============================] - 317s 812ms/step - loss: 2.9170 - accuracy: 0.2836 - val_loss: 2.8537 - val_accuracy: 0.3001\n",
      "Epoch 11/100\n",
      "391/391 [==============================] - ETA: 0s - loss: 2.8148 - accuracy: 0.3034\n",
      "Epoch 00011: val_accuracy did not improve from 0.30010\n",
      "391/391 [==============================] - 317s 811ms/step - loss: 2.8148 - accuracy: 0.3034 - val_loss: 2.9289 - val_accuracy: 0.2945\n",
      "Epoch 12/100\n",
      "391/391 [==============================] - ETA: 0s - loss: 2.7305 - accuracy: 0.3169\n",
      "Epoch 00012: val_accuracy improved from 0.30010 to 0.35280, saving model to VGG_DropOut_SGD_weights_amit.hdf5\n",
      "391/391 [==============================] - 318s 813ms/step - loss: 2.7305 - accuracy: 0.3169 - val_loss: 2.5687 - val_accuracy: 0.3528\n",
      "Epoch 13/100\n",
      "391/391 [==============================] - ETA: 0s - loss: 2.6408 - accuracy: 0.3377\n",
      "Epoch 00013: val_accuracy did not improve from 0.35280\n",
      "391/391 [==============================] - 317s 811ms/step - loss: 2.6408 - accuracy: 0.3377 - val_loss: 2.6933 - val_accuracy: 0.3517\n",
      "Epoch 14/100\n",
      "391/391 [==============================] - ETA: 0s - loss: 2.5681 - accuracy: 0.3518\n",
      "Epoch 00014: val_accuracy improved from 0.35280 to 0.38710, saving model to VGG_DropOut_SGD_weights_amit.hdf5\n",
      "391/391 [==============================] - 318s 812ms/step - loss: 2.5681 - accuracy: 0.3518 - val_loss: 2.4090 - val_accuracy: 0.3871\n",
      "Epoch 15/100\n",
      "391/391 [==============================] - ETA: 0s - loss: 2.4832 - accuracy: 0.3668\n",
      "Epoch 00015: val_accuracy did not improve from 0.38710\n",
      "391/391 [==============================] - 317s 810ms/step - loss: 2.4832 - accuracy: 0.3668 - val_loss: 2.4741 - val_accuracy: 0.3856\n",
      "Epoch 16/100\n",
      "391/391 [==============================] - ETA: 0s - loss: 2.4331 - accuracy: 0.3787\n",
      "Epoch 00016: val_accuracy improved from 0.38710 to 0.43180, saving model to VGG_DropOut_SGD_weights_amit.hdf5\n",
      "391/391 [==============================] - 318s 814ms/step - loss: 2.4331 - accuracy: 0.3787 - val_loss: 2.1963 - val_accuracy: 0.4318\n",
      "Epoch 17/100\n",
      "391/391 [==============================] - ETA: 0s - loss: 2.3699 - accuracy: 0.3937\n",
      "Epoch 00017: val_accuracy improved from 0.43180 to 0.45660, saving model to VGG_DropOut_SGD_weights_amit.hdf5\n",
      "391/391 [==============================] - 318s 813ms/step - loss: 2.3699 - accuracy: 0.3937 - val_loss: 2.1211 - val_accuracy: 0.4566\n",
      "Epoch 18/100\n",
      "391/391 [==============================] - ETA: 0s - loss: 2.3154 - accuracy: 0.4022\n",
      "Epoch 00018: val_accuracy did not improve from 0.45660\n",
      "391/391 [==============================] - 318s 812ms/step - loss: 2.3154 - accuracy: 0.4022 - val_loss: 2.2031 - val_accuracy: 0.4337\n",
      "Epoch 19/100\n",
      "391/391 [==============================] - ETA: 0s - loss: 2.2637 - accuracy: 0.4119\n",
      "Epoch 00019: val_accuracy improved from 0.45660 to 0.47850, saving model to VGG_DropOut_SGD_weights_amit.hdf5\n",
      "391/391 [==============================] - 318s 813ms/step - loss: 2.2637 - accuracy: 0.4119 - val_loss: 2.0269 - val_accuracy: 0.4785\n",
      "Epoch 20/100\n",
      "391/391 [==============================] - ETA: 0s - loss: 2.2132 - accuracy: 0.4237\n",
      "Epoch 00020: val_accuracy did not improve from 0.47850\n",
      "391/391 [==============================] - 318s 813ms/step - loss: 2.2132 - accuracy: 0.4237 - val_loss: 2.1131 - val_accuracy: 0.4627\n",
      "Epoch 21/100\n",
      "391/391 [==============================] - ETA: 0s - loss: 2.1663 - accuracy: 0.4350\n",
      "Epoch 00021: val_accuracy did not improve from 0.47850\n",
      "391/391 [==============================] - 317s 810ms/step - loss: 2.1663 - accuracy: 0.4350 - val_loss: 2.2064 - val_accuracy: 0.4550\n",
      "Epoch 22/100\n",
      "391/391 [==============================] - ETA: 0s - loss: 2.1198 - accuracy: 0.4474\n",
      "Epoch 00022: val_accuracy improved from 0.47850 to 0.48840, saving model to VGG_DropOut_SGD_weights_amit.hdf5\n",
      "391/391 [==============================] - 318s 814ms/step - loss: 2.1198 - accuracy: 0.4474 - val_loss: 1.9693 - val_accuracy: 0.4884\n",
      "Epoch 23/100\n",
      "391/391 [==============================] - ETA: 0s - loss: 2.0849 - accuracy: 0.4538\n",
      "Epoch 00023: val_accuracy improved from 0.48840 to 0.50300, saving model to VGG_DropOut_SGD_weights_amit.hdf5\n",
      "391/391 [==============================] - 318s 814ms/step - loss: 2.0849 - accuracy: 0.4538 - val_loss: 1.9095 - val_accuracy: 0.5030\n",
      "Epoch 24/100\n",
      "391/391 [==============================] - ETA: 0s - loss: 2.0399 - accuracy: 0.4642\n",
      "Epoch 00024: val_accuracy did not improve from 0.50300\n",
      "391/391 [==============================] - 317s 811ms/step - loss: 2.0399 - accuracy: 0.4642 - val_loss: 2.0385 - val_accuracy: 0.4878\n",
      "Epoch 25/100\n",
      "391/391 [==============================] - ETA: 0s - loss: 2.0057 - accuracy: 0.4703\n",
      "Epoch 00025: val_accuracy did not improve from 0.50300\n",
      "391/391 [==============================] - 317s 810ms/step - loss: 2.0057 - accuracy: 0.4703 - val_loss: 2.2642 - val_accuracy: 0.4723\n",
      "Epoch 26/100\n",
      "391/391 [==============================] - ETA: 0s - loss: 1.9811 - accuracy: 0.4747\n",
      "Epoch 00026: val_accuracy improved from 0.50300 to 0.50910, saving model to VGG_DropOut_SGD_weights_amit.hdf5\n",
      "391/391 [==============================] - 317s 812ms/step - loss: 1.9811 - accuracy: 0.4747 - val_loss: 1.9109 - val_accuracy: 0.5091\n",
      "Epoch 27/100\n",
      "391/391 [==============================] - ETA: 0s - loss: 1.9405 - accuracy: 0.4847\n",
      "Epoch 00027: val_accuracy did not improve from 0.50910\n",
      "391/391 [==============================] - 317s 811ms/step - loss: 1.9405 - accuracy: 0.4847 - val_loss: 2.0035 - val_accuracy: 0.5008\n",
      "Epoch 28/100\n",
      "391/391 [==============================] - ETA: 0s - loss: 1.9090 - accuracy: 0.4935\n",
      "Epoch 00028: val_accuracy did not improve from 0.50910\n",
      "391/391 [==============================] - 317s 811ms/step - loss: 1.9090 - accuracy: 0.4935 - val_loss: 1.9915 - val_accuracy: 0.5070\n",
      "Epoch 29/100\n",
      "391/391 [==============================] - ETA: 0s - loss: 1.8767 - accuracy: 0.4980\n",
      "Epoch 00029: val_accuracy improved from 0.50910 to 0.52830, saving model to VGG_DropOut_SGD_weights_amit.hdf5\n",
      "391/391 [==============================] - 318s 812ms/step - loss: 1.8767 - accuracy: 0.4980 - val_loss: 1.8273 - val_accuracy: 0.5283\n",
      "Epoch 30/100\n",
      "391/391 [==============================] - ETA: 0s - loss: 1.8394 - accuracy: 0.5057\n",
      "Epoch 00030: val_accuracy improved from 0.52830 to 0.55200, saving model to VGG_DropOut_SGD_weights_amit.hdf5\n",
      "391/391 [==============================] - 318s 814ms/step - loss: 1.8394 - accuracy: 0.5057 - val_loss: 1.7225 - val_accuracy: 0.5520\n",
      "Epoch 31/100\n",
      "391/391 [==============================] - ETA: 0s - loss: 1.8220 - accuracy: 0.5110\n",
      "Epoch 00031: val_accuracy improved from 0.55200 to 0.56770, saving model to VGG_DropOut_SGD_weights_amit.hdf5\n",
      "391/391 [==============================] - 318s 814ms/step - loss: 1.8220 - accuracy: 0.5110 - val_loss: 1.5712 - val_accuracy: 0.5677\n",
      "Epoch 32/100\n",
      "391/391 [==============================] - ETA: 0s - loss: 1.7925 - accuracy: 0.5170\n",
      "Epoch 00032: val_accuracy did not improve from 0.56770\n",
      "391/391 [==============================] - 318s 813ms/step - loss: 1.7925 - accuracy: 0.5170 - val_loss: 1.6548 - val_accuracy: 0.5592\n",
      "Epoch 33/100\n",
      "391/391 [==============================] - ETA: 0s - loss: 1.7708 - accuracy: 0.5245\n",
      "Epoch 00033: val_accuracy did not improve from 0.56770\n",
      "391/391 [==============================] - 318s 813ms/step - loss: 1.7708 - accuracy: 0.5245 - val_loss: 1.6350 - val_accuracy: 0.5676\n",
      "Epoch 34/100\n",
      "391/391 [==============================] - ETA: 0s - loss: 1.7464 - accuracy: 0.5275\n",
      "Epoch 00034: val_accuracy improved from 0.56770 to 0.56900, saving model to VGG_DropOut_SGD_weights_amit.hdf5\n",
      "391/391 [==============================] - 318s 814ms/step - loss: 1.7464 - accuracy: 0.5275 - val_loss: 1.6284 - val_accuracy: 0.5690\n",
      "Epoch 35/100\n",
      "391/391 [==============================] - ETA: 0s - loss: 1.7237 - accuracy: 0.5345\n",
      "Epoch 00035: val_accuracy did not improve from 0.56900\n",
      "391/391 [==============================] - 318s 812ms/step - loss: 1.7237 - accuracy: 0.5345 - val_loss: 1.9772 - val_accuracy: 0.5235\n",
      "Epoch 36/100\n",
      "391/391 [==============================] - ETA: 0s - loss: 1.6914 - accuracy: 0.5407\n",
      "Epoch 00036: val_accuracy did not improve from 0.56900\n",
      "391/391 [==============================] - 318s 813ms/step - loss: 1.6914 - accuracy: 0.5407 - val_loss: 1.8447 - val_accuracy: 0.5481\n",
      "Epoch 37/100\n",
      "391/391 [==============================] - ETA: 0s - loss: 1.6848 - accuracy: 0.5422\n",
      "Epoch 00037: val_accuracy did not improve from 0.56900\n",
      "391/391 [==============================] - 318s 814ms/step - loss: 1.6848 - accuracy: 0.5422 - val_loss: 1.7269 - val_accuracy: 0.5530\n",
      "Epoch 38/100\n",
      "391/391 [==============================] - ETA: 0s - loss: 1.6540 - accuracy: 0.5497\n",
      "Epoch 00038: val_accuracy improved from 0.56900 to 0.59560, saving model to VGG_DropOut_SGD_weights_amit.hdf5\n",
      "391/391 [==============================] - 318s 814ms/step - loss: 1.6540 - accuracy: 0.5497 - val_loss: 1.4841 - val_accuracy: 0.5956\n",
      "Epoch 39/100\n",
      "391/391 [==============================] - ETA: 0s - loss: 1.6302 - accuracy: 0.5558\n",
      "Epoch 00039: val_accuracy did not improve from 0.59560\n",
      "391/391 [==============================] - 318s 813ms/step - loss: 1.6302 - accuracy: 0.5558 - val_loss: 1.7902 - val_accuracy: 0.5502\n",
      "Epoch 40/100\n",
      "391/391 [==============================] - ETA: 0s - loss: 1.6261 - accuracy: 0.5566\n",
      "Epoch 00040: val_accuracy did not improve from 0.59560\n",
      "391/391 [==============================] - 317s 812ms/step - loss: 1.6261 - accuracy: 0.5566 - val_loss: 1.8010 - val_accuracy: 0.5458\n",
      "Epoch 41/100\n",
      "391/391 [==============================] - ETA: 0s - loss: 1.5963 - accuracy: 0.5626\n",
      "Epoch 00041: val_accuracy did not improve from 0.59560\n",
      "391/391 [==============================] - 318s 813ms/step - loss: 1.5963 - accuracy: 0.5626 - val_loss: 1.5562 - val_accuracy: 0.5880\n",
      "Epoch 42/100\n",
      "391/391 [==============================] - ETA: 0s - loss: 1.5753 - accuracy: 0.5683\n",
      "Epoch 00042: val_accuracy did not improve from 0.59560\n",
      "391/391 [==============================] - 318s 813ms/step - loss: 1.5753 - accuracy: 0.5683 - val_loss: 1.5912 - val_accuracy: 0.5793\n",
      "Epoch 43/100\n",
      "391/391 [==============================] - ETA: 0s - loss: 1.5741 - accuracy: 0.5682\n",
      "Epoch 00043: val_accuracy did not improve from 0.59560\n",
      "Restoring model weights from the end of the best epoch.\n",
      "391/391 [==============================] - 318s 813ms/step - loss: 1.5741 - accuracy: 0.5682 - val_loss: 1.6302 - val_accuracy: 0.5700\n",
      "Epoch 00043: early stopping\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.keras.callbacks.History at 0x7f68d052ec50>"
      ]
     },
     "execution_count": 5,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.fit(aug_data.flow(x_train, y_train, batch_size=128), batch_size=128, epochs=100, verbose=1, validation_data=(x_test, y_test),callbacks=[checkpoint,early])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "id": "1bE-YpyQDiTg",
    "outputId": "87cac856-58b7-43c2-b6c7-1e71ffb3463c"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From <ipython-input-6-44ed652cd446>:1: Sequential.predict_classes (from tensorflow.python.keras.engine.sequential) is deprecated and will be removed after 2021-01-01.\n",
      "Instructions for updating:\n",
      "Please use instead:* `np.argmax(model.predict(x), axis=-1)`,   if your model does multi-class classification   (e.g. if it uses a `softmax` last-layer activation).* `(model.predict(x) > 0.5).astype(\"int32\")`,   if your model does binary classification   (e.g. if it uses a `sigmoid` last-layer activation).\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.89      0.75      0.82       100\n",
      "           1       0.80      0.66      0.73       100\n",
      "           2       0.47      0.54      0.50       100\n",
      "           3       0.51      0.18      0.27       100\n",
      "           4       0.33      0.40      0.36       100\n",
      "           5       0.56      0.63      0.59       100\n",
      "           6       0.57      0.77      0.66       100\n",
      "           7       0.57      0.69      0.63       100\n",
      "           8       0.62      0.88      0.73       100\n",
      "           9       0.86      0.65      0.74       100\n",
      "          10       0.46      0.37      0.41       100\n",
      "          11       0.41      0.46      0.44       100\n",
      "          12       0.71      0.60      0.65       100\n",
      "          13       0.67      0.48      0.56       100\n",
      "          14       0.68      0.54      0.60       100\n",
      "          15       0.52      0.57      0.54       100\n",
      "          16       0.63      0.67      0.65       100\n",
      "          17       0.71      0.82      0.76       100\n",
      "          18       0.59      0.39      0.47       100\n",
      "          19       0.66      0.57      0.61       100\n",
      "          20       0.81      0.84      0.82       100\n",
      "          21       0.81      0.75      0.78       100\n",
      "          22       0.76      0.45      0.57       100\n",
      "          23       0.74      0.70      0.72       100\n",
      "          24       0.94      0.60      0.73       100\n",
      "          25       0.58      0.49      0.53       100\n",
      "          26       0.58      0.58      0.58       100\n",
      "          27       0.33      0.59      0.42       100\n",
      "          28       0.73      0.76      0.75       100\n",
      "          29       0.72      0.58      0.64       100\n",
      "          30       0.60      0.50      0.54       100\n",
      "          31       0.59      0.67      0.63       100\n",
      "          32       0.78      0.50      0.61       100\n",
      "          33       0.30      0.65      0.41       100\n",
      "          34       0.65      0.51      0.57       100\n",
      "          35       0.47      0.49      0.48       100\n",
      "          36       0.63      0.81      0.71       100\n",
      "          37       0.63      0.55      0.59       100\n",
      "          38       0.40      0.46      0.43       100\n",
      "          39       0.64      0.67      0.65       100\n",
      "          40       0.87      0.40      0.55       100\n",
      "          41       0.89      0.73      0.80       100\n",
      "          42       0.26      0.75      0.38       100\n",
      "          43       0.59      0.62      0.60       100\n",
      "          44       0.46      0.37      0.41       100\n",
      "          45       0.45      0.37      0.41       100\n",
      "          46       0.50      0.38      0.43       100\n",
      "          47       0.62      0.46      0.53       100\n",
      "          48       0.77      0.89      0.83       100\n",
      "          49       0.79      0.76      0.78       100\n",
      "          50       0.46      0.43      0.45       100\n",
      "          51       0.70      0.53      0.60       100\n",
      "          52       0.47      0.85      0.61       100\n",
      "          53       0.87      0.77      0.81       100\n",
      "          54       0.71      0.70      0.71       100\n",
      "          55       0.34      0.15      0.21       100\n",
      "          56       0.87      0.65      0.74       100\n",
      "          57       0.83      0.64      0.72       100\n",
      "          58       0.78      0.77      0.77       100\n",
      "          59       0.56      0.55      0.55       100\n",
      "          60       0.72      0.83      0.77       100\n",
      "          61       0.49      0.74      0.59       100\n",
      "          62       0.65      0.71      0.68       100\n",
      "          63       0.30      0.75      0.43       100\n",
      "          64       0.60      0.34      0.43       100\n",
      "          65       0.59      0.36      0.45       100\n",
      "          66       0.71      0.70      0.71       100\n",
      "          67       0.47      0.42      0.44       100\n",
      "          68       0.77      0.92      0.84       100\n",
      "          69       0.77      0.73      0.75       100\n",
      "          70       0.74      0.74      0.74       100\n",
      "          71       0.65      0.76      0.70       100\n",
      "          72       0.32      0.13      0.18       100\n",
      "          73       0.48      0.36      0.41       100\n",
      "          74       0.30      0.52      0.38       100\n",
      "          75       0.81      0.89      0.85       100\n",
      "          76       0.80      0.79      0.79       100\n",
      "          77       0.69      0.49      0.57       100\n",
      "          78       0.43      0.66      0.52       100\n",
      "          79       0.51      0.77      0.62       100\n",
      "          80       0.49      0.28      0.36       100\n",
      "          81       0.60      0.69      0.64       100\n",
      "          82       0.94      0.85      0.89       100\n",
      "          83       0.78      0.35      0.48       100\n",
      "          84       0.66      0.42      0.51       100\n",
      "          85       0.78      0.68      0.73       100\n",
      "          86       0.78      0.62      0.69       100\n",
      "          87       0.53      0.72      0.61       100\n",
      "          88       0.43      0.72      0.54       100\n",
      "          89       0.47      0.79      0.59       100\n",
      "          90       0.83      0.52      0.64       100\n",
      "          91       0.72      0.77      0.74       100\n",
      "          92       0.79      0.31      0.45       100\n",
      "          93       0.57      0.36      0.44       100\n",
      "          94       0.75      0.85      0.79       100\n",
      "          95       0.52      0.62      0.56       100\n",
      "          96       0.55      0.34      0.42       100\n",
      "          97       0.81      0.56      0.66       100\n",
      "          98       0.65      0.31      0.42       100\n",
      "          99       0.73      0.65      0.69       100\n",
      "\n",
      "    accuracy                           0.60     10000\n",
      "   macro avg       0.63      0.60      0.59     10000\n",
      "weighted avg       0.63      0.60      0.59     10000\n",
      "\n",
      "Accuracy is 0.5956\n",
      "Precision is 0.6283979516123185\n",
      "Recall is 0.5956\n"
     ]
    }
   ],
   "source": [
    "y_pred=model.predict_classes(x_test)\n",
    "from sklearn.metrics import classification_report,accuracy_score,recall_score,precision_score\n",
    "print(classification_report(Y_test,y_pred))\n",
    "print(\"Accuracy is {}\".format(accuracy_score(Y_test,y_pred)))\n",
    "print(\"Precision is {}\".format(precision_score(Y_test,y_pred,average=\"weighted\")))\n",
    "print(\"Recall is {}\".format(recall_score(Y_test,y_pred,average=\"weighted\")))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "id": "RbA6fe2JDocn",
    "outputId": "16f126c9-d0ed-42ae-b50d-a6be0ba909f2"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy is 0.68172\n"
     ]
    }
   ],
   "source": [
    "YY=model.predict_classes(x_train)\n",
    "print(\"Accuracy is {}\".format(accuracy_score(Y_train,YY)))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "id": "Nkwb7BH1MSWP",
    "outputId": "8df36963-b1da-4519-fb6c-105442ed6cd6"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_1\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "conv2d_13 (Conv2D)           (None, 32, 32, 64)        1792      \n",
      "_________________________________________________________________\n",
      "leaky_re_lu_14 (LeakyReLU)   (None, 32, 32, 64)        0         \n",
      "_________________________________________________________________\n",
      "conv2d_14 (Conv2D)           (None, 32, 32, 64)        36928     \n",
      "_________________________________________________________________\n",
      "leaky_re_lu_15 (LeakyReLU)   (None, 32, 32, 64)        0         \n",
      "_________________________________________________________________\n",
      "max_pooling2d_5 (MaxPooling2 (None, 31, 31, 64)        0         \n",
      "_________________________________________________________________\n",
      "dropout_7 (Dropout)          (None, 31, 31, 64)        0         \n",
      "_________________________________________________________________\n",
      "conv2d_15 (Conv2D)           (None, 31, 31, 128)       73856     \n",
      "_________________________________________________________________\n",
      "leaky_re_lu_16 (LeakyReLU)   (None, 31, 31, 128)       0         \n",
      "_________________________________________________________________\n",
      "conv2d_16 (Conv2D)           (None, 31, 31, 128)       147584    \n",
      "_________________________________________________________________\n",
      "leaky_re_lu_17 (LeakyReLU)   (None, 31, 31, 128)       0         \n",
      "_________________________________________________________________\n",
      "max_pooling2d_6 (MaxPooling2 (None, 30, 30, 128)       0         \n",
      "_________________________________________________________________\n",
      "dropout_8 (Dropout)          (None, 30, 30, 128)       0         \n",
      "_________________________________________________________________\n",
      "conv2d_17 (Conv2D)           (None, 30, 30, 256)       295168    \n",
      "_________________________________________________________________\n",
      "leaky_re_lu_18 (LeakyReLU)   (None, 30, 30, 256)       0         \n",
      "_________________________________________________________________\n",
      "conv2d_18 (Conv2D)           (None, 30, 30, 256)       590080    \n",
      "_________________________________________________________________\n",
      "leaky_re_lu_19 (LeakyReLU)   (None, 30, 30, 256)       0         \n",
      "_________________________________________________________________\n",
      "conv2d_19 (Conv2D)           (None, 30, 30, 256)       590080    \n",
      "_________________________________________________________________\n",
      "leaky_re_lu_20 (LeakyReLU)   (None, 30, 30, 256)       0         \n",
      "_________________________________________________________________\n",
      "max_pooling2d_7 (MaxPooling2 (None, 29, 29, 256)       0         \n",
      "_________________________________________________________________\n",
      "dropout_9 (Dropout)          (None, 29, 29, 256)       0         \n",
      "_________________________________________________________________\n",
      "conv2d_20 (Conv2D)           (None, 29, 29, 512)       1180160   \n",
      "_________________________________________________________________\n",
      "leaky_re_lu_21 (LeakyReLU)   (None, 29, 29, 512)       0         \n",
      "_________________________________________________________________\n",
      "conv2d_21 (Conv2D)           (None, 29, 29, 512)       2359808   \n",
      "_________________________________________________________________\n",
      "leaky_re_lu_22 (LeakyReLU)   (None, 29, 29, 512)       0         \n",
      "_________________________________________________________________\n",
      "conv2d_22 (Conv2D)           (None, 29, 29, 512)       2359808   \n",
      "_________________________________________________________________\n",
      "leaky_re_lu_23 (LeakyReLU)   (None, 29, 29, 512)       0         \n",
      "_________________________________________________________________\n",
      "max_pooling2d_8 (MaxPooling2 (None, 14, 14, 512)       0         \n",
      "_________________________________________________________________\n",
      "dropout_10 (Dropout)         (None, 14, 14, 512)       0         \n",
      "_________________________________________________________________\n",
      "conv2d_23 (Conv2D)           (None, 14, 14, 512)       2359808   \n",
      "_________________________________________________________________\n",
      "leaky_re_lu_24 (LeakyReLU)   (None, 14, 14, 512)       0         \n",
      "_________________________________________________________________\n",
      "conv2d_24 (Conv2D)           (None, 14, 14, 512)       2359808   \n",
      "_________________________________________________________________\n",
      "leaky_re_lu_25 (LeakyReLU)   (None, 14, 14, 512)       0         \n",
      "_________________________________________________________________\n",
      "conv2d_25 (Conv2D)           (None, 14, 14, 512)       2359808   \n",
      "_________________________________________________________________\n",
      "leaky_re_lu_26 (LeakyReLU)   (None, 14, 14, 512)       0         \n",
      "_________________________________________________________________\n",
      "max_pooling2d_9 (MaxPooling2 (None, 13, 13, 512)       0         \n",
      "_________________________________________________________________\n",
      "dropout_11 (Dropout)         (None, 13, 13, 512)       0         \n",
      "_________________________________________________________________\n",
      "flatten_1 (Flatten)          (None, 86528)             0         \n",
      "_________________________________________________________________\n",
      "dense_2 (Dense)              (None, 512)               44302848  \n",
      "_________________________________________________________________\n",
      "leaky_re_lu_27 (LeakyReLU)   (None, 512)               0         \n",
      "_________________________________________________________________\n",
      "dropout_12 (Dropout)         (None, 512)               0         \n",
      "_________________________________________________________________\n",
      "dropout_13 (Dropout)         (None, 512)               0         \n",
      "_________________________________________________________________\n",
      "dense_3 (Dense)              (None, 100)               51300     \n",
      "=================================================================\n",
      "Total params: 59,068,836\n",
      "Trainable params: 59,068,836\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From C:\\Users\\uamit\\Anaconda3\\lib\\site-packages\\keras\\backend\\tensorflow_backend.py:422: The name tf.global_variables is deprecated. Please use tf.compat.v1.global_variables instead.\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.89      0.75      0.82       100\n",
      "           1       0.80      0.66      0.73       100\n",
      "           2       0.47      0.54      0.50       100\n",
      "           3       0.51      0.18      0.27       100\n",
      "           4       0.33      0.40      0.36       100\n",
      "           5       0.56      0.63      0.59       100\n",
      "           6       0.57      0.77      0.66       100\n",
      "           7       0.57      0.69      0.63       100\n",
      "           8       0.62      0.88      0.73       100\n",
      "           9       0.86      0.65      0.74       100\n",
      "          10       0.46      0.37      0.41       100\n",
      "          11       0.41      0.46      0.44       100\n",
      "          12       0.71      0.60      0.65       100\n",
      "          13       0.67      0.48      0.56       100\n",
      "          14       0.68      0.54      0.60       100\n",
      "          15       0.52      0.57      0.54       100\n",
      "          16       0.63      0.67      0.65       100\n",
      "          17       0.71      0.82      0.76       100\n",
      "          18       0.59      0.39      0.47       100\n",
      "          19       0.66      0.57      0.61       100\n",
      "          20       0.81      0.84      0.82       100\n",
      "          21       0.81      0.75      0.78       100\n",
      "          22       0.76      0.45      0.57       100\n",
      "          23       0.74      0.70      0.72       100\n",
      "          24       0.94      0.60      0.73       100\n",
      "          25       0.58      0.49      0.53       100\n",
      "          26       0.58      0.58      0.58       100\n",
      "          27       0.33      0.59      0.42       100\n",
      "          28       0.73      0.76      0.75       100\n",
      "          29       0.72      0.58      0.64       100\n",
      "          30       0.60      0.50      0.54       100\n",
      "          31       0.59      0.67      0.63       100\n",
      "          32       0.78      0.50      0.61       100\n",
      "          33       0.30      0.65      0.41       100\n",
      "          34       0.65      0.51      0.57       100\n",
      "          35       0.47      0.49      0.48       100\n",
      "          36       0.63      0.81      0.71       100\n",
      "          37       0.63      0.55      0.59       100\n",
      "          38       0.40      0.46      0.43       100\n",
      "          39       0.64      0.67      0.65       100\n",
      "          40       0.87      0.40      0.55       100\n",
      "          41       0.89      0.73      0.80       100\n",
      "          42       0.26      0.75      0.38       100\n",
      "          43       0.59      0.62      0.60       100\n",
      "          44       0.46      0.37      0.41       100\n",
      "          45       0.45      0.37      0.41       100\n",
      "          46       0.50      0.38      0.43       100\n",
      "          47       0.62      0.46      0.53       100\n",
      "          48       0.77      0.89      0.83       100\n",
      "          49       0.79      0.76      0.78       100\n",
      "          50       0.46      0.43      0.45       100\n",
      "          51       0.70      0.53      0.60       100\n",
      "          52       0.47      0.85      0.61       100\n",
      "          53       0.87      0.77      0.81       100\n",
      "          54       0.71      0.70      0.71       100\n",
      "          55       0.34      0.15      0.21       100\n",
      "          56       0.87      0.65      0.74       100\n",
      "          57       0.83      0.64      0.72       100\n",
      "          58       0.78      0.77      0.77       100\n",
      "          59       0.56      0.55      0.55       100\n",
      "          60       0.72      0.83      0.77       100\n",
      "          61       0.49      0.74      0.59       100\n",
      "          62       0.65      0.71      0.68       100\n",
      "          63       0.30      0.75      0.43       100\n",
      "          64       0.60      0.34      0.43       100\n",
      "          65       0.59      0.36      0.45       100\n",
      "          66       0.71      0.70      0.71       100\n",
      "          67       0.47      0.42      0.44       100\n",
      "          68       0.77      0.92      0.84       100\n",
      "          69       0.77      0.73      0.75       100\n",
      "          70       0.74      0.74      0.74       100\n",
      "          71       0.65      0.76      0.70       100\n",
      "          72       0.32      0.13      0.18       100\n",
      "          73       0.48      0.36      0.41       100\n",
      "          74       0.30      0.52      0.38       100\n",
      "          75       0.81      0.89      0.85       100\n",
      "          76       0.80      0.79      0.79       100\n",
      "          77       0.69      0.49      0.57       100\n",
      "          78       0.43      0.66      0.52       100\n",
      "          79       0.51      0.77      0.62       100\n",
      "          80       0.49      0.28      0.36       100\n",
      "          81       0.60      0.69      0.64       100\n",
      "          82       0.94      0.85      0.89       100\n",
      "          83       0.78      0.35      0.48       100\n",
      "          84       0.66      0.42      0.51       100\n",
      "          85       0.78      0.68      0.73       100\n",
      "          86       0.78      0.62      0.69       100\n",
      "          87       0.53      0.72      0.61       100\n",
      "          88       0.43      0.72      0.54       100\n",
      "          89       0.47      0.79      0.59       100\n",
      "          90       0.83      0.52      0.64       100\n",
      "          91       0.72      0.77      0.74       100\n",
      "          92       0.79      0.31      0.45       100\n",
      "          93       0.57      0.36      0.44       100\n",
      "          94       0.75      0.85      0.79       100\n",
      "          95       0.52      0.62      0.56       100\n",
      "          96       0.55      0.34      0.42       100\n",
      "          97       0.81      0.56      0.66       100\n",
      "          98       0.65      0.31      0.42       100\n",
      "          99       0.73      0.65      0.69       100\n",
      "\n",
      "    accuracy                           0.60     10000\n",
      "   macro avg       0.63      0.60      0.59     10000\n",
      "weighted avg       0.63      0.60      0.59     10000\n",
      "\n",
      "Accuracy is 0.5956\n",
      "Precision is 0.6283979516123185\n",
      "Recall is 0.5956\n"
     ]
    }
   ],
   "source": [
    "from keras.callbacks import ModelCheckpoint, EarlyStopping\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Conv2D, MaxPool2D, Flatten, Dense,BatchNormalization,Activation,Dropout,LeakyReLU\n",
    "from tensorflow.keras.optimizers import SGD,Adam\n",
    "from keras.preprocessing.image import ImageDataGenerator\n",
    "from keras.utils import to_categorical\n",
    "from tensorflow.keras.datasets import cifar100\n",
    "\n",
    "\n",
    "\n",
    "(x_train, Y_train), (x_test, Y_test) = cifar100.load_data()\n",
    "#x_train = x_train.astype('float32') / 255\n",
    "#x_test = x_test.astype('float32') / 255\n",
    "\n",
    "y_train = to_categorical(Y_train)\n",
    "y_test = to_categorical(Y_test)\n",
    "\n",
    "\n",
    "#import tensorflow as \n",
    "model = Sequential()\n",
    "\n",
    "# Creating first block- (2 Convolution + 1 Max pool)\n",
    "model.add(Conv2D(filters= 64, kernel_size= (3,3), strides= (1,1), padding='same', input_shape= (32, 32, 3)))\n",
    "model.add(LeakyReLU(alpha=0.1))\n",
    "model.add(Conv2D(filters= 64, kernel_size= (3,3), strides= (1,1), padding='same'))\n",
    "model.add(LeakyReLU(alpha=0.1))\n",
    "#model.add(BatchNormalization())\n",
    "model.add(MaxPool2D(pool_size= (2,2), strides=(1,1)))\n",
    "model.add(Dropout(0.5))\n",
    "\n",
    "# Creating second block- (2 Convolution + 1 Max pool)\n",
    "model.add(Conv2D(filters= 128, kernel_size= (3,3), strides= (1,1), padding='same'))\n",
    "model.add(LeakyReLU(alpha=0.1))\n",
    "model.add(Conv2D(filters= 128, kernel_size= (3,3), strides= (1,1), padding='same'))\n",
    "model.add(LeakyReLU(alpha=0.1))\n",
    "#model.add(BatchNormalization())\n",
    "model.add(MaxPool2D(pool_size= (2,2), strides=(1,1)))\n",
    "model.add(Dropout(0.5))\n",
    "\n",
    "\n",
    "# Creating third block- (3 Convolution + 1 Max pool)\n",
    "model.add(Conv2D(filters= 256, kernel_size= (3,3), strides= (1,1), padding='same'))\n",
    "model.add(LeakyReLU(alpha=0.1))\n",
    "model.add(Conv2D(filters= 256, kernel_size= (3,3), strides= (1,1), padding='same'))\n",
    "model.add(LeakyReLU(alpha=0.1))\n",
    "model.add(Conv2D(filters= 256, kernel_size= (3,3), strides= (1,1), padding='same'))\n",
    "model.add(LeakyReLU(alpha=0.1))\n",
    "#model.add(BatchNormalization())\n",
    "model.add(MaxPool2D(pool_size= (2,2), strides=(1,1)))\n",
    "model.add(Dropout(0.5))\n",
    "\n",
    "# Creating fourth block- (3 Convolution + 1 Max pool)\n",
    "model.add(Conv2D(filters= 512, kernel_size= (3,3), strides= (1,1), padding='same'))\n",
    "model.add(LeakyReLU(alpha=0.1))\n",
    "model.add(Conv2D(filters= 512, kernel_size= (3,3), strides= (1,1), padding='same'))\n",
    "model.add(LeakyReLU(alpha=0.1))\n",
    "model.add(Conv2D(filters= 512, kernel_size= (3,3), strides= (1,1), padding='same'))\n",
    "model.add(LeakyReLU(alpha=0.1))\n",
    "#model.add(Dropout(0.3))\n",
    "model.add(MaxPool2D(pool_size= (2,2), strides=(2,2)))\n",
    "model.add(Dropout(0.5))\n",
    "\n",
    "# Creating fifth block- (3 Convolution + 1 Max pool)\n",
    "model.add(Conv2D(filters= 512, kernel_size= (3,3), strides= (1,1), padding='same'))\n",
    "model.add(LeakyReLU(alpha=0.1))\n",
    "model.add(Conv2D(filters= 512, kernel_size= (3,3), strides= (1,1), padding='same'))\n",
    "model.add(LeakyReLU(alpha=0.1))\n",
    "model.add(Conv2D(filters= 512, kernel_size= (3,3), strides= (1,1), padding='same'))\n",
    "model.add(LeakyReLU(alpha=0.1))\n",
    "#model.add(Dropout(0.3))\n",
    "model.add(MaxPool2D(pool_size= (2,2), strides=(1,1)))\n",
    "model.add(Dropout(0.5))\n",
    "\n",
    "# Flattening the pooled image pixels\n",
    "model.add(Flatten())\n",
    "\n",
    "# Creating 2 Dense Layers\n",
    "model.add(Dense(units= 512))\n",
    "model.add(LeakyReLU(alpha=0.1))\n",
    "model.add(Dropout(0.5))\n",
    "#model.add(Dense(units= 512))\n",
    "#model.add(LeakyReLU(alpha=0.1))\n",
    "model.add(Dropout(0.5))\n",
    "#model.add(Dropout(0.3))\n",
    "# Creating an output layer\n",
    "model.add(Dense(units= 100, activation='softmax'))\n",
    "\n",
    "model.load_weights(\"../weights/VGG_DropOut_Adam_weights_amit.hdf5\")\n",
    "\n",
    "y_true = y_test.argmax(-1)\n",
    "y_pred = model.predict(x_test).argmax(-1)\n",
    "\n",
    "from sklearn.metrics import classification_report,accuracy_score,recall_score,precision_score\n",
    "print(classification_report(Y_test,y_pred))\n",
    "print(\"Accuracy is {}\".format(accuracy_score(Y_test,y_pred)))\n",
    "print(\"Precision is {}\".format(precision_score(Y_test,y_pred,average=\"weighted\")))\n",
    "print(\"Recall is {}\".format(recall_score(Y_test,y_pred,average=\"weighted\")))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "name": "VGG_DropOut_Adam_weights_amit.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
