{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "id": "bFnK2Z-HI_Rd"
   },
   "outputs": [],
   "source": [
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras.layers import Conv2D,Dense,MaxPool2D,AveragePooling2D,Flatten\n",
    "import tensorflow.keras as keras\n",
    "from tensorflow.keras.datasets import cifar100\n",
    "from tensorflow.keras.utils import to_categorical\n",
    "from tensorflow.keras import layers\n",
    "\n",
    "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
    "from tensorflow.keras.optimizers import SGD,Adam\n",
    "\n",
    "from tensorflow.keras.layers import LeakyReLU,BatchNormalization\n",
    "\n",
    "from tensorflow.keras.callbacks import EarlyStopping,ModelCheckpoint"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 51
    },
    "id": "6DizgfodJOXM",
    "outputId": "620e63f0-592f-47e4-b751-69db98f5383d"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading data from https://www.cs.toronto.edu/~kriz/cifar-100-python.tar.gz\n",
      "169009152/169001437 [==============================] - 3s 0us/step\n"
     ]
    }
   ],
   "source": [
    "(X_train,Y_train),(X_test,Y_test)=cifar100.load_data()\n",
    "\n",
    "X_train,X_test=X_train/255,X_test/255\n",
    "y_train,y_test=to_categorical(Y_train),to_categorical(Y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "id": "LP3A8A75JPBZ"
   },
   "outputs": [],
   "source": [
    "aug_data=ImageDataGenerator(rotation_range=15,horizontal_flip=True,width_shift_range=0.1,height_shift_range=0.1,zoom_range=0.3)\n",
    "aug_data.fit(X_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "id": "Ik44xzLsJR-y"
   },
   "outputs": [],
   "source": [
    "shape=(32,32,3)\n",
    "\n",
    "inputs = keras.Input(shape=shape)\n",
    "\n",
    "a=layers.Conv2D(32,(3,3),strides=1,padding=\"same\",activation=\"relu\")(inputs)  #1\n",
    "a=layers.BatchNormalization()(a)\n",
    "a=layers.Conv2D(32,(3,3),strides=1,padding=\"same\",activation=\"relu\")(a)   #2\n",
    "a=layers.BatchNormalization()(a)\n",
    "a_temp=a\n",
    "a=layers.Conv2D(32,(3,3),strides=1,padding=\"same\",activation=\"relu\")(a)#\\3\n",
    "a=layers.BatchNormalization()(a)\n",
    "a=layers.Conv2D(32,(3,3),strides=1,padding=\"same\")(a)#\n",
    "a=layers.BatchNormalization()(a)\n",
    "\n",
    "a=layers.Add()([a,a_temp])\n",
    "a=layers.Activation(\"relu\")(a)\n",
    "#a=layers.BatchNormalization()(a)\n",
    "\n",
    "\n",
    "b=layers.Conv2D(64,(3,3),strides=1,padding=\"same\",activation=\"relu\")(a) #1\n",
    "                               \n",
    "#a_temp=a\n",
    "##########################################################################\n",
    "a=layers.Conv2D(64,(3,3),strides=1,padding=\"same\",activation=\"relu\")(a) #5\n",
    "a=layers.BatchNormalization()(a)\n",
    "\n",
    "a=layers.Conv2D(64,(3,3),strides=1,padding=\"same\")(a) #6\n",
    "a=layers.BatchNormalization()(a)\n",
    "\n",
    "a=layers.Add()([b,a])\n",
    "a=layers.Activation(\"relu\")(a)\n",
    "#a=layers.BatchNormalization()(a)\n",
    "\n",
    "a_temp=a\n",
    "a=layers.Conv2D(64,(3,3),strides=1,padding=\"same\",activation=\"relu\")(a) #7\n",
    "a=layers.BatchNormalization()(a)\n",
    "\n",
    "a=layers.Conv2D(64,(3,3),strides=1,padding=\"same\")(a) #8\n",
    "a=layers.BatchNormalization()(a)\n",
    "\n",
    "a=layers.Add()([a,a_temp])\n",
    "a=layers.Activation(\"relu\")(a)\n",
    "#a=layers.BatchNormalization()(a)\n",
    "\n",
    "\n",
    "b=layers.Conv2D(128,(3,3),strides=1,padding=\"same\",activation=\"relu\")(a) #2\n",
    "\n",
    "###############################################################################\n",
    "\n",
    "a=layers.Conv2D(128,(3,3),strides=1,padding=\"same\",activation=\"relu\")(a) #9\n",
    "a=layers.BatchNormalization()(a)\n",
    "\n",
    "a=layers.Conv2D(128,(3,3),strides=1,padding=\"same\")(a) #10\n",
    "a=layers.BatchNormalization()(a)\n",
    "\n",
    "a=layers.Add()([b,a])\n",
    "a=layers.Activation(\"relu\")(a)\n",
    "#a=layers.BatchNormalization()(a)\n",
    "\n",
    "a_temp=a\n",
    "a=layers.Conv2D(128,(3,3),strides=1,padding=\"same\",activation=\"relu\")(a) #11\n",
    "a=layers.BatchNormalization()(a)\n",
    "\n",
    "a=layers.Conv2D(128,(3,3),strides=1,padding=\"same\")(a) #12\n",
    "a=layers.BatchNormalization()(a)\n",
    "\n",
    "a=layers.Add()([a,a_temp])\n",
    "a=layers.Activation(\"relu\")(a)\n",
    "#a=layers.BatchNormalization()(a)\n",
    "\n",
    "\n",
    "b=layers.Conv2D(256,(3,3),strides=1,padding=\"same\",activation=\"relu\")(a) #3\n",
    "\n",
    "#################################################################################\n",
    "\n",
    "a=layers.Conv2D(256,(3,3),strides=1,padding=\"same\",activation=\"relu\")(a) #\n",
    "a=layers.BatchNormalization()(a)\n",
    "\n",
    "a=layers.Conv2D(256,(3,3),strides=1,padding=\"same\")(a) #14\n",
    "a=layers.BatchNormalization()(a)\n",
    "\n",
    "a=layers.Add()([b,a])\n",
    "a=layers.Activation(\"relu\")(a)\n",
    "#a=layers.BatchNormalization()(a)\n",
    "\n",
    "a_temp=a\n",
    "a=layers.Conv2D(256,(3,3),strides=1,padding=\"same\",activation=\"relu\")(a) #15\n",
    "a=layers.BatchNormalization()(a)\n",
    "\n",
    "a=layers.Conv2D(256,(3,3),strides=1,padding=\"same\")(a) #16\n",
    "a=layers.BatchNormalization()(a)\n",
    "\n",
    "a=layers.Add()([a,a_temp])\n",
    "a=layers.Activation(\"relu\")(a)\n",
    "#a=layers.BatchNormalization()(a)\n",
    "\n",
    "############################################################3\n",
    "a=layers.AveragePooling2D(pool_size=(2,2),strides=1,padding=\"same\")(a)\n",
    "a=layers.BatchNormalization()(a)\n",
    "\n",
    "a=Flatten()(a)\n",
    "\n",
    "#############################################################\n",
    "\n",
    "a=layers.Dense(512,activation=\"relu\")(a)\n",
    "a=layers.BatchNormalization()(a)\n",
    "\n",
    "outputs=layers.Dense(y_train.shape[1],activation=\"softmax\")(a)\n",
    "\n",
    "\n",
    "model = keras.Model(inputs, outputs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "id": "2DfQthpnJU2X",
    "outputId": "fbf7972d-cf0e-49f7-e59b-52c11c4ab671"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:`period` argument is deprecated. Please use `save_freq` to specify the frequency in number of batches seen.\n"
     ]
    }
   ],
   "source": [
    "adam= Adam(learning_rate=0.0001,clipvalue=0.6)\n",
    "\n",
    "\n",
    "model.compile(loss=\"categorical_crossentropy\",optimizer=adam,metrics=[\"accuracy\"])\n",
    "call=EarlyStopping(monitor=\"val_loss\",verbose=1,mode=\"auto\",patience=5,restore_best_weights=True)\n",
    "checkpoint=ModelCheckpoint('ADAM_WITH_BATCHNORM_RESNET.hdf5',monitor='val_accuracy',verbose=1,save_best_only=True,save_weights_only=True,model='auto',period=1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "id": "wStUbN-qJX8T",
    "outputId": "109678b9-8649-401a-fc8f-f03c70ab9b90"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From <ipython-input-7-c77f76be1324>:1: Model.fit_generator (from tensorflow.python.keras.engine.training) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please use Model.fit, which supports generators.\n",
      "Epoch 1/80\n",
      "  2/390 [..............................] - ETA: 1:40 - loss: 5.8515 - accuracy: 0.0156WARNING:tensorflow:Callbacks method `on_train_batch_end` is slow compared to the batch time (batch time: 0.1744s vs `on_train_batch_end` time: 0.3441s). Check your callbacks.\n",
      "391/390 [==============================] - ETA: 0s - loss: 3.5620 - accuracy: 0.1801\n",
      "Epoch 00001: val_accuracy improved from -inf to 0.05250, saving model to ADAM_WITH_BATCHNORM_RESNET.hdf5\n",
      "391/390 [==============================] - 224s 573ms/step - loss: 3.5620 - accuracy: 0.1801 - val_loss: 4.6471 - val_accuracy: 0.0525\n",
      "Epoch 2/80\n",
      "391/390 [==============================] - ETA: 0s - loss: 2.9344 - accuracy: 0.2821\n",
      "Epoch 00002: val_accuracy improved from 0.05250 to 0.31100, saving model to ADAM_WITH_BATCHNORM_RESNET.hdf5\n",
      "391/390 [==============================] - 223s 571ms/step - loss: 2.9344 - accuracy: 0.2821 - val_loss: 2.8649 - val_accuracy: 0.3110\n",
      "Epoch 3/80\n",
      "391/390 [==============================] - ETA: 0s - loss: 2.6201 - accuracy: 0.3446\n",
      "Epoch 00003: val_accuracy improved from 0.31100 to 0.34180, saving model to ADAM_WITH_BATCHNORM_RESNET.hdf5\n",
      "391/390 [==============================] - 224s 572ms/step - loss: 2.6201 - accuracy: 0.3446 - val_loss: 2.6384 - val_accuracy: 0.3418\n",
      "Epoch 4/80\n",
      "391/390 [==============================] - ETA: 0s - loss: 2.3678 - accuracy: 0.3963\n",
      "Epoch 00004: val_accuracy improved from 0.34180 to 0.38370, saving model to ADAM_WITH_BATCHNORM_RESNET.hdf5\n",
      "391/390 [==============================] - 224s 572ms/step - loss: 2.3678 - accuracy: 0.3963 - val_loss: 2.5897 - val_accuracy: 0.3837\n",
      "Epoch 5/80\n",
      "391/390 [==============================] - ETA: 0s - loss: 2.1699 - accuracy: 0.4339\n",
      "Epoch 00005: val_accuracy improved from 0.38370 to 0.42960, saving model to ADAM_WITH_BATCHNORM_RESNET.hdf5\n",
      "391/390 [==============================] - 223s 571ms/step - loss: 2.1699 - accuracy: 0.4339 - val_loss: 2.2889 - val_accuracy: 0.4296\n",
      "Epoch 6/80\n",
      "391/390 [==============================] - ETA: 0s - loss: 2.0067 - accuracy: 0.4732\n",
      "Epoch 00006: val_accuracy improved from 0.42960 to 0.43860, saving model to ADAM_WITH_BATCHNORM_RESNET.hdf5\n",
      "391/390 [==============================] - 224s 572ms/step - loss: 2.0067 - accuracy: 0.4732 - val_loss: 2.2629 - val_accuracy: 0.4386\n",
      "Epoch 7/80\n",
      "391/390 [==============================] - ETA: 0s - loss: 1.8624 - accuracy: 0.5069\n",
      "Epoch 00007: val_accuracy improved from 0.43860 to 0.46100, saving model to ADAM_WITH_BATCHNORM_RESNET.hdf5\n",
      "391/390 [==============================] - 223s 571ms/step - loss: 1.8624 - accuracy: 0.5069 - val_loss: 2.1211 - val_accuracy: 0.4610\n",
      "Epoch 8/80\n",
      "391/390 [==============================] - ETA: 0s - loss: 1.7463 - accuracy: 0.5324\n",
      "Epoch 00008: val_accuracy improved from 0.46100 to 0.47170, saving model to ADAM_WITH_BATCHNORM_RESNET.hdf5\n",
      "391/390 [==============================] - 223s 569ms/step - loss: 1.7463 - accuracy: 0.5324 - val_loss: 2.1089 - val_accuracy: 0.4717\n",
      "Epoch 9/80\n",
      "391/390 [==============================] - ETA: 0s - loss: 1.6394 - accuracy: 0.5584\n",
      "Epoch 00009: val_accuracy improved from 0.47170 to 0.52180, saving model to ADAM_WITH_BATCHNORM_RESNET.hdf5\n",
      "391/390 [==============================] - 222s 569ms/step - loss: 1.6394 - accuracy: 0.5584 - val_loss: 1.8326 - val_accuracy: 0.5218\n",
      "Epoch 10/80\n",
      "391/390 [==============================] - ETA: 0s - loss: 1.5392 - accuracy: 0.5819\n",
      "Epoch 00010: val_accuracy did not improve from 0.52180\n",
      "391/390 [==============================] - 220s 563ms/step - loss: 1.5392 - accuracy: 0.5819 - val_loss: 1.8581 - val_accuracy: 0.5192\n",
      "Epoch 11/80\n",
      "391/390 [==============================] - ETA: 0s - loss: 1.4525 - accuracy: 0.6032\n",
      "Epoch 00011: val_accuracy did not improve from 0.52180\n",
      "391/390 [==============================] - 220s 563ms/step - loss: 1.4525 - accuracy: 0.6032 - val_loss: 1.8676 - val_accuracy: 0.5164\n",
      "Epoch 12/80\n",
      "391/390 [==============================] - ETA: 0s - loss: 1.3748 - accuracy: 0.6241\n",
      "Epoch 00012: val_accuracy improved from 0.52180 to 0.53860, saving model to ADAM_WITH_BATCHNORM_RESNET.hdf5\n",
      "391/390 [==============================] - 223s 570ms/step - loss: 1.3748 - accuracy: 0.6241 - val_loss: 1.7796 - val_accuracy: 0.5386\n",
      "Epoch 13/80\n",
      "391/390 [==============================] - ETA: 0s - loss: 1.3067 - accuracy: 0.6397\n",
      "Epoch 00013: val_accuracy did not improve from 0.53860\n",
      "391/390 [==============================] - 220s 564ms/step - loss: 1.3067 - accuracy: 0.6397 - val_loss: 1.8402 - val_accuracy: 0.5327\n",
      "Epoch 14/80\n",
      "391/390 [==============================] - ETA: 0s - loss: 1.2463 - accuracy: 0.6542\n",
      "Epoch 00014: val_accuracy improved from 0.53860 to 0.54930, saving model to ADAM_WITH_BATCHNORM_RESNET.hdf5\n",
      "391/390 [==============================] - 223s 570ms/step - loss: 1.2463 - accuracy: 0.6542 - val_loss: 1.7546 - val_accuracy: 0.5493\n",
      "Epoch 15/80\n",
      "391/390 [==============================] - ETA: 0s - loss: 1.1760 - accuracy: 0.6742\n",
      "Epoch 00015: val_accuracy improved from 0.54930 to 0.55730, saving model to ADAM_WITH_BATCHNORM_RESNET.hdf5\n",
      "391/390 [==============================] - 224s 572ms/step - loss: 1.1760 - accuracy: 0.6742 - val_loss: 1.6837 - val_accuracy: 0.5573\n",
      "Epoch 16/80\n",
      "391/390 [==============================] - ETA: 0s - loss: 1.1123 - accuracy: 0.6911\n",
      "Epoch 00016: val_accuracy did not improve from 0.55730\n",
      "391/390 [==============================] - 220s 564ms/step - loss: 1.1123 - accuracy: 0.6911 - val_loss: 1.7403 - val_accuracy: 0.5557\n",
      "Epoch 17/80\n",
      "391/390 [==============================] - ETA: 0s - loss: 1.0610 - accuracy: 0.7032\n",
      "Epoch 00017: val_accuracy did not improve from 0.55730\n",
      "391/390 [==============================] - 221s 565ms/step - loss: 1.0610 - accuracy: 0.7032 - val_loss: 1.7864 - val_accuracy: 0.5537\n",
      "Epoch 18/80\n",
      "391/390 [==============================] - ETA: 0s - loss: 1.0080 - accuracy: 0.7189\n",
      "Epoch 00018: val_accuracy improved from 0.55730 to 0.57460, saving model to ADAM_WITH_BATCHNORM_RESNET.hdf5\n",
      "391/390 [==============================] - 223s 571ms/step - loss: 1.0080 - accuracy: 0.7189 - val_loss: 1.6574 - val_accuracy: 0.5746\n",
      "Epoch 19/80\n",
      "391/390 [==============================] - ETA: 0s - loss: 0.9579 - accuracy: 0.7300\n",
      "Epoch 00019: val_accuracy did not improve from 0.57460\n",
      "391/390 [==============================] - 221s 565ms/step - loss: 0.9579 - accuracy: 0.7300 - val_loss: 1.8629 - val_accuracy: 0.5426\n",
      "Epoch 20/80\n",
      "391/390 [==============================] - ETA: 0s - loss: 0.9209 - accuracy: 0.7415\n",
      "Epoch 00020: val_accuracy improved from 0.57460 to 0.58150, saving model to ADAM_WITH_BATCHNORM_RESNET.hdf5\n",
      "391/390 [==============================] - 223s 570ms/step - loss: 0.9209 - accuracy: 0.7415 - val_loss: 1.6368 - val_accuracy: 0.5815\n",
      "Epoch 21/80\n",
      "391/390 [==============================] - ETA: 0s - loss: 0.8770 - accuracy: 0.7529\n",
      "Epoch 00021: val_accuracy did not improve from 0.58150\n",
      "391/390 [==============================] - 221s 565ms/step - loss: 0.8770 - accuracy: 0.7529 - val_loss: 1.6827 - val_accuracy: 0.5721\n",
      "Epoch 22/80\n",
      "391/390 [==============================] - ETA: 0s - loss: 0.8382 - accuracy: 0.7655\n",
      "Epoch 00022: val_accuracy did not improve from 0.58150\n",
      "391/390 [==============================] - 221s 564ms/step - loss: 0.8382 - accuracy: 0.7655 - val_loss: 1.8050 - val_accuracy: 0.5516\n",
      "Epoch 23/80\n",
      "391/390 [==============================] - ETA: 0s - loss: 0.7992 - accuracy: 0.7730\n",
      "Epoch 00023: val_accuracy improved from 0.58150 to 0.58300, saving model to ADAM_WITH_BATCHNORM_RESNET.hdf5\n",
      "391/390 [==============================] - 223s 569ms/step - loss: 0.7992 - accuracy: 0.7730 - val_loss: 1.6700 - val_accuracy: 0.5830\n",
      "Epoch 24/80\n",
      "391/390 [==============================] - ETA: 0s - loss: 0.7681 - accuracy: 0.7812\n",
      "Epoch 00024: val_accuracy did not improve from 0.58300\n",
      "391/390 [==============================] - 221s 566ms/step - loss: 0.7681 - accuracy: 0.7812 - val_loss: 1.6539 - val_accuracy: 0.5830\n",
      "Epoch 25/80\n",
      "391/390 [==============================] - ETA: 0s - loss: 0.7358 - accuracy: 0.7907Restoring model weights from the end of the best epoch.\n",
      "\n",
      "Epoch 00025: val_accuracy did not improve from 0.58300\n",
      "391/390 [==============================] - 221s 565ms/step - loss: 0.7358 - accuracy: 0.7907 - val_loss: 1.6919 - val_accuracy: 0.5756\n",
      "Epoch 00025: early stopping\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.keras.callbacks.History at 0x7feefde167b8>"
      ]
     },
     "execution_count": 7,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.fit_generator(aug_data.flow(X_train,y_train,batch_size=128),steps_per_epoch=len(X_train)/128,epochs=80,validation_data=(X_test,y_test),verbose=1,callbacks=[call,checkpoint])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "id": "R_anFGeAgXv6"
   },
   "outputs": [],
   "source": [
    "y_true = y_test.argmax(-1)\n",
    "y_pred = model.predict(X_test).argmax(-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "id": "eKQ8nufegev5",
    "outputId": "8146022f-70cd-4f0f-e6f9-04a2bf81af71"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.83      0.78      0.80       100\n",
      "           1       0.70      0.68      0.69       100\n",
      "           2       0.48      0.48      0.48       100\n",
      "           3       0.48      0.34      0.40       100\n",
      "           4       0.57      0.36      0.44       100\n",
      "           5       0.40      0.68      0.51       100\n",
      "           6       0.63      0.73      0.68       100\n",
      "           7       0.70      0.59      0.64       100\n",
      "           8       0.55      0.73      0.63       100\n",
      "           9       0.78      0.63      0.70       100\n",
      "          10       0.49      0.35      0.41       100\n",
      "          11       0.45      0.42      0.44       100\n",
      "          12       0.62      0.70      0.66       100\n",
      "          13       0.41      0.60      0.49       100\n",
      "          14       0.53      0.57      0.55       100\n",
      "          15       0.42      0.65      0.51       100\n",
      "          16       0.73      0.53      0.61       100\n",
      "          17       0.61      0.77      0.68       100\n",
      "          18       0.60      0.49      0.54       100\n",
      "          19       0.50      0.53      0.52       100\n",
      "          20       0.74      0.83      0.78       100\n",
      "          21       0.84      0.73      0.78       100\n",
      "          22       0.40      0.68      0.51       100\n",
      "          23       0.83      0.57      0.67       100\n",
      "          24       0.84      0.73      0.78       100\n",
      "          25       0.41      0.43      0.42       100\n",
      "          26       0.44      0.71      0.54       100\n",
      "          27       0.57      0.40      0.47       100\n",
      "          28       0.76      0.68      0.72       100\n",
      "          29       0.62      0.60      0.61       100\n",
      "          30       0.51      0.47      0.49       100\n",
      "          31       0.66      0.59      0.62       100\n",
      "          32       0.69      0.55      0.61       100\n",
      "          33       0.53      0.62      0.57       100\n",
      "          34       0.62      0.57      0.59       100\n",
      "          35       0.35      0.43      0.39       100\n",
      "          36       0.71      0.62      0.66       100\n",
      "          37       0.46      0.69      0.55       100\n",
      "          38       0.59      0.26      0.36       100\n",
      "          39       0.51      0.75      0.61       100\n",
      "          40       0.59      0.41      0.49       100\n",
      "          41       0.75      0.77      0.76       100\n",
      "          42       0.40      0.75      0.52       100\n",
      "          43       0.72      0.61      0.66       100\n",
      "          44       0.33      0.23      0.27       100\n",
      "          45       0.49      0.56      0.52       100\n",
      "          46       0.40      0.40      0.40       100\n",
      "          47       0.51      0.68      0.58       100\n",
      "          48       0.68      0.90      0.78       100\n",
      "          49       0.79      0.58      0.67       100\n",
      "          50       0.49      0.31      0.38       100\n",
      "          51       0.68      0.58      0.63       100\n",
      "          52       0.65      0.62      0.64       100\n",
      "          53       0.86      0.77      0.81       100\n",
      "          54       0.75      0.69      0.72       100\n",
      "          55       0.40      0.10      0.16       100\n",
      "          56       0.70      0.85      0.77       100\n",
      "          57       0.80      0.57      0.67       100\n",
      "          58       0.52      0.77      0.62       100\n",
      "          59       0.60      0.56      0.58       100\n",
      "          60       0.85      0.77      0.81       100\n",
      "          61       0.59      0.59      0.59       100\n",
      "          62       0.59      0.62      0.60       100\n",
      "          63       0.47      0.59      0.52       100\n",
      "          64       0.34      0.47      0.39       100\n",
      "          65       0.61      0.20      0.30       100\n",
      "          66       0.42      0.76      0.54       100\n",
      "          67       0.57      0.37      0.45       100\n",
      "          68       0.90      0.84      0.87       100\n",
      "          69       0.68      0.69      0.69       100\n",
      "          70       0.75      0.51      0.61       100\n",
      "          71       0.74      0.69      0.72       100\n",
      "          72       0.33      0.31      0.32       100\n",
      "          73       0.48      0.48      0.48       100\n",
      "          74       0.39      0.48      0.43       100\n",
      "          75       0.75      0.80      0.77       100\n",
      "          76       0.73      0.80      0.76       100\n",
      "          77       0.67      0.39      0.49       100\n",
      "          78       0.41      0.48      0.44       100\n",
      "          79       0.68      0.51      0.58       100\n",
      "          80       0.51      0.33      0.40       100\n",
      "          81       0.54      0.74      0.62       100\n",
      "          82       0.88      0.75      0.81       100\n",
      "          83       0.78      0.39      0.52       100\n",
      "          84       0.64      0.45      0.53       100\n",
      "          85       0.84      0.57      0.68       100\n",
      "          86       0.53      0.66      0.59       100\n",
      "          87       0.43      0.78      0.56       100\n",
      "          88       0.60      0.67      0.64       100\n",
      "          89       0.58      0.65      0.61       100\n",
      "          90       0.72      0.56      0.63       100\n",
      "          91       0.72      0.70      0.71       100\n",
      "          92       0.62      0.44      0.51       100\n",
      "          93       0.46      0.36      0.40       100\n",
      "          94       0.74      0.89      0.81       100\n",
      "          95       0.64      0.63      0.64       100\n",
      "          96       0.67      0.43      0.52       100\n",
      "          97       0.58      0.62      0.60       100\n",
      "          98       0.36      0.33      0.35       100\n",
      "          99       0.54      0.62      0.58       100\n",
      "\n",
      "    accuracy                           0.58     10000\n",
      "   macro avg       0.60      0.58      0.58     10000\n",
      "weighted avg       0.60      0.58      0.58     10000\n",
      "\n",
      "0.5815\n",
      "0.6002942604953259\n",
      "0.5815\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import classification_report,accuracy_score,recall_score,precision_score\n",
    "print(classification_report(Y_test,y_pred))\n",
    "print(accuracy_score(Y_test,y_pred))\n",
    "print(precision_score(Y_test,y_pred,average=\"weighted\"))\n",
    "print(recall_score(Y_test,y_pred,average=\"weighted\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras.layers import Conv2D,Dense,MaxPool2D,AveragePooling2D,Flatten\n",
    "import tensorflow.keras as keras\n",
    "from tensorflow.keras.datasets import cifar100\n",
    "from tensorflow.keras.utils import to_categorical\n",
    "from tensorflow.keras import layers\n",
    "\n",
    "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
    "from tensorflow.keras.optimizers import SGD,Adam\n",
    "\n",
    "from tensorflow.keras.layers import LeakyReLU,BatchNormalization\n",
    "\n",
    "from tensorflow.keras.callbacks import EarlyStopping,ModelCheckpoint\n",
    "\n",
    "(X_train,Y_train),(X_test,Y_test)=cifar100.load_data()\n",
    "\n",
    "X_train,X_test=X_train/255,X_test/255\n",
    "y_train,y_test=to_categorical(Y_train),to_categorical(Y_test)\n",
    "\n",
    "shape=(32,32,3)\n",
    "\n",
    "inputs = keras.Input(shape=shape)\n",
    "\n",
    "a=layers.Conv2D(32,(3,3),strides=1,padding=\"same\",activation=\"relu\")(inputs)  #1\n",
    "a=layers.BatchNormalization()(a)\n",
    "a=layers.Conv2D(32,(3,3),strides=1,padding=\"same\",activation=\"relu\")(a)   #2\n",
    "a=layers.BatchNormalization()(a)\n",
    "a_temp=a\n",
    "a=layers.Conv2D(32,(3,3),strides=1,padding=\"same\",activation=\"relu\")(a)#\\3\n",
    "a=layers.BatchNormalization()(a)\n",
    "a=layers.Conv2D(32,(3,3),strides=1,padding=\"same\")(a)#\n",
    "a=layers.BatchNormalization()(a)\n",
    "\n",
    "a=layers.Add()([a,a_temp])\n",
    "a=layers.Activation(\"relu\")(a)\n",
    "#a=layers.BatchNormalization()(a)\n",
    "\n",
    "\n",
    "b=layers.Conv2D(64,(3,3),strides=1,padding=\"same\",activation=\"relu\")(a) #1\n",
    "                               \n",
    "#a_temp=a\n",
    "##########################################################################\n",
    "a=layers.Conv2D(64,(3,3),strides=1,padding=\"same\",activation=\"relu\")(a) #5\n",
    "a=layers.BatchNormalization()(a)\n",
    "\n",
    "a=layers.Conv2D(64,(3,3),strides=1,padding=\"same\")(a) #6\n",
    "a=layers.BatchNormalization()(a)\n",
    "\n",
    "a=layers.Add()([b,a])\n",
    "a=layers.Activation(\"relu\")(a)\n",
    "#a=layers.BatchNormalization()(a)\n",
    "\n",
    "a_temp=a\n",
    "a=layers.Conv2D(64,(3,3),strides=1,padding=\"same\",activation=\"relu\")(a) #7\n",
    "a=layers.BatchNormalization()(a)\n",
    "\n",
    "a=layers.Conv2D(64,(3,3),strides=1,padding=\"same\")(a) #8\n",
    "a=layers.BatchNormalization()(a)\n",
    "\n",
    "a=layers.Add()([a,a_temp])\n",
    "a=layers.Activation(\"relu\")(a)\n",
    "#a=layers.BatchNormalization()(a)\n",
    "\n",
    "\n",
    "b=layers.Conv2D(128,(3,3),strides=1,padding=\"same\",activation=\"relu\")(a) #2\n",
    "\n",
    "###############################################################################\n",
    "\n",
    "a=layers.Conv2D(128,(3,3),strides=1,padding=\"same\",activation=\"relu\")(a) #9\n",
    "a=layers.BatchNormalization()(a)\n",
    "\n",
    "a=layers.Conv2D(128,(3,3),strides=1,padding=\"same\")(a) #10\n",
    "a=layers.BatchNormalization()(a)\n",
    "\n",
    "a=layers.Add()([b,a])\n",
    "a=layers.Activation(\"relu\")(a)\n",
    "#a=layers.BatchNormalization()(a)\n",
    "\n",
    "a_temp=a\n",
    "a=layers.Conv2D(128,(3,3),strides=1,padding=\"same\",activation=\"relu\")(a) #11\n",
    "a=layers.BatchNormalization()(a)\n",
    "\n",
    "a=layers.Conv2D(128,(3,3),strides=1,padding=\"same\")(a) #12\n",
    "a=layers.BatchNormalization()(a)\n",
    "\n",
    "a=layers.Add()([a,a_temp])\n",
    "a=layers.Activation(\"relu\")(a)\n",
    "#a=layers.BatchNormalization()(a)\n",
    "\n",
    "\n",
    "b=layers.Conv2D(256,(3,3),strides=1,padding=\"same\",activation=\"relu\")(a) #3\n",
    "\n",
    "#################################################################################\n",
    "\n",
    "a=layers.Conv2D(256,(3,3),strides=1,padding=\"same\",activation=\"relu\")(a) #\n",
    "a=layers.BatchNormalization()(a)\n",
    "\n",
    "a=layers.Conv2D(256,(3,3),strides=1,padding=\"same\")(a) #14\n",
    "a=layers.BatchNormalization()(a)\n",
    "\n",
    "a=layers.Add()([b,a])\n",
    "a=layers.Activation(\"relu\")(a)\n",
    "#a=layers.BatchNormalization()(a)\n",
    "\n",
    "a_temp=a\n",
    "a=layers.Conv2D(256,(3,3),strides=1,padding=\"same\",activation=\"relu\")(a) #15\n",
    "a=layers.BatchNormalization()(a)\n",
    "\n",
    "a=layers.Conv2D(256,(3,3),strides=1,padding=\"same\")(a) #16\n",
    "a=layers.BatchNormalization()(a)\n",
    "\n",
    "a=layers.Add()([a,a_temp])\n",
    "a=layers.Activation(\"relu\")(a)\n",
    "#a=layers.BatchNormalization()(a)\n",
    "\n",
    "############################################################3\n",
    "a=layers.AveragePooling2D(pool_size=(2,2),strides=1,padding=\"same\")(a)\n",
    "a=layers.BatchNormalization()(a)\n",
    "\n",
    "a=Flatten()(a)\n",
    "\n",
    "#############################################################\n",
    "\n",
    "a=layers.Dense(512,activation=\"relu\")(a)\n",
    "a=layers.BatchNormalization()(a)\n",
    "\n",
    "outputs=layers.Dense(y_train.shape[1],activation=\"softmax\")(a)\n",
    "\n",
    "\n",
    "model = keras.Model(inputs, outputs)\n",
    "\n",
    "model.load_weights('../weights/ADAM_WITH_BATCHNORM_RESNET.hdf5')\n",
    "\n",
    "y_true = y_test.argmax(-1)\n",
    "y_pred = model.predict(X_test).argmax(-1)\n",
    "\n",
    "from sklearn.metrics import classification_report,accuracy_score,recall_score,precision_score\n",
    "print(classification_report(Y_test,y_pred))\n",
    "print(\"the accuracy obtained is {}\".format(accuracy_score(Y_test,y_pred)))\n",
    "print(\"precision is {}\".format(precision_score(Y_test,y_pred,average=\"weighted\")))\n",
    "print(\"the recall is {}\".format(recall_score(Y_test,y_pred,average=\"weighted\")))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "name": "ADAM_WITH_BATCHNORM_RESNET.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
