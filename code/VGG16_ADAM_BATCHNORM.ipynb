{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 51
    },
    "id": "H-osVVSF_7J8",
    "outputId": "345cb145-fa41-487d-fee2-6030566beba9"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading data from https://www.cs.toronto.edu/~kriz/cifar-100-python.tar.gz\n",
      "169009152/169001437 [==============================] - 4s 0us/step\n"
     ]
    }
   ],
   "source": [
    "from keras.callbacks import ModelCheckpoint, EarlyStopping\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Conv2D, MaxPool2D, Flatten, Dense,BatchNormalization,Activation,Dropout,LeakyReLU\n",
    "from tensorflow.keras.optimizers import SGD,Adam\n",
    "from keras.preprocessing.image import ImageDataGenerator\n",
    "from keras.utils import to_categorical\n",
    "from tensorflow.keras.datasets import cifar100\n",
    "\n",
    "\n",
    "\n",
    "(x_train, Y_train), (x_test, Y_test) = cifar100.load_data()\n",
    "#x_train = x_train.astype('float32') / 255\n",
    "#x_test = x_test.astype('float32') / 255\n",
    "\n",
    "y_train = to_categorical(Y_train)\n",
    "y_test = to_categorical(Y_test)\n",
    "\n",
    "aug_data=ImageDataGenerator(rotation_range=20,horizontal_flip=True,width_shift_range=0.1,shear_range = 0.2,height_shift_range=0.1,zoom_range=0.2,brightness_range = (0.5, 1.5))\n",
    "aug_data.fit(x_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "id": "uaHXGbcZBDeZ",
    "outputId": "e8a21d6a-4219-4ed3-b0d6-6e005d0e0673"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:`period` argument is deprecated. Please use `save_freq` to specify the frequency in number of batches seen.\n"
     ]
    }
   ],
   "source": [
    "\n",
    "#import tensorflow as \n",
    "model = Sequential()\n",
    "\n",
    "# Creating first block- (2 Convolution + 1 Max pool)\n",
    "model.add(Conv2D(filters= 64, kernel_size= (3,3), strides= (1,1), padding='same', input_shape= (32, 32, 3)))\n",
    "model.add(LeakyReLU(alpha=0.1))\n",
    "model.add(BatchNormalization())\n",
    "\n",
    "model.add(Conv2D(filters= 64, kernel_size= (3,3), strides= (1,1), padding='same'))\n",
    "model.add(LeakyReLU(alpha=0.1))\n",
    "model.add(BatchNormalization())\n",
    "model.add(MaxPool2D(pool_size= (2,2), strides=(1,1)))\n",
    "model.add(BatchNormalization())\n",
    "\n",
    "#model.add(Dropout(0.5))\n",
    "\n",
    "# Creating second block- (2 Convolution + 1 Max pool)\n",
    "model.add(Conv2D(filters= 128, kernel_size= (3,3), strides= (1,1), padding='same'))\n",
    "model.add(LeakyReLU(alpha=0.1))\n",
    "model.add(BatchNormalization())\n",
    "\n",
    "model.add(Conv2D(filters= 128, kernel_size= (3,3), strides= (1,1), padding='same'))\n",
    "model.add(LeakyReLU(alpha=0.1))\n",
    "model.add(BatchNormalization())\n",
    "\n",
    "#model.add(BatchNormalization())\n",
    "model.add(MaxPool2D(pool_size= (2,2), strides=(1,1)))\n",
    "model.add(BatchNormalization())\n",
    "\n",
    "#model.add(Dropout(0.5))\n",
    "\n",
    "\n",
    "# Creating third block- (3 Convolution + 1 Max pool)\n",
    "model.add(Conv2D(filters= 256, kernel_size= (3,3), strides= (1,1), padding='same'))\n",
    "model.add(LeakyReLU(alpha=0.1))\n",
    "model.add(BatchNormalization())\n",
    "\n",
    "model.add(Conv2D(filters= 256, kernel_size= (3,3), strides= (1,1), padding='same'))\n",
    "model.add(LeakyReLU(alpha=0.1))\n",
    "model.add(BatchNormalization())\n",
    "\n",
    "model.add(Conv2D(filters= 256, kernel_size= (3,3), strides= (1,1), padding='same'))\n",
    "model.add(LeakyReLU(alpha=0.1))\n",
    "model.add(BatchNormalization())\n",
    "\n",
    "model.add(MaxPool2D(pool_size= (2,2), strides=(1,1)))\n",
    "model.add(BatchNormalization())\n",
    "\n",
    "#model.add(Dropout(0.5))\n",
    "\n",
    "# Creating fourth block- (3 Convolution + 1 Max pool)\n",
    "model.add(Conv2D(filters= 512, kernel_size= (3,3), strides= (1,1), padding='same'))\n",
    "model.add(LeakyReLU(alpha=0.1))\n",
    "model.add(BatchNormalization())\n",
    "\n",
    "model.add(Conv2D(filters= 512, kernel_size= (3,3), strides= (1,1), padding='same'))\n",
    "model.add(LeakyReLU(alpha=0.1))\n",
    "model.add(BatchNormalization())\n",
    "\n",
    "model.add(Conv2D(filters= 512, kernel_size= (3,3), strides= (1,1), padding='same'))\n",
    "model.add(LeakyReLU(alpha=0.1))\n",
    "model.add(BatchNormalization())\n",
    "\n",
    "#model.add(Dropout(0.3))\n",
    "model.add(MaxPool2D(pool_size= (2,2), strides=(2,2)))\n",
    "model.add(BatchNormalization())\n",
    "\n",
    "#model.add(Dropout(0.5))\n",
    "\n",
    "# Creating fifth block- (3 Convolution + 1 Max pool)\n",
    "model.add(Conv2D(filters= 512, kernel_size= (3,3), strides= (1,1), padding='same'))\n",
    "model.add(LeakyReLU(alpha=0.1))\n",
    "model.add(BatchNormalization())\n",
    "\n",
    "model.add(Conv2D(filters= 512, kernel_size= (3,3), strides= (1,1), padding='same'))\n",
    "model.add(LeakyReLU(alpha=0.1))\n",
    "model.add(BatchNormalization())\n",
    "\n",
    "model.add(Conv2D(filters= 512, kernel_size= (3,3), strides= (1,1), padding='same'))\n",
    "model.add(LeakyReLU(alpha=0.1))\n",
    "model.add(BatchNormalization())\n",
    "\n",
    "#model.add(Dropout(0.3))\n",
    "model.add(MaxPool2D(pool_size= (2,2), strides=(1,1)))\n",
    "model.add(BatchNormalization())\n",
    "\n",
    "#model.add(Dropout(0.5))\n",
    "\n",
    "# Flattening the pooled image pixels\n",
    "model.add(Flatten())\n",
    "\n",
    "# Creating 2 Dense Layers\n",
    "model.add(Dense(units= 512))\n",
    "model.add(LeakyReLU(alpha=0.1))\n",
    "model.add(BatchNormalization())\n",
    "\n",
    "model.add(Dropout(0.5))\n",
    "#model.add(Dense(units= 512))\n",
    "#model.add(LeakyReLU(alpha=0.1))\n",
    "#model.add(Dropout(0.5))\n",
    "#model.add(Dropout(0.3))\n",
    "# Creating an output layer\n",
    "model.add(Dense(units= 100, activation='softmax'))\n",
    "'''\n",
    "optimizer = tf.train.AdamOptimizer(learning_rate=0.1)\n",
    "gvs = optimizer.compute_gradients(cost)\n",
    "capped_gvs = [(tf.clip_by_value(grad, -1., 1.), var) for grad, var in gvs]\n",
    "train_op = optimizer.apply_gradients(capped_gvs)\n",
    "'''\n",
    "#adam=Adam(learning_rate=0.0001,clipnorm=1,name='adam')\n",
    "\n",
    "adam=Adam(learning_rate=0.0001,clipnorm=1,name='adam')\n",
    "\n",
    "\n",
    "model.compile(optimizer=adam,loss='categorical_crossentropy',metrics=['accuracy'])\n",
    "\n",
    "checkpoint = ModelCheckpoint(\"VGG_BATCHNORM_ADAM_weights_amit.hdf5\", monitor='val_accuracy', verbose=1, save_best_only=True, save_weights_only=True, mode='auto', period=1)\n",
    "early = EarlyStopping(monitor='val_accuracy', min_delta=0, patience=5, verbose=1, mode='auto',restore_best_weights=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "id": "pAuqnUrdBDak",
    "outputId": "d9b3ddda-7b06-4965-cb4a-b26fe06d4391"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100\n",
      "  2/391 [..............................] - ETA: 2:28 - loss: 5.9871 - accuracy: 0.0156WARNING:tensorflow:Callbacks method `on_train_batch_end` is slow compared to the batch time (batch time: 0.2744s vs `on_train_batch_end` time: 0.4904s). Check your callbacks.\n",
      "391/391 [==============================] - ETA: 0s - loss: 4.2326 - accuracy: 0.1144\n",
      "Epoch 00001: val_accuracy improved from -inf to 0.25540, saving model to VGG_BATCHNORM_ADAM_weights_amit.hdf5\n",
      "391/391 [==============================] - 353s 904ms/step - loss: 4.2326 - accuracy: 0.1144 - val_loss: 3.2488 - val_accuracy: 0.2554\n",
      "Epoch 2/100\n",
      "391/391 [==============================] - ETA: 0s - loss: 3.4721 - accuracy: 0.2145\n",
      "Epoch 00002: val_accuracy improved from 0.25540 to 0.32470, saving model to VGG_BATCHNORM_ADAM_weights_amit.hdf5\n",
      "391/391 [==============================] - 365s 935ms/step - loss: 3.4721 - accuracy: 0.2145 - val_loss: 2.8040 - val_accuracy: 0.3247\n",
      "Epoch 3/100\n",
      "391/391 [==============================] - ETA: 0s - loss: 3.0890 - accuracy: 0.2715\n",
      "Epoch 00003: val_accuracy improved from 0.32470 to 0.38220, saving model to VGG_BATCHNORM_ADAM_weights_amit.hdf5\n",
      "391/391 [==============================] - 370s 947ms/step - loss: 3.0890 - accuracy: 0.2715 - val_loss: 2.4848 - val_accuracy: 0.3822\n",
      "Epoch 4/100\n",
      "391/391 [==============================] - ETA: 0s - loss: 2.8219 - accuracy: 0.3182\n",
      "Epoch 00004: val_accuracy improved from 0.38220 to 0.41560, saving model to VGG_BATCHNORM_ADAM_weights_amit.hdf5\n",
      "391/391 [==============================] - 369s 943ms/step - loss: 2.8219 - accuracy: 0.3182 - val_loss: 2.4585 - val_accuracy: 0.4156\n",
      "Epoch 5/100\n",
      "391/391 [==============================] - ETA: 0s - loss: 2.5956 - accuracy: 0.3561\n",
      "Epoch 00005: val_accuracy improved from 0.41560 to 0.45100, saving model to VGG_BATCHNORM_ADAM_weights_amit.hdf5\n",
      "391/391 [==============================] - 368s 942ms/step - loss: 2.5956 - accuracy: 0.3561 - val_loss: 2.1413 - val_accuracy: 0.4510\n",
      "Epoch 6/100\n",
      "391/391 [==============================] - ETA: 0s - loss: 2.4265 - accuracy: 0.3867\n",
      "Epoch 00006: val_accuracy improved from 0.45100 to 0.47490, saving model to VGG_BATCHNORM_ADAM_weights_amit.hdf5\n",
      "391/391 [==============================] - 369s 945ms/step - loss: 2.4265 - accuracy: 0.3867 - val_loss: 2.0475 - val_accuracy: 0.4749\n",
      "Epoch 7/100\n",
      "391/391 [==============================] - ETA: 0s - loss: 2.2820 - accuracy: 0.4138\n",
      "Epoch 00007: val_accuracy improved from 0.47490 to 0.51510, saving model to VGG_BATCHNORM_ADAM_weights_amit.hdf5\n",
      "391/391 [==============================] - 371s 948ms/step - loss: 2.2820 - accuracy: 0.4138 - val_loss: 1.8294 - val_accuracy: 0.5151\n",
      "Epoch 8/100\n",
      "391/391 [==============================] - ETA: 0s - loss: 2.1467 - accuracy: 0.4433\n",
      "Epoch 00008: val_accuracy improved from 0.51510 to 0.53280, saving model to VGG_BATCHNORM_ADAM_weights_amit.hdf5\n",
      "391/391 [==============================] - 370s 947ms/step - loss: 2.1467 - accuracy: 0.4433 - val_loss: 1.7596 - val_accuracy: 0.5328\n",
      "Epoch 9/100\n",
      "391/391 [==============================] - ETA: 0s - loss: 2.0328 - accuracy: 0.4650\n",
      "Epoch 00009: val_accuracy improved from 0.53280 to 0.53600, saving model to VGG_BATCHNORM_ADAM_weights_amit.hdf5\n",
      "391/391 [==============================] - 369s 944ms/step - loss: 2.0328 - accuracy: 0.4650 - val_loss: 1.7386 - val_accuracy: 0.5360\n",
      "Epoch 10/100\n",
      "391/391 [==============================] - ETA: 0s - loss: 1.9169 - accuracy: 0.4869\n",
      "Epoch 00010: val_accuracy improved from 0.53600 to 0.53990, saving model to VGG_BATCHNORM_ADAM_weights_amit.hdf5\n",
      "391/391 [==============================] - 371s 948ms/step - loss: 1.9169 - accuracy: 0.4869 - val_loss: 1.7328 - val_accuracy: 0.5399\n",
      "Epoch 11/100\n",
      "391/391 [==============================] - ETA: 0s - loss: 1.8267 - accuracy: 0.5111\n",
      "Epoch 00011: val_accuracy improved from 0.53990 to 0.56000, saving model to VGG_BATCHNORM_ADAM_weights_amit.hdf5\n",
      "391/391 [==============================] - 369s 943ms/step - loss: 1.8267 - accuracy: 0.5111 - val_loss: 1.6196 - val_accuracy: 0.5600\n",
      "Epoch 12/100\n",
      "391/391 [==============================] - ETA: 0s - loss: 1.7407 - accuracy: 0.5307\n",
      "Epoch 00012: val_accuracy improved from 0.56000 to 0.57430, saving model to VGG_BATCHNORM_ADAM_weights_amit.hdf5\n",
      "391/391 [==============================] - 370s 947ms/step - loss: 1.7407 - accuracy: 0.5307 - val_loss: 1.5682 - val_accuracy: 0.5743\n",
      "Epoch 13/100\n",
      "391/391 [==============================] - ETA: 0s - loss: 1.6545 - accuracy: 0.5477\n",
      "Epoch 00013: val_accuracy did not improve from 0.57430\n",
      "391/391 [==============================] - 369s 944ms/step - loss: 1.6545 - accuracy: 0.5477 - val_loss: 1.5771 - val_accuracy: 0.5737\n",
      "Epoch 14/100\n",
      "391/391 [==============================] - ETA: 0s - loss: 1.5910 - accuracy: 0.5665\n",
      "Epoch 00014: val_accuracy improved from 0.57430 to 0.58400, saving model to VGG_BATCHNORM_ADAM_weights_amit.hdf5\n",
      "391/391 [==============================] - 369s 944ms/step - loss: 1.5910 - accuracy: 0.5665 - val_loss: 1.5400 - val_accuracy: 0.5840\n",
      "Epoch 15/100\n",
      "391/391 [==============================] - ETA: 0s - loss: 1.5146 - accuracy: 0.5827\n",
      "Epoch 00015: val_accuracy improved from 0.58400 to 0.60320, saving model to VGG_BATCHNORM_ADAM_weights_amit.hdf5\n",
      "391/391 [==============================] - 369s 944ms/step - loss: 1.5146 - accuracy: 0.5827 - val_loss: 1.4678 - val_accuracy: 0.6032\n",
      "Epoch 16/100\n",
      "391/391 [==============================] - ETA: 0s - loss: 1.4502 - accuracy: 0.5990\n",
      "Epoch 00016: val_accuracy improved from 0.60320 to 0.60430, saving model to VGG_BATCHNORM_ADAM_weights_amit.hdf5\n",
      "391/391 [==============================] - 369s 945ms/step - loss: 1.4502 - accuracy: 0.5990 - val_loss: 1.4676 - val_accuracy: 0.6043\n",
      "Epoch 17/100\n",
      "391/391 [==============================] - ETA: 0s - loss: 1.3977 - accuracy: 0.6116\n",
      "Epoch 00017: val_accuracy improved from 0.60430 to 0.63370, saving model to VGG_BATCHNORM_ADAM_weights_amit.hdf5\n",
      "391/391 [==============================] - 370s 945ms/step - loss: 1.3977 - accuracy: 0.6116 - val_loss: 1.3257 - val_accuracy: 0.6337\n",
      "Epoch 18/100\n",
      "391/391 [==============================] - ETA: 0s - loss: 1.3521 - accuracy: 0.6227\n",
      "Epoch 00018: val_accuracy did not improve from 0.63370\n",
      "391/391 [==============================] - 369s 944ms/step - loss: 1.3521 - accuracy: 0.6227 - val_loss: 1.4071 - val_accuracy: 0.6158\n",
      "Epoch 19/100\n",
      "391/391 [==============================] - ETA: 0s - loss: 1.2959 - accuracy: 0.6332\n",
      "Epoch 00019: val_accuracy did not improve from 0.63370\n",
      "391/391 [==============================] - 370s 945ms/step - loss: 1.2959 - accuracy: 0.6332 - val_loss: 1.4497 - val_accuracy: 0.6155\n",
      "Epoch 20/100\n",
      "391/391 [==============================] - ETA: 0s - loss: 1.2552 - accuracy: 0.6439\n",
      "Epoch 00020: val_accuracy did not improve from 0.63370\n",
      "391/391 [==============================] - 370s 945ms/step - loss: 1.2552 - accuracy: 0.6439 - val_loss: 1.3497 - val_accuracy: 0.6323\n",
      "Epoch 21/100\n",
      "391/391 [==============================] - ETA: 0s - loss: 1.2075 - accuracy: 0.6586\n",
      "Epoch 00021: val_accuracy improved from 0.63370 to 0.65270, saving model to VGG_BATCHNORM_ADAM_weights_amit.hdf5\n",
      "391/391 [==============================] - 370s 946ms/step - loss: 1.2075 - accuracy: 0.6586 - val_loss: 1.2742 - val_accuracy: 0.6527\n",
      "Epoch 22/100\n",
      "391/391 [==============================] - ETA: 0s - loss: 1.1666 - accuracy: 0.6653\n",
      "Epoch 00022: val_accuracy improved from 0.65270 to 0.65580, saving model to VGG_BATCHNORM_ADAM_weights_amit.hdf5\n",
      "391/391 [==============================] - 370s 946ms/step - loss: 1.1666 - accuracy: 0.6653 - val_loss: 1.2596 - val_accuracy: 0.6558\n",
      "Epoch 23/100\n",
      "391/391 [==============================] - ETA: 0s - loss: 1.1275 - accuracy: 0.6758\n",
      "Epoch 00023: val_accuracy did not improve from 0.65580\n",
      "391/391 [==============================] - 370s 946ms/step - loss: 1.1275 - accuracy: 0.6758 - val_loss: 1.2860 - val_accuracy: 0.6507\n",
      "Epoch 24/100\n",
      "391/391 [==============================] - ETA: 0s - loss: 1.0912 - accuracy: 0.6880\n",
      "Epoch 00024: val_accuracy improved from 0.65580 to 0.65760, saving model to VGG_BATCHNORM_ADAM_weights_amit.hdf5\n",
      "391/391 [==============================] - 371s 949ms/step - loss: 1.0912 - accuracy: 0.6880 - val_loss: 1.2075 - val_accuracy: 0.6576\n",
      "Epoch 25/100\n",
      "391/391 [==============================] - ETA: 0s - loss: 1.0556 - accuracy: 0.6955\n",
      "Epoch 00025: val_accuracy did not improve from 0.65760\n",
      "391/391 [==============================] - 368s 942ms/step - loss: 1.0556 - accuracy: 0.6955 - val_loss: 1.2573 - val_accuracy: 0.6545\n",
      "Epoch 26/100\n",
      "391/391 [==============================] - ETA: 0s - loss: 1.0154 - accuracy: 0.7058\n",
      "Epoch 00026: val_accuracy improved from 0.65760 to 0.66600, saving model to VGG_BATCHNORM_ADAM_weights_amit.hdf5\n",
      "391/391 [==============================] - 369s 944ms/step - loss: 1.0154 - accuracy: 0.7058 - val_loss: 1.2070 - val_accuracy: 0.6660\n",
      "Epoch 27/100\n",
      "391/391 [==============================] - ETA: 0s - loss: 0.9826 - accuracy: 0.7139\n",
      "Epoch 00027: val_accuracy improved from 0.66600 to 0.67230, saving model to VGG_BATCHNORM_ADAM_weights_amit.hdf5\n",
      "391/391 [==============================] - 370s 945ms/step - loss: 0.9826 - accuracy: 0.7139 - val_loss: 1.1857 - val_accuracy: 0.6723\n",
      "Epoch 28/100\n",
      "391/391 [==============================] - ETA: 0s - loss: 0.9556 - accuracy: 0.7211\n",
      "Epoch 00028: val_accuracy improved from 0.67230 to 0.67970, saving model to VGG_BATCHNORM_ADAM_weights_amit.hdf5\n",
      "391/391 [==============================] - 370s 946ms/step - loss: 0.9556 - accuracy: 0.7211 - val_loss: 1.1613 - val_accuracy: 0.6797\n",
      "Epoch 29/100\n",
      "391/391 [==============================] - ETA: 0s - loss: 0.9248 - accuracy: 0.7291\n",
      "Epoch 00029: val_accuracy did not improve from 0.67970\n",
      "391/391 [==============================] - 368s 940ms/step - loss: 0.9248 - accuracy: 0.7291 - val_loss: 1.1978 - val_accuracy: 0.6747\n",
      "Epoch 30/100\n",
      "391/391 [==============================] - ETA: 0s - loss: 0.8915 - accuracy: 0.7395\n",
      "Epoch 00030: val_accuracy did not improve from 0.67970\n",
      "391/391 [==============================] - 369s 944ms/step - loss: 0.8915 - accuracy: 0.7395 - val_loss: 1.2059 - val_accuracy: 0.6740\n",
      "Epoch 31/100\n",
      "391/391 [==============================] - ETA: 0s - loss: 0.8791 - accuracy: 0.7439\n",
      "Epoch 00031: val_accuracy did not improve from 0.67970\n",
      "391/391 [==============================] - 368s 941ms/step - loss: 0.8791 - accuracy: 0.7439 - val_loss: 1.2048 - val_accuracy: 0.6689\n",
      "Epoch 32/100\n",
      "391/391 [==============================] - ETA: 0s - loss: 0.8448 - accuracy: 0.7499\n",
      "Epoch 00032: val_accuracy improved from 0.67970 to 0.69400, saving model to VGG_BATCHNORM_ADAM_weights_amit.hdf5\n",
      "391/391 [==============================] - 371s 948ms/step - loss: 0.8448 - accuracy: 0.7499 - val_loss: 1.1478 - val_accuracy: 0.6940\n",
      "Epoch 33/100\n",
      "391/391 [==============================] - ETA: 0s - loss: 0.8257 - accuracy: 0.7548\n",
      "Epoch 00033: val_accuracy did not improve from 0.69400\n",
      "391/391 [==============================] - 368s 942ms/step - loss: 0.8257 - accuracy: 0.7548 - val_loss: 1.1566 - val_accuracy: 0.6854\n",
      "Epoch 34/100\n",
      "391/391 [==============================] - ETA: 0s - loss: 0.7982 - accuracy: 0.7626\n",
      "Epoch 00034: val_accuracy did not improve from 0.69400\n",
      "391/391 [==============================] - 368s 941ms/step - loss: 0.7982 - accuracy: 0.7626 - val_loss: 1.1487 - val_accuracy: 0.6862\n",
      "Epoch 35/100\n",
      "391/391 [==============================] - ETA: 0s - loss: 0.7657 - accuracy: 0.7730\n",
      "Epoch 00035: val_accuracy did not improve from 0.69400\n",
      "391/391 [==============================] - 368s 942ms/step - loss: 0.7657 - accuracy: 0.7730 - val_loss: 1.2190 - val_accuracy: 0.6731\n",
      "Epoch 36/100\n",
      "391/391 [==============================] - ETA: 0s - loss: 0.7499 - accuracy: 0.7792\n",
      "Epoch 00036: val_accuracy did not improve from 0.69400\n",
      "391/391 [==============================] - 370s 945ms/step - loss: 0.7499 - accuracy: 0.7792 - val_loss: 1.1521 - val_accuracy: 0.6893\n",
      "Epoch 37/100\n",
      "391/391 [==============================] - ETA: 0s - loss: 0.7292 - accuracy: 0.7809\n",
      "Epoch 00037: val_accuracy did not improve from 0.69400\n",
      "Restoring model weights from the end of the best epoch.\n",
      "391/391 [==============================] - 369s 945ms/step - loss: 0.7292 - accuracy: 0.7809 - val_loss: 1.1375 - val_accuracy: 0.6931\n",
      "Epoch 00037: early stopping\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.keras.callbacks.History at 0x7f794d85cbe0>"
      ]
     },
     "execution_count": 3,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.fit(aug_data.flow(x_train, y_train, batch_size=128), batch_size=128, epochs=100, verbose=1, validation_data=(x_test, y_test),callbacks=[checkpoint,early])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "id": "hfjXrw0JBDWg",
    "outputId": "c507ba54-9cd3-419a-e4fb-88716959aaa7"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From <ipython-input-4-44ed652cd446>:1: Sequential.predict_classes (from tensorflow.python.keras.engine.sequential) is deprecated and will be removed after 2021-01-01.\n",
      "Instructions for updating:\n",
      "Please use instead:* `np.argmax(model.predict(x), axis=-1)`,   if your model does multi-class classification   (e.g. if it uses a `softmax` last-layer activation).* `(model.predict(x) > 0.5).astype(\"int32\")`,   if your model does binary classification   (e.g. if it uses a `sigmoid` last-layer activation).\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.90      0.89      0.89       100\n",
      "           1       0.90      0.86      0.88       100\n",
      "           2       0.68      0.65      0.66       100\n",
      "           3       0.72      0.29      0.41       100\n",
      "           4       0.50      0.51      0.51       100\n",
      "           5       0.63      0.79      0.70       100\n",
      "           6       0.76      0.75      0.75       100\n",
      "           7       0.72      0.68      0.70       100\n",
      "           8       0.67      0.95      0.79       100\n",
      "           9       0.83      0.75      0.79       100\n",
      "          10       0.58      0.49      0.53       100\n",
      "          11       0.55      0.56      0.55       100\n",
      "          12       0.67      0.76      0.71       100\n",
      "          13       0.64      0.60      0.62       100\n",
      "          14       0.53      0.80      0.63       100\n",
      "          15       0.71      0.67      0.69       100\n",
      "          16       0.64      0.69      0.67       100\n",
      "          17       0.83      0.86      0.84       100\n",
      "          18       0.74      0.67      0.71       100\n",
      "          19       0.81      0.55      0.65       100\n",
      "          20       0.87      0.83      0.85       100\n",
      "          21       0.88      0.84      0.86       100\n",
      "          22       0.62      0.79      0.69       100\n",
      "          23       0.82      0.84      0.83       100\n",
      "          24       0.90      0.81      0.85       100\n",
      "          25       0.63      0.59      0.61       100\n",
      "          26       0.65      0.74      0.69       100\n",
      "          27       0.51      0.64      0.57       100\n",
      "          28       0.71      0.77      0.74       100\n",
      "          29       0.76      0.65      0.70       100\n",
      "          30       0.68      0.66      0.67       100\n",
      "          31       0.74      0.64      0.69       100\n",
      "          32       0.78      0.51      0.62       100\n",
      "          33       0.75      0.65      0.70       100\n",
      "          34       0.72      0.77      0.74       100\n",
      "          35       0.57      0.36      0.44       100\n",
      "          36       0.87      0.75      0.81       100\n",
      "          37       0.73      0.73      0.73       100\n",
      "          38       0.75      0.53      0.62       100\n",
      "          39       0.71      0.84      0.77       100\n",
      "          40       0.70      0.62      0.66       100\n",
      "          41       0.91      0.83      0.87       100\n",
      "          42       0.38      0.82      0.52       100\n",
      "          43       0.88      0.59      0.71       100\n",
      "          44       0.49      0.38      0.43       100\n",
      "          45       0.67      0.51      0.58       100\n",
      "          46       0.57      0.51      0.54       100\n",
      "          47       0.69      0.68      0.69       100\n",
      "          48       0.89      0.93      0.91       100\n",
      "          49       0.83      0.82      0.82       100\n",
      "          50       0.56      0.49      0.52       100\n",
      "          51       0.74      0.71      0.72       100\n",
      "          52       0.57      0.73      0.64       100\n",
      "          53       0.88      0.87      0.87       100\n",
      "          54       0.80      0.81      0.81       100\n",
      "          55       0.51      0.22      0.31       100\n",
      "          56       0.87      0.86      0.86       100\n",
      "          57       0.77      0.82      0.79       100\n",
      "          58       0.82      0.80      0.81       100\n",
      "          59       0.72      0.70      0.71       100\n",
      "          60       0.91      0.81      0.86       100\n",
      "          61       0.66      0.75      0.70       100\n",
      "          62       0.80      0.66      0.73       100\n",
      "          63       0.43      0.80      0.56       100\n",
      "          64       0.56      0.52      0.54       100\n",
      "          65       0.75      0.50      0.60       100\n",
      "          66       0.74      0.81      0.78       100\n",
      "          67       0.60      0.54      0.57       100\n",
      "          68       0.86      0.92      0.89       100\n",
      "          69       0.81      0.75      0.78       100\n",
      "          70       0.87      0.76      0.81       100\n",
      "          71       0.82      0.68      0.74       100\n",
      "          72       0.47      0.38      0.42       100\n",
      "          73       0.60      0.54      0.57       100\n",
      "          74       0.34      0.60      0.44       100\n",
      "          75       0.86      0.90      0.88       100\n",
      "          76       0.74      0.91      0.82       100\n",
      "          77       0.62      0.71      0.66       100\n",
      "          78       0.51      0.73      0.60       100\n",
      "          79       0.62      0.80      0.70       100\n",
      "          80       0.58      0.52      0.55       100\n",
      "          81       0.60      0.81      0.69       100\n",
      "          82       0.88      0.87      0.87       100\n",
      "          83       0.81      0.56      0.66       100\n",
      "          84       0.71      0.63      0.67       100\n",
      "          85       0.82      0.81      0.81       100\n",
      "          86       0.61      0.77      0.68       100\n",
      "          87       0.72      0.81      0.76       100\n",
      "          88       0.80      0.67      0.73       100\n",
      "          89       0.75      0.84      0.79       100\n",
      "          90       0.69      0.67      0.68       100\n",
      "          91       0.82      0.80      0.81       100\n",
      "          92       0.78      0.69      0.73       100\n",
      "          93       0.66      0.57      0.61       100\n",
      "          94       0.80      0.91      0.85       100\n",
      "          95       0.57      0.65      0.61       100\n",
      "          96       0.72      0.46      0.56       100\n",
      "          97       0.86      0.65      0.74       100\n",
      "          98       0.49      0.56      0.52       100\n",
      "          99       0.64      0.67      0.66       100\n",
      "\n",
      "    accuracy                           0.69     10000\n",
      "   macro avg       0.71      0.69      0.69     10000\n",
      "weighted avg       0.71      0.69      0.69     10000\n",
      "\n",
      "Accuracy is 0.694\n",
      "Precision is 0.7081820365801569\n",
      "Recall is 0.694\n"
     ]
    }
   ],
   "source": [
    "y_pred=model.predict_classes(x_test)\n",
    "from sklearn.metrics import classification_report,accuracy_score,recall_score,precision_score\n",
    "print(classification_report(Y_test,y_pred))\n",
    "print(\"Accuracy is {}\".format(accuracy_score(Y_test,y_pred)))\n",
    "print(\"Precision is {}\".format(precision_score(Y_test,y_pred,average=\"weighted\")))\n",
    "print(\"Recall is {}\".format(recall_score(Y_test,y_pred,average=\"weighted\")))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "id": "fwy6y3DtBDT_",
    "outputId": "6f21f309-9c9d-4284-a80c-ddacd89af43f"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "conv2d (Conv2D)              (None, 32, 32, 64)        1792      \n",
      "_________________________________________________________________\n",
      "leaky_re_lu (LeakyReLU)      (None, 32, 32, 64)        0         \n",
      "_________________________________________________________________\n",
      "batch_normalization (BatchNo (None, 32, 32, 64)        256       \n",
      "_________________________________________________________________\n",
      "conv2d_1 (Conv2D)            (None, 32, 32, 64)        36928     \n",
      "_________________________________________________________________\n",
      "leaky_re_lu_1 (LeakyReLU)    (None, 32, 32, 64)        0         \n",
      "_________________________________________________________________\n",
      "batch_normalization_1 (Batch (None, 32, 32, 64)        256       \n",
      "_________________________________________________________________\n",
      "max_pooling2d (MaxPooling2D) (None, 31, 31, 64)        0         \n",
      "_________________________________________________________________\n",
      "batch_normalization_2 (Batch (None, 31, 31, 64)        256       \n",
      "_________________________________________________________________\n",
      "conv2d_2 (Conv2D)            (None, 31, 31, 128)       73856     \n",
      "_________________________________________________________________\n",
      "leaky_re_lu_2 (LeakyReLU)    (None, 31, 31, 128)       0         \n",
      "_________________________________________________________________\n",
      "batch_normalization_3 (Batch (None, 31, 31, 128)       512       \n",
      "_________________________________________________________________\n",
      "conv2d_3 (Conv2D)            (None, 31, 31, 128)       147584    \n",
      "_________________________________________________________________\n",
      "leaky_re_lu_3 (LeakyReLU)    (None, 31, 31, 128)       0         \n",
      "_________________________________________________________________\n",
      "batch_normalization_4 (Batch (None, 31, 31, 128)       512       \n",
      "_________________________________________________________________\n",
      "max_pooling2d_1 (MaxPooling2 (None, 30, 30, 128)       0         \n",
      "_________________________________________________________________\n",
      "batch_normalization_5 (Batch (None, 30, 30, 128)       512       \n",
      "_________________________________________________________________\n",
      "conv2d_4 (Conv2D)            (None, 30, 30, 256)       295168    \n",
      "_________________________________________________________________\n",
      "leaky_re_lu_4 (LeakyReLU)    (None, 30, 30, 256)       0         \n",
      "_________________________________________________________________\n",
      "batch_normalization_6 (Batch (None, 30, 30, 256)       1024      \n",
      "_________________________________________________________________\n",
      "conv2d_5 (Conv2D)            (None, 30, 30, 256)       590080    \n",
      "_________________________________________________________________\n",
      "leaky_re_lu_5 (LeakyReLU)    (None, 30, 30, 256)       0         \n",
      "_________________________________________________________________\n",
      "batch_normalization_7 (Batch (None, 30, 30, 256)       1024      \n",
      "_________________________________________________________________\n",
      "conv2d_6 (Conv2D)            (None, 30, 30, 256)       590080    \n",
      "_________________________________________________________________\n",
      "leaky_re_lu_6 (LeakyReLU)    (None, 30, 30, 256)       0         \n",
      "_________________________________________________________________\n",
      "batch_normalization_8 (Batch (None, 30, 30, 256)       1024      \n",
      "_________________________________________________________________\n",
      "max_pooling2d_2 (MaxPooling2 (None, 29, 29, 256)       0         \n",
      "_________________________________________________________________\n",
      "batch_normalization_9 (Batch (None, 29, 29, 256)       1024      \n",
      "_________________________________________________________________\n",
      "conv2d_7 (Conv2D)            (None, 29, 29, 512)       1180160   \n",
      "_________________________________________________________________\n",
      "leaky_re_lu_7 (LeakyReLU)    (None, 29, 29, 512)       0         \n",
      "_________________________________________________________________\n",
      "batch_normalization_10 (Batc (None, 29, 29, 512)       2048      \n",
      "_________________________________________________________________\n",
      "conv2d_8 (Conv2D)            (None, 29, 29, 512)       2359808   \n",
      "_________________________________________________________________\n",
      "leaky_re_lu_8 (LeakyReLU)    (None, 29, 29, 512)       0         \n",
      "_________________________________________________________________\n",
      "batch_normalization_11 (Batc (None, 29, 29, 512)       2048      \n",
      "_________________________________________________________________\n",
      "conv2d_9 (Conv2D)            (None, 29, 29, 512)       2359808   \n",
      "_________________________________________________________________\n",
      "leaky_re_lu_9 (LeakyReLU)    (None, 29, 29, 512)       0         \n",
      "_________________________________________________________________\n",
      "batch_normalization_12 (Batc (None, 29, 29, 512)       2048      \n",
      "_________________________________________________________________\n",
      "max_pooling2d_3 (MaxPooling2 (None, 14, 14, 512)       0         \n",
      "_________________________________________________________________\n",
      "batch_normalization_13 (Batc (None, 14, 14, 512)       2048      \n",
      "_________________________________________________________________\n",
      "conv2d_10 (Conv2D)           (None, 14, 14, 512)       2359808   \n",
      "_________________________________________________________________\n",
      "leaky_re_lu_10 (LeakyReLU)   (None, 14, 14, 512)       0         \n",
      "_________________________________________________________________\n",
      "batch_normalization_14 (Batc (None, 14, 14, 512)       2048      \n",
      "_________________________________________________________________\n",
      "conv2d_11 (Conv2D)           (None, 14, 14, 512)       2359808   \n",
      "_________________________________________________________________\n",
      "leaky_re_lu_11 (LeakyReLU)   (None, 14, 14, 512)       0         \n",
      "_________________________________________________________________\n",
      "batch_normalization_15 (Batc (None, 14, 14, 512)       2048      \n",
      "_________________________________________________________________\n",
      "conv2d_12 (Conv2D)           (None, 14, 14, 512)       2359808   \n",
      "_________________________________________________________________\n",
      "leaky_re_lu_12 (LeakyReLU)   (None, 14, 14, 512)       0         \n",
      "_________________________________________________________________\n",
      "batch_normalization_16 (Batc (None, 14, 14, 512)       2048      \n",
      "_________________________________________________________________\n",
      "max_pooling2d_4 (MaxPooling2 (None, 13, 13, 512)       0         \n",
      "_________________________________________________________________\n",
      "batch_normalization_17 (Batc (None, 13, 13, 512)       2048      \n",
      "_________________________________________________________________\n",
      "flatten (Flatten)            (None, 86528)             0         \n",
      "_________________________________________________________________\n",
      "dense (Dense)                (None, 512)               44302848  \n",
      "_________________________________________________________________\n",
      "leaky_re_lu_13 (LeakyReLU)   (None, 512)               0         \n",
      "_________________________________________________________________\n",
      "batch_normalization_18 (Batc (None, 512)               2048      \n",
      "_________________________________________________________________\n",
      "dropout (Dropout)            (None, 512)               0         \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 100)               51300     \n",
      "=================================================================\n",
      "Total params: 59,093,668\n",
      "Trainable params: 59,081,252\n",
      "Non-trainable params: 12,416\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "id": "3UqB7TyOBDRc"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From C:\\Users\\uamit\\Anaconda3\\lib\\site-packages\\keras\\backend\\tensorflow_backend.py:4070: The name tf.nn.max_pool is deprecated. Please use tf.nn.max_pool2d instead.\n",
      "\n"
     ]
    },
    {
     "ename": "OSError",
     "evalue": "Unable to open file (file signature not found)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mOSError\u001b[0m                                   Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-1-993459f66564>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m    127\u001b[0m '''\n\u001b[0;32m    128\u001b[0m \u001b[1;31m#adam=Adam(learning_rate=0.0001,clipnorm=1,name='adam')\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 129\u001b[1;33m \u001b[0mmodel\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mload_weights\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"../weights/VGG_ADAM_BATCHNORM.ipynb\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    130\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    131\u001b[0m \u001b[0my_true\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0my_test\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0margmax\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m-\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\keras\\engine\\saving.py\u001b[0m in \u001b[0;36mload_wrapper\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    490\u001b[0m                 \u001b[0mos\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mremove\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtmp_filepath\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    491\u001b[0m             \u001b[1;32mreturn\u001b[0m \u001b[0mres\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 492\u001b[1;33m         \u001b[1;32mreturn\u001b[0m \u001b[0mload_function\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    493\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    494\u001b[0m     \u001b[1;32mreturn\u001b[0m \u001b[0mload_wrapper\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\keras\\engine\\network.py\u001b[0m in \u001b[0;36mload_weights\u001b[1;34m(self, filepath, by_name, skip_mismatch, reshape)\u001b[0m\n\u001b[0;32m   1219\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mh5py\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1220\u001b[0m             \u001b[1;32mraise\u001b[0m \u001b[0mImportError\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'`load_weights` requires h5py.'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1221\u001b[1;33m         \u001b[1;32mwith\u001b[0m \u001b[0mh5py\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mFile\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfilepath\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmode\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m'r'\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0mf\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1222\u001b[0m             \u001b[1;32mif\u001b[0m \u001b[1;34m'layer_names'\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mf\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mattrs\u001b[0m \u001b[1;32mand\u001b[0m \u001b[1;34m'model_weights'\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mf\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1223\u001b[0m                 \u001b[0mf\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mf\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'model_weights'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\h5py\\_hl\\files.py\u001b[0m in \u001b[0;36m__init__\u001b[1;34m(self, name, mode, driver, libver, userblock_size, swmr, rdcc_nslots, rdcc_nbytes, rdcc_w0, track_order, **kwds)\u001b[0m\n\u001b[0;32m    392\u001b[0m                 fid = make_fid(name, mode, userblock_size,\n\u001b[0;32m    393\u001b[0m                                \u001b[0mfapl\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfcpl\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mmake_fcpl\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtrack_order\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mtrack_order\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 394\u001b[1;33m                                swmr=swmr)\n\u001b[0m\u001b[0;32m    395\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    396\u001b[0m             \u001b[1;32mif\u001b[0m \u001b[0mswmr_support\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\h5py\\_hl\\files.py\u001b[0m in \u001b[0;36mmake_fid\u001b[1;34m(name, mode, userblock_size, fapl, fcpl, swmr)\u001b[0m\n\u001b[0;32m    168\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mswmr\u001b[0m \u001b[1;32mand\u001b[0m \u001b[0mswmr_support\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    169\u001b[0m             \u001b[0mflags\u001b[0m \u001b[1;33m|=\u001b[0m \u001b[0mh5f\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mACC_SWMR_READ\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 170\u001b[1;33m         \u001b[0mfid\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mh5f\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mopen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mname\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mflags\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfapl\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mfapl\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    171\u001b[0m     \u001b[1;32melif\u001b[0m \u001b[0mmode\u001b[0m \u001b[1;33m==\u001b[0m \u001b[1;34m'r+'\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    172\u001b[0m         \u001b[0mfid\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mh5f\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mopen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mname\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mh5f\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mACC_RDWR\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfapl\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mfapl\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mh5py\\_objects.pyx\u001b[0m in \u001b[0;36mh5py._objects.with_phil.wrapper\u001b[1;34m()\u001b[0m\n",
      "\u001b[1;32mh5py\\_objects.pyx\u001b[0m in \u001b[0;36mh5py._objects.with_phil.wrapper\u001b[1;34m()\u001b[0m\n",
      "\u001b[1;32mh5py\\h5f.pyx\u001b[0m in \u001b[0;36mh5py.h5f.open\u001b[1;34m()\u001b[0m\n",
      "\u001b[1;31mOSError\u001b[0m: Unable to open file (file signature not found)"
     ]
    }
   ],
   "source": [
    "from keras.callbacks import ModelCheckpoint, EarlyStopping\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Conv2D, MaxPool2D, Flatten, Dense,BatchNormalization,Activation,Dropout,LeakyReLU\n",
    "from tensorflow.keras.optimizers import SGD,Adam\n",
    "from keras.preprocessing.image import ImageDataGenerator\n",
    "from keras.utils import to_categorical\n",
    "from tensorflow.keras.datasets import cifar100\n",
    "\n",
    "\n",
    "\n",
    "(x_train, Y_train), (x_test, Y_test) = cifar100.load_data()\n",
    "#x_train = x_train.astype('float32') / 255\n",
    "#x_test = x_test.astype('float32') / 255\n",
    "\n",
    "y_train = to_categorical(Y_train)\n",
    "y_test = to_categorical(Y_test)\n",
    "\n",
    "\n",
    "#import tensorflow as \n",
    "model = Sequential()\n",
    "\n",
    "# Creating first block- (2 Convolution + 1 Max pool)\n",
    "model.add(Conv2D(filters= 64, kernel_size= (3,3), strides= (1,1), padding='same', input_shape= (32, 32, 3)))\n",
    "model.add(LeakyReLU(alpha=0.1))\n",
    "model.add(BatchNormalization())\n",
    "\n",
    "model.add(Conv2D(filters= 64, kernel_size= (3,3), strides= (1,1), padding='same'))\n",
    "model.add(LeakyReLU(alpha=0.1))\n",
    "model.add(BatchNormalization())\n",
    "model.add(MaxPool2D(pool_size= (2,2), strides=(1,1)))\n",
    "model.add(BatchNormalization())\n",
    "\n",
    "#model.add(Dropout(0.5))\n",
    "\n",
    "# Creating second block- (2 Convolution + 1 Max pool)\n",
    "model.add(Conv2D(filters= 128, kernel_size= (3,3), strides= (1,1), padding='same'))\n",
    "model.add(LeakyReLU(alpha=0.1))\n",
    "model.add(BatchNormalization())\n",
    "\n",
    "model.add(Conv2D(filters= 128, kernel_size= (3,3), strides= (1,1), padding='same'))\n",
    "model.add(LeakyReLU(alpha=0.1))\n",
    "model.add(BatchNormalization())\n",
    "\n",
    "#model.add(BatchNormalization())\n",
    "model.add(MaxPool2D(pool_size= (2,2), strides=(1,1)))\n",
    "model.add(BatchNormalization())\n",
    "\n",
    "#model.add(Dropout(0.5))\n",
    "\n",
    "\n",
    "# Creating third block- (3 Convolution + 1 Max pool)\n",
    "model.add(Conv2D(filters= 256, kernel_size= (3,3), strides= (1,1), padding='same'))\n",
    "model.add(LeakyReLU(alpha=0.1))\n",
    "model.add(BatchNormalization())\n",
    "\n",
    "model.add(Conv2D(filters= 256, kernel_size= (3,3), strides= (1,1), padding='same'))\n",
    "model.add(LeakyReLU(alpha=0.1))\n",
    "model.add(BatchNormalization())\n",
    "\n",
    "model.add(Conv2D(filters= 256, kernel_size= (3,3), strides= (1,1), padding='same'))\n",
    "model.add(LeakyReLU(alpha=0.1))\n",
    "model.add(BatchNormalization())\n",
    "\n",
    "model.add(MaxPool2D(pool_size= (2,2), strides=(1,1)))\n",
    "model.add(BatchNormalization())\n",
    "\n",
    "#model.add(Dropout(0.5))\n",
    "\n",
    "# Creating fourth block- (3 Convolution + 1 Max pool)\n",
    "model.add(Conv2D(filters= 512, kernel_size= (3,3), strides= (1,1), padding='same'))\n",
    "model.add(LeakyReLU(alpha=0.1))\n",
    "model.add(BatchNormalization())\n",
    "\n",
    "model.add(Conv2D(filters= 512, kernel_size= (3,3), strides= (1,1), padding='same'))\n",
    "model.add(LeakyReLU(alpha=0.1))\n",
    "model.add(BatchNormalization())\n",
    "\n",
    "model.add(Conv2D(filters= 512, kernel_size= (3,3), strides= (1,1), padding='same'))\n",
    "model.add(LeakyReLU(alpha=0.1))\n",
    "model.add(BatchNormalization())\n",
    "\n",
    "#model.add(Dropout(0.3))\n",
    "model.add(MaxPool2D(pool_size= (2,2), strides=(2,2)))\n",
    "model.add(BatchNormalization())\n",
    "\n",
    "#model.add(Dropout(0.5))\n",
    "\n",
    "# Creating fifth block- (3 Convolution + 1 Max pool)\n",
    "model.add(Conv2D(filters= 512, kernel_size= (3,3), strides= (1,1), padding='same'))\n",
    "model.add(LeakyReLU(alpha=0.1))\n",
    "model.add(BatchNormalization())\n",
    "\n",
    "model.add(Conv2D(filters= 512, kernel_size= (3,3), strides= (1,1), padding='same'))\n",
    "model.add(LeakyReLU(alpha=0.1))\n",
    "model.add(BatchNormalization())\n",
    "\n",
    "model.add(Conv2D(filters= 512, kernel_size= (3,3), strides= (1,1), padding='same'))\n",
    "model.add(LeakyReLU(alpha=0.1))\n",
    "model.add(BatchNormalization())\n",
    "\n",
    "#model.add(Dropout(0.3))\n",
    "model.add(MaxPool2D(pool_size= (2,2), strides=(1,1)))\n",
    "model.add(BatchNormalization())\n",
    "\n",
    "#model.add(Dropout(0.5))\n",
    "\n",
    "# Flattening the pooled image pixels\n",
    "model.add(Flatten())\n",
    "\n",
    "# Creating 2 Dense Layers\n",
    "model.add(Dense(units= 512))\n",
    "model.add(LeakyReLU(alpha=0.1))\n",
    "model.add(BatchNormalization())\n",
    "\n",
    "model.add(Dropout(0.5))\n",
    "#model.add(Dense(units= 512))\n",
    "#model.add(LeakyReLU(alpha=0.1))\n",
    "#model.add(Dropout(0.5))\n",
    "#model.add(Dropout(0.3))\n",
    "# Creating an output layer\n",
    "model.add(Dense(units= 100, activation='softmax'))\n",
    "'''\n",
    "optimizer = tf.train.AdamOptimizer(learning_rate=0.1)\n",
    "gvs = optimizer.compute_gradients(cost)\n",
    "capped_gvs = [(tf.clip_by_value(grad, -1., 1.), var) for grad, var in gvs]\n",
    "train_op = optimizer.apply_gradients(capped_gvs)\n",
    "'''\n",
    "#adam=Adam(learning_rate=0.0001,clipnorm=1,name='adam')\n",
    "model.load_weights(\"../weights/V\")\n",
    "\n",
    "y_true = y_test.argmax(-1)\n",
    "y_pred = model.predict(X_test).argmax(-1)\n",
    "\n",
    "from sklearn.metrics import classification_report,accuracy_score,recall_score,precision_score\n",
    "print(classification_report(Y_test,y_pred))\n",
    "print(\"Accuracy is {}\".format(accuracy_score(Y_test,y_pred)))\n",
    "print(\"Precision is {}\".format(precision_score(Y_test,y_pred,average=\"weighted\")))\n",
    "print(\"Recall is {}\".format(recall_score(Y_test,y_pred,average=\"weighted\")))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "id": "MEWFywPDBDJF"
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "name": "VGG_ADAM_BATCHNORM.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
