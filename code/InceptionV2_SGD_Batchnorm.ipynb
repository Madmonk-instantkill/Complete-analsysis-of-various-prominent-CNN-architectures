{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "id": "_cHN953igZba"
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from tensorflow.keras.models import Sequential,Model\n",
    "from tensorflow.keras.layers import Dense,Flatten,Input,Conv2D,MaxPool2D,concatenate,BatchNormalization\n",
    "from keras.layers.convolutional import MaxPooling2D\n",
    "from keras.layers.merge import concatenate\n",
    "from keras.layers import Concatenate, Dense, LSTM, Input, concatenate\n",
    "from keras.layers import Conv2D\n",
    "\n",
    "from tensorflow.keras import layers\n",
    "\n",
    "import tensorflow.keras as keras\n",
    "\n",
    "import tensorflow.keras as keras\n",
    "\n",
    "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
    "\n",
    "from tensorflow.keras.layers import LeakyReLU\n",
    "\n",
    "from tensorflow.keras.datasets import cifar100\n",
    "from tensorflow.keras.utils import to_categorical\n",
    "\n",
    "from tensorflow.keras.callbacks import EarlyStopping,ModelCheckpoint\n",
    "\n",
    "from keras.optimizers import Adam,SGD\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "id": "6FeH3xVTi2an"
   },
   "outputs": [],
   "source": [
    "(X_train,Y_train),(X_test,Y_test)=cifar100.load_data()\n",
    "y_train,y_test=to_categorical(Y_train),to_categorical(Y_test)\n",
    "X_train,X_test=X_train/255,X_test/255\n",
    "\n",
    "aug_data=ImageDataGenerator(rotation_range=15,horizontal_flip=True,width_shift_range=0.1,height_shift_range=0.1,zoom_range=0.3)\n",
    "aug_data.fit(X_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "id": "3MzOf40Zi6R5",
    "outputId": "dcf27740-2358-4c48-a56c-412c599e8657"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:`period` argument is deprecated. Please use `save_freq` to specify the frequency in number of batches seen.\n"
     ]
    }
   ],
   "source": [
    "\n",
    "inputs=keras.Input(shape=(32,32,3))\n",
    "\n",
    "a=Conv2D(filters=32,kernel_size=(3,3),padding=\"same\",strides=2,activation=LeakyReLU(alpha=0.1))(inputs)\n",
    "a=layers.BatchNormalization()(a)\n",
    "\n",
    "\n",
    "a=Conv2D(filters=32,kernel_size=(3,3),padding=\"same\",strides=1,activation=LeakyReLU(alpha=0.1))(a)\n",
    "a=layers.BatchNormalization()(a)\n",
    "\n",
    "\n",
    "a=Conv2D(filters=32, kernel_size=(3,3), padding=\"same\",strides=1,activation=LeakyReLU(alpha=0.1))(a)\n",
    "a=layers.BatchNormalization()(a)\n",
    "\n",
    "a=(MaxPool2D(pool_size=(3,3),strides=(2,2)))(a)\n",
    "\n",
    "\n",
    "a=Conv2D(filters=32, kernel_size=(3,3), padding=\"same\",strides=1,activation=LeakyReLU(alpha=0.1))(a)\n",
    "a=layers.BatchNormalization()(a)\n",
    "\n",
    "\n",
    "a=Conv2D(filters=32, kernel_size=(3,3), padding=\"same\",strides=2,activation=LeakyReLU(alpha=0.1))(a)\n",
    "a=layers.BatchNormalization()(a)\n",
    "\n",
    "\n",
    "a=Conv2D(filters=32, kernel_size=(3,3), padding=\"same\",strides=1,activation=LeakyReLU(alpha=0.1))(a)\n",
    "a=layers.BatchNormalization()(a)\n",
    "\n",
    "\n",
    "#inception-1\n",
    "b=Conv2D(64, (1,1), padding='same',strides=1,activation=LeakyReLU(alpha=0.1))(a)\n",
    "b=layers.BatchNormalization()(b)\n",
    "\n",
    "b=Conv2D(64, (3,3), padding='same',strides=1 ,activation=LeakyReLU(alpha=0.1))(b)\n",
    "b=layers.BatchNormalization()(b)\n",
    "\n",
    "b=Conv2D(64, (3,3), padding='same', strides=1,activation=LeakyReLU(alpha=0.1))(b)\n",
    "b=layers.BatchNormalization()(b)\n",
    "\n",
    "\n",
    "\n",
    "c=Conv2D(64, (1,1), padding='same', strides=1, activation=LeakyReLU(alpha=0.1))(a)\n",
    "c=layers.BatchNormalization()(c)\n",
    "\n",
    "c=Conv2D(64, (3,3), padding='same', strides=1, activation=LeakyReLU(alpha=0.1))(c)\n",
    "c=layers.BatchNormalization()(c)\n",
    "\n",
    "\n",
    "d=MaxPooling2D((3,3), strides=(1,1), padding='same')(a)\n",
    "d=layers.BatchNormalization()(d)\n",
    "\n",
    "d=Conv2D(64, (1,1), padding='same', strides=1, activation=LeakyReLU(alpha=0.1))(d)\n",
    "d=layers.BatchNormalization()(d)\n",
    "\n",
    "\n",
    "e=Conv2D(64, (1,1), padding='same', strides=1, activation=LeakyReLU(alpha=0.1))(a)\n",
    "e=layers.BatchNormalization()(e)\n",
    "\n",
    "\n",
    "a=concatenate([b, c, d,e],axis=3)\n",
    "\n",
    "#inception-2\n",
    "f=Conv2D(128, (1,1), padding='same', strides=1, activation=LeakyReLU(alpha=0.1))(a)\n",
    "f=layers.BatchNormalization()(f)\n",
    "\n",
    "f=Conv2D(128, (1,7), padding='same', strides=1, activation=LeakyReLU(alpha=0.1))(f)\n",
    "f=layers.BatchNormalization()(f)\n",
    "\n",
    "\n",
    "f=Conv2D(128, (7,1), padding='same', strides=1, activation=LeakyReLU(alpha=0.1))(f)\n",
    "f=layers.BatchNormalization()(f)\n",
    "\n",
    "f=Conv2D(128, (1,7), padding='same', strides=1, activation=LeakyReLU(alpha=0.1))(f)\n",
    "f=layers.BatchNormalization()(f)\n",
    "\n",
    "\n",
    "f=Conv2D(128, (7,1), padding='same', strides=1, activation=LeakyReLU(alpha=0.1))(f)\n",
    "f=layers.BatchNormalization()(f)\n",
    "\n",
    "\n",
    "g=Conv2D(128, (1,1), padding='same', strides=1, activation=LeakyReLU(alpha=0.1))(a)\n",
    "g=layers.BatchNormalization()(g)\n",
    "\n",
    "g=Conv2D(128, (1,7), padding='same', strides=1,activation=LeakyReLU(alpha=0.1))(g)\n",
    "g=layers.BatchNormalization()(g)\n",
    "\n",
    "g=Conv2D(128, (7,1), padding='same', strides=1, activation=LeakyReLU(alpha=0.1))(g)\n",
    "\n",
    "g=layers.BatchNormalization()(g)\n",
    "\n",
    "\n",
    "h=MaxPooling2D((3,3), strides=(1,1), padding='same')(a)\n",
    "h=layers.BatchNormalization()(h)\n",
    "\n",
    "h=Conv2D(128, (1,1), padding='same', strides=1, activation=LeakyReLU(alpha=0.1))(h)\n",
    "h=layers.BatchNormalization()(h)\n",
    "\n",
    "\n",
    "i=Conv2D(128, (1,1), padding='same', strides=1, activation=LeakyReLU(alpha=0.1))(h)\n",
    "i=layers.BatchNormalization()(i)\n",
    "\n",
    "\n",
    "a=concatenate([f,g,h,i],axis=3)\n",
    "\n",
    "#inception-3\n",
    "j=Conv2D(256, (1,1), padding='same', strides=1, activation=LeakyReLU(alpha=0.1))(a)\n",
    "j=layers.BatchNormalization()(j)\n",
    "\n",
    "\n",
    "j=Conv2D(256, (3,3), padding='same', strides=1, activation=LeakyReLU(alpha=0.1))(j)\n",
    "j=layers.BatchNormalization()(j)\n",
    "\n",
    "k=Conv2D(256, (1,3), padding='same', strides=1, activation=LeakyReLU(alpha=0.1))(j)\n",
    "#k=layers.BatchNormalization()(k)\n",
    "\n",
    "l=Conv2D(256, (3,1), padding='same', strides=1, activation=LeakyReLU(alpha=0.1))(j)\n",
    "#l=layers.BatchNormalization()(l)\n",
    "\n",
    "\n",
    "m=Conv2D(256, (1,1), padding='same', activation=LeakyReLU(alpha=0.1))(a)\n",
    "m=layers.BatchNormalization()(m)\n",
    "\n",
    "n=Conv2D(256, (1,3), padding='same', activation=LeakyReLU(alpha=0.1))(m)\n",
    "#n=layers.BatchNormalization()(n)\n",
    "\n",
    "o=Conv2D(256, (3,1), padding='same', activation=LeakyReLU(alpha=0.1))(m)\n",
    "#o=layers.BatchNormalization()(o)\n",
    "\n",
    "\n",
    "p=MaxPool2D((3,3), strides=(1,1), padding='same')(a)\n",
    "p=Conv2D(256, (1,1), padding='same', strides=1, activation=LeakyReLU(alpha=0.1))(p)\n",
    "p=layers.BatchNormalization()(p)\n",
    "\n",
    "\n",
    "q=Conv2D(256, (1,1), padding='same', strides=1, activation=LeakyReLU(alpha=0.1))(a)\n",
    "q=layers.BatchNormalization()(q)\n",
    "\n",
    "\n",
    "a=concatenate([k,l,n,o,p,q],axis=3)\n",
    "\n",
    "a=MaxPooling2D((8,8), strides=(1,1), padding='same')(a)\n",
    "a=layers.BatchNormalization()(a)\n",
    "\n",
    "\n",
    "a=Flatten()(a)\n",
    "\n",
    "a=Dense(units=256,activation=LeakyReLU(alpha=0.3))(a)\n",
    "a=layers.BatchNormalization()(a)\n",
    "\n",
    "\n",
    "outputs=Dense(y_train.shape[1],activation=\"softmax\")(a)\n",
    "\n",
    "model=keras.Model(inputs,outputs)\n",
    "\n",
    "#adam=Adam(learning_rate=0.001,clipnorm=0.7)\n",
    "\n",
    "sgd=SGD(learning_rate=0.01,momentum=0.9,clipnorm=1)\n",
    "\n",
    "\n",
    "model.compile(loss=\"categorical_crossentropy\",optimizer=sgd,metrics=['accuracy'])\n",
    "\n",
    "\n",
    "call=EarlyStopping(monitor=\"val_loss\",verbose=1,mode=\"auto\",patience=10,restore_best_weights=\"true\")\n",
    "\n",
    "checkpoint=ModelCheckpoint('Inception_SGD_Batchnorm_regularization.hdf5',monitor='val_accuracy',verbose=1,save_best_only=True,save_weights_only=True,model='auto',period=1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "id": "y4Ur_szhi9WN",
    "outputId": "ce5428c8-621c-473b-a467-cd0ffe4e0556"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/80\n",
      "391/390 [==============================] - ETA: 0s - loss: 3.8873 - accuracy: 0.1118\n",
      "Epoch 00001: val_accuracy improved from -inf to 0.09240, saving model to Inception_SGD_Batchnorm_regularization.hdf5\n",
      "391/390 [==============================] - 31s 80ms/step - loss: 3.8873 - accuracy: 0.1118 - val_loss: 4.0758 - val_accuracy: 0.0924\n",
      "Epoch 2/80\n",
      "391/390 [==============================] - ETA: 0s - loss: 3.3115 - accuracy: 0.1989\n",
      "Epoch 00002: val_accuracy improved from 0.09240 to 0.19380, saving model to Inception_SGD_Batchnorm_regularization.hdf5\n",
      "391/390 [==============================] - 30s 77ms/step - loss: 3.3115 - accuracy: 0.1989 - val_loss: 3.4355 - val_accuracy: 0.1938\n",
      "Epoch 3/80\n",
      "391/390 [==============================] - ETA: 0s - loss: 3.0366 - accuracy: 0.2476\n",
      "Epoch 00003: val_accuracy improved from 0.19380 to 0.23370, saving model to Inception_SGD_Batchnorm_regularization.hdf5\n",
      "391/390 [==============================] - 30s 77ms/step - loss: 3.0366 - accuracy: 0.2476 - val_loss: 3.2105 - val_accuracy: 0.2337\n",
      "Epoch 4/80\n",
      "391/390 [==============================] - ETA: 0s - loss: 2.8538 - accuracy: 0.2803\n",
      "Epoch 00004: val_accuracy improved from 0.23370 to 0.26780, saving model to Inception_SGD_Batchnorm_regularization.hdf5\n",
      "391/390 [==============================] - 30s 77ms/step - loss: 2.8538 - accuracy: 0.2803 - val_loss: 2.9848 - val_accuracy: 0.2678\n",
      "Epoch 5/80\n",
      "391/390 [==============================] - ETA: 0s - loss: 2.7213 - accuracy: 0.3070\n",
      "Epoch 00005: val_accuracy improved from 0.26780 to 0.32880, saving model to Inception_SGD_Batchnorm_regularization.hdf5\n",
      "391/390 [==============================] - 31s 79ms/step - loss: 2.7213 - accuracy: 0.3070 - val_loss: 2.6815 - val_accuracy: 0.3288\n",
      "Epoch 6/80\n",
      "391/390 [==============================] - ETA: 0s - loss: 2.5938 - accuracy: 0.3319\n",
      "Epoch 00006: val_accuracy improved from 0.32880 to 0.33320, saving model to Inception_SGD_Batchnorm_regularization.hdf5\n",
      "391/390 [==============================] - 31s 79ms/step - loss: 2.5938 - accuracy: 0.3319 - val_loss: 2.6564 - val_accuracy: 0.3332\n",
      "Epoch 7/80\n",
      "391/390 [==============================] - ETA: 0s - loss: 2.4952 - accuracy: 0.3515\n",
      "Epoch 00007: val_accuracy did not improve from 0.33320\n",
      "391/390 [==============================] - 30s 78ms/step - loss: 2.4952 - accuracy: 0.3515 - val_loss: 2.7214 - val_accuracy: 0.3305\n",
      "Epoch 8/80\n",
      "391/390 [==============================] - ETA: 0s - loss: 2.4057 - accuracy: 0.3715\n",
      "Epoch 00008: val_accuracy improved from 0.33320 to 0.35490, saving model to Inception_SGD_Batchnorm_regularization.hdf5\n",
      "391/390 [==============================] - 31s 79ms/step - loss: 2.4057 - accuracy: 0.3715 - val_loss: 2.5714 - val_accuracy: 0.3549\n",
      "Epoch 9/80\n",
      "391/390 [==============================] - ETA: 0s - loss: 2.3287 - accuracy: 0.3885\n",
      "Epoch 00009: val_accuracy improved from 0.35490 to 0.36160, saving model to Inception_SGD_Batchnorm_regularization.hdf5\n",
      "391/390 [==============================] - 31s 78ms/step - loss: 2.3287 - accuracy: 0.3885 - val_loss: 2.5710 - val_accuracy: 0.3616\n",
      "Epoch 10/80\n",
      "391/390 [==============================] - ETA: 0s - loss: 2.2591 - accuracy: 0.4024\n",
      "Epoch 00010: val_accuracy improved from 0.36160 to 0.40630, saving model to Inception_SGD_Batchnorm_regularization.hdf5\n",
      "391/390 [==============================] - 31s 78ms/step - loss: 2.2591 - accuracy: 0.4024 - val_loss: 2.3454 - val_accuracy: 0.4063\n",
      "Epoch 11/80\n",
      "391/390 [==============================] - ETA: 0s - loss: 2.1838 - accuracy: 0.4183\n",
      "Epoch 00011: val_accuracy did not improve from 0.40630\n",
      "391/390 [==============================] - 30s 77ms/step - loss: 2.1838 - accuracy: 0.4183 - val_loss: 2.3996 - val_accuracy: 0.3912\n",
      "Epoch 12/80\n",
      "391/390 [==============================] - ETA: 0s - loss: 2.1237 - accuracy: 0.4325\n",
      "Epoch 00012: val_accuracy improved from 0.40630 to 0.42660, saving model to Inception_SGD_Batchnorm_regularization.hdf5\n",
      "391/390 [==============================] - 30s 77ms/step - loss: 2.1237 - accuracy: 0.4325 - val_loss: 2.2386 - val_accuracy: 0.4266\n",
      "Epoch 13/80\n",
      "391/390 [==============================] - ETA: 0s - loss: 2.0650 - accuracy: 0.4447\n",
      "Epoch 00013: val_accuracy did not improve from 0.42660\n",
      "391/390 [==============================] - 31s 79ms/step - loss: 2.0650 - accuracy: 0.4447 - val_loss: 2.2940 - val_accuracy: 0.4140\n",
      "Epoch 14/80\n",
      "391/390 [==============================] - ETA: 0s - loss: 1.9962 - accuracy: 0.4600\n",
      "Epoch 00014: val_accuracy did not improve from 0.42660\n",
      "391/390 [==============================] - 30s 77ms/step - loss: 1.9962 - accuracy: 0.4600 - val_loss: 2.2530 - val_accuracy: 0.4264\n",
      "Epoch 15/80\n",
      "391/390 [==============================] - ETA: 0s - loss: 1.9609 - accuracy: 0.4663\n",
      "Epoch 00015: val_accuracy improved from 0.42660 to 0.43490, saving model to Inception_SGD_Batchnorm_regularization.hdf5\n",
      "391/390 [==============================] - 31s 79ms/step - loss: 1.9609 - accuracy: 0.4663 - val_loss: 2.2075 - val_accuracy: 0.4349\n",
      "Epoch 16/80\n",
      "391/390 [==============================] - ETA: 0s - loss: 1.9013 - accuracy: 0.4816\n",
      "Epoch 00016: val_accuracy improved from 0.43490 to 0.43550, saving model to Inception_SGD_Batchnorm_regularization.hdf5\n",
      "391/390 [==============================] - 30s 76ms/step - loss: 1.9013 - accuracy: 0.4816 - val_loss: 2.2080 - val_accuracy: 0.4355\n",
      "Epoch 17/80\n",
      "391/390 [==============================] - ETA: 0s - loss: 1.8549 - accuracy: 0.4923\n",
      "Epoch 00017: val_accuracy did not improve from 0.43550\n",
      "391/390 [==============================] - 30s 78ms/step - loss: 1.8549 - accuracy: 0.4923 - val_loss: 2.2126 - val_accuracy: 0.4345\n",
      "Epoch 18/80\n",
      "391/390 [==============================] - ETA: 0s - loss: 1.8152 - accuracy: 0.5018\n",
      "Epoch 00018: val_accuracy did not improve from 0.43550\n",
      "391/390 [==============================] - 30s 78ms/step - loss: 1.8152 - accuracy: 0.5018 - val_loss: 2.2808 - val_accuracy: 0.4334\n",
      "Epoch 19/80\n",
      "391/390 [==============================] - ETA: 0s - loss: 1.7641 - accuracy: 0.5121\n",
      "Epoch 00019: val_accuracy improved from 0.43550 to 0.45650, saving model to Inception_SGD_Batchnorm_regularization.hdf5\n",
      "391/390 [==============================] - 30s 78ms/step - loss: 1.7641 - accuracy: 0.5121 - val_loss: 2.1548 - val_accuracy: 0.4565\n",
      "Epoch 20/80\n",
      "391/390 [==============================] - ETA: 0s - loss: 1.7134 - accuracy: 0.5253\n",
      "Epoch 00020: val_accuracy did not improve from 0.45650\n",
      "391/390 [==============================] - 30s 77ms/step - loss: 1.7134 - accuracy: 0.5253 - val_loss: 2.2450 - val_accuracy: 0.4355\n",
      "Epoch 21/80\n",
      "391/390 [==============================] - ETA: 0s - loss: 1.6692 - accuracy: 0.5342\n",
      "Epoch 00021: val_accuracy did not improve from 0.45650\n",
      "391/390 [==============================] - 30s 78ms/step - loss: 1.6692 - accuracy: 0.5342 - val_loss: 2.1754 - val_accuracy: 0.4511\n",
      "Epoch 22/80\n",
      "391/390 [==============================] - ETA: 0s - loss: 1.6375 - accuracy: 0.5444\n",
      "Epoch 00022: val_accuracy did not improve from 0.45650\n",
      "391/390 [==============================] - 31s 78ms/step - loss: 1.6375 - accuracy: 0.5444 - val_loss: 2.1770 - val_accuracy: 0.4513\n",
      "Epoch 23/80\n",
      "391/390 [==============================] - ETA: 0s - loss: 1.5908 - accuracy: 0.5545\n",
      "Epoch 00023: val_accuracy improved from 0.45650 to 0.46400, saving model to Inception_SGD_Batchnorm_regularization.hdf5\n",
      "391/390 [==============================] - 31s 79ms/step - loss: 1.5908 - accuracy: 0.5545 - val_loss: 2.1428 - val_accuracy: 0.4640\n",
      "Epoch 24/80\n",
      "391/390 [==============================] - ETA: 0s - loss: 1.5620 - accuracy: 0.5600\n",
      "Epoch 00024: val_accuracy did not improve from 0.46400\n",
      "391/390 [==============================] - 31s 78ms/step - loss: 1.5620 - accuracy: 0.5600 - val_loss: 2.2105 - val_accuracy: 0.4503\n",
      "Epoch 25/80\n",
      "391/390 [==============================] - ETA: 0s - loss: 1.5192 - accuracy: 0.5715\n",
      "Epoch 00025: val_accuracy did not improve from 0.46400\n",
      "391/390 [==============================] - 30s 78ms/step - loss: 1.5192 - accuracy: 0.5715 - val_loss: 2.2327 - val_accuracy: 0.4577\n",
      "Epoch 26/80\n",
      "391/390 [==============================] - ETA: 0s - loss: 1.4843 - accuracy: 0.5786\n",
      "Epoch 00026: val_accuracy improved from 0.46400 to 0.47810, saving model to Inception_SGD_Batchnorm_regularization.hdf5\n",
      "391/390 [==============================] - 31s 80ms/step - loss: 1.4843 - accuracy: 0.5786 - val_loss: 2.1054 - val_accuracy: 0.4781\n",
      "Epoch 27/80\n",
      "391/390 [==============================] - ETA: 0s - loss: 1.4393 - accuracy: 0.5865\n",
      "Epoch 00027: val_accuracy did not improve from 0.47810\n",
      "391/390 [==============================] - 31s 78ms/step - loss: 1.4393 - accuracy: 0.5865 - val_loss: 2.2562 - val_accuracy: 0.4521\n",
      "Epoch 28/80\n",
      "391/390 [==============================] - ETA: 0s - loss: 1.4121 - accuracy: 0.5949\n",
      "Epoch 00028: val_accuracy did not improve from 0.47810\n",
      "391/390 [==============================] - 31s 79ms/step - loss: 1.4121 - accuracy: 0.5949 - val_loss: 2.2170 - val_accuracy: 0.4607\n",
      "Epoch 29/80\n",
      "391/390 [==============================] - ETA: 0s - loss: 1.3760 - accuracy: 0.6055\n",
      "Epoch 00029: val_accuracy did not improve from 0.47810\n",
      "391/390 [==============================] - 31s 79ms/step - loss: 1.3760 - accuracy: 0.6055 - val_loss: 2.1857 - val_accuracy: 0.4669\n",
      "Epoch 30/80\n",
      "391/390 [==============================] - ETA: 0s - loss: 1.3422 - accuracy: 0.6109\n",
      "Epoch 00030: val_accuracy did not improve from 0.47810\n",
      "391/390 [==============================] - 30s 78ms/step - loss: 1.3422 - accuracy: 0.6109 - val_loss: 2.1798 - val_accuracy: 0.4719\n",
      "Epoch 31/80\n",
      "391/390 [==============================] - ETA: 0s - loss: 1.2990 - accuracy: 0.6256\n",
      "Epoch 00031: val_accuracy did not improve from 0.47810\n",
      "391/390 [==============================] - 30s 77ms/step - loss: 1.2990 - accuracy: 0.6256 - val_loss: 2.2991 - val_accuracy: 0.4545\n",
      "Epoch 32/80\n",
      "391/390 [==============================] - ETA: 0s - loss: 1.2686 - accuracy: 0.6333\n",
      "Epoch 00032: val_accuracy did not improve from 0.47810\n",
      "391/390 [==============================] - 30s 76ms/step - loss: 1.2686 - accuracy: 0.6333 - val_loss: 2.3512 - val_accuracy: 0.4565\n",
      "Epoch 33/80\n",
      "391/390 [==============================] - ETA: 0s - loss: 1.2523 - accuracy: 0.6356\n",
      "Epoch 00033: val_accuracy did not improve from 0.47810\n",
      "391/390 [==============================] - 30s 76ms/step - loss: 1.2523 - accuracy: 0.6356 - val_loss: 2.2688 - val_accuracy: 0.4625\n",
      "Epoch 34/80\n",
      "391/390 [==============================] - ETA: 0s - loss: 1.2204 - accuracy: 0.6437\n",
      "Epoch 00034: val_accuracy did not improve from 0.47810\n",
      "391/390 [==============================] - 29s 75ms/step - loss: 1.2204 - accuracy: 0.6437 - val_loss: 2.2913 - val_accuracy: 0.4697\n",
      "Epoch 35/80\n",
      "391/390 [==============================] - ETA: 0s - loss: 1.1784 - accuracy: 0.6529\n",
      "Epoch 00035: val_accuracy did not improve from 0.47810\n",
      "391/390 [==============================] - 29s 74ms/step - loss: 1.1784 - accuracy: 0.6529 - val_loss: 2.2471 - val_accuracy: 0.4719\n",
      "Epoch 36/80\n",
      "391/390 [==============================] - ETA: 0s - loss: 1.1493 - accuracy: 0.6627Restoring model weights from the end of the best epoch.\n",
      "\n",
      "Epoch 00036: val_accuracy did not improve from 0.47810\n",
      "391/390 [==============================] - 29s 75ms/step - loss: 1.1493 - accuracy: 0.6627 - val_loss: 2.3605 - val_accuracy: 0.4657\n",
      "Epoch 00036: early stopping\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.keras.callbacks.History at 0x7f750d5df828>"
      ]
     },
     "execution_count": 10,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.fit_generator(aug_data.flow(X_train,y_train,batch_size=128),steps_per_epoch=len(X_train)/128,epochs=180,validation_data=(X_test,y_test),verbose=1,callbacks=[call,checkpoint])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "id": "AnziTqW3jAzS"
   },
   "outputs": [],
   "source": [
    "y_true = y_test.argmax(-1)\n",
    "y_pred = model.predict(X_test).argmax(-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "id": "ZgdzI9WYjK8E",
    "outputId": "1e177a4b-132a-4ba2-af07-0ff4c960f052"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.82      0.72      0.77       100\n",
      "           1       0.59      0.59      0.59       100\n",
      "           2       0.39      0.39      0.39       100\n",
      "           3       0.33      0.27      0.30       100\n",
      "           4       0.32      0.33      0.33       100\n",
      "           5       0.45      0.47      0.46       100\n",
      "           6       0.46      0.50      0.48       100\n",
      "           7       0.45      0.54      0.49       100\n",
      "           8       0.40      0.58      0.48       100\n",
      "           9       0.59      0.61      0.60       100\n",
      "          10       0.28      0.29      0.28       100\n",
      "          11       0.32      0.36      0.34       100\n",
      "          12       0.63      0.38      0.48       100\n",
      "          13       0.36      0.49      0.42       100\n",
      "          14       0.45      0.40      0.43       100\n",
      "          15       0.46      0.28      0.35       100\n",
      "          16       0.46      0.49      0.47       100\n",
      "          17       0.73      0.64      0.68       100\n",
      "          18       0.49      0.42      0.45       100\n",
      "          19       0.48      0.42      0.45       100\n",
      "          20       0.60      0.77      0.67       100\n",
      "          21       0.63      0.55      0.59       100\n",
      "          22       0.35      0.43      0.39       100\n",
      "          23       0.87      0.54      0.67       100\n",
      "          24       0.61      0.67      0.64       100\n",
      "          25       0.33      0.33      0.33       100\n",
      "          26       0.28      0.39      0.33       100\n",
      "          27       0.33      0.26      0.29       100\n",
      "          28       0.62      0.60      0.61       100\n",
      "          29       0.42      0.44      0.43       100\n",
      "          30       0.44      0.44      0.44       100\n",
      "          31       0.58      0.46      0.51       100\n",
      "          32       0.49      0.39      0.44       100\n",
      "          33       0.49      0.42      0.45       100\n",
      "          34       0.45      0.49      0.47       100\n",
      "          35       0.41      0.34      0.37       100\n",
      "          36       0.49      0.39      0.44       100\n",
      "          37       0.46      0.39      0.42       100\n",
      "          38       0.42      0.27      0.33       100\n",
      "          39       0.58      0.61      0.60       100\n",
      "          40       0.51      0.41      0.46       100\n",
      "          41       0.58      0.74      0.65       100\n",
      "          42       0.40      0.51      0.45       100\n",
      "          43       0.49      0.59      0.54       100\n",
      "          44       0.24      0.18      0.20       100\n",
      "          45       0.26      0.28      0.27       100\n",
      "          46       0.44      0.34      0.38       100\n",
      "          47       0.56      0.44      0.49       100\n",
      "          48       0.41      0.83      0.55       100\n",
      "          49       0.76      0.47      0.58       100\n",
      "          50       0.37      0.17      0.23       100\n",
      "          51       0.45      0.56      0.50       100\n",
      "          52       0.37      0.91      0.53       100\n",
      "          53       0.62      0.77      0.68       100\n",
      "          54       0.58      0.59      0.59       100\n",
      "          55       0.55      0.11      0.18       100\n",
      "          56       0.75      0.69      0.72       100\n",
      "          57       0.63      0.49      0.55       100\n",
      "          58       0.36      0.61      0.45       100\n",
      "          59       0.46      0.38      0.42       100\n",
      "          60       0.83      0.69      0.75       100\n",
      "          61       0.45      0.62      0.52       100\n",
      "          62       0.52      0.69      0.59       100\n",
      "          63       0.42      0.42      0.42       100\n",
      "          64       0.27      0.31      0.29       100\n",
      "          65       0.48      0.30      0.37       100\n",
      "          66       0.32      0.46      0.38       100\n",
      "          67       0.39      0.34      0.36       100\n",
      "          68       0.89      0.77      0.82       100\n",
      "          69       0.62      0.67      0.64       100\n",
      "          70       0.54      0.57      0.55       100\n",
      "          71       0.66      0.73      0.69       100\n",
      "          72       0.23      0.25      0.24       100\n",
      "          73       0.53      0.27      0.36       100\n",
      "          74       0.36      0.33      0.35       100\n",
      "          75       0.72      0.79      0.76       100\n",
      "          76       0.78      0.64      0.70       100\n",
      "          77       0.46      0.33      0.38       100\n",
      "          78       0.31      0.39      0.35       100\n",
      "          79       0.47      0.48      0.48       100\n",
      "          80       0.40      0.12      0.18       100\n",
      "          81       0.36      0.60      0.45       100\n",
      "          82       0.65      0.88      0.75       100\n",
      "          83       0.55      0.46      0.50       100\n",
      "          84       0.49      0.37      0.42       100\n",
      "          85       0.51      0.49      0.50       100\n",
      "          86       0.46      0.52      0.49       100\n",
      "          87       0.42      0.65      0.51       100\n",
      "          88       0.40      0.57      0.47       100\n",
      "          89       0.36      0.62      0.46       100\n",
      "          90       0.46      0.53      0.49       100\n",
      "          91       0.63      0.61      0.62       100\n",
      "          92       0.57      0.25      0.35       100\n",
      "          93       0.28      0.22      0.25       100\n",
      "          94       0.70      0.80      0.75       100\n",
      "          95       0.54      0.57      0.55       100\n",
      "          96       0.49      0.25      0.33       100\n",
      "          97       0.53      0.30      0.38       100\n",
      "          98       0.22      0.13      0.16       100\n",
      "          99       0.52      0.40      0.45       100\n",
      "\n",
      "    accuracy                           0.48     10000\n",
      "   macro avg       0.49      0.48      0.47     10000\n",
      "weighted avg       0.49      0.48      0.47     10000\n",
      "\n",
      "Accuracy is 0.4781\n",
      "Precision is 0.4891218936664765\n",
      "Recall is 0.4781\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import classification_report,accuracy_score,recall_score,precision_score\n",
    "print(classification_report(Y_test,y_pred))\n",
    "print(\"Accuracy is {}\".format(accuracy_score(Y_test,y_pred)))\n",
    "print(\"Precision is {}\".format(precision_score(Y_test,y_pred,average=\"weighted\")))\n",
    "print(\"Recall is {}\".format(recall_score(Y_test,y_pred,average=\"weighted\")))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "id": "cDYy6s4ajrLa",
    "outputId": "6af7bc91-02cb-41b3-e7b9-5d87fa9d6a8c"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"functional_3\"\n",
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "input_2 (InputLayer)            [(None, 32, 32, 3)]  0                                            \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_32 (Conv2D)              (None, 16, 16, 32)   896         input_2[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_36 (BatchNo (None, 16, 16, 32)   128         conv2d_32[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_33 (Conv2D)              (None, 16, 16, 32)   9248        batch_normalization_36[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_37 (BatchNo (None, 16, 16, 32)   128         conv2d_33[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_34 (Conv2D)              (None, 16, 16, 32)   9248        batch_normalization_37[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_38 (BatchNo (None, 16, 16, 32)   128         conv2d_34[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling2d_5 (MaxPooling2D)  (None, 7, 7, 32)     0           batch_normalization_38[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_35 (Conv2D)              (None, 7, 7, 32)     9248        max_pooling2d_5[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_39 (BatchNo (None, 7, 7, 32)     128         conv2d_35[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_36 (Conv2D)              (None, 4, 4, 32)     9248        batch_normalization_39[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_40 (BatchNo (None, 4, 4, 32)     128         conv2d_36[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_37 (Conv2D)              (None, 4, 4, 32)     9248        batch_normalization_40[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_41 (BatchNo (None, 4, 4, 32)     128         conv2d_37[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_38 (Conv2D)              (None, 4, 4, 64)     2112        batch_normalization_41[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_42 (BatchNo (None, 4, 4, 64)     256         conv2d_38[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_39 (Conv2D)              (None, 4, 4, 64)     36928       batch_normalization_42[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_41 (Conv2D)              (None, 4, 4, 64)     2112        batch_normalization_41[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling2d_6 (MaxPooling2D)  (None, 4, 4, 32)     0           batch_normalization_41[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_43 (BatchNo (None, 4, 4, 64)     256         conv2d_39[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_45 (BatchNo (None, 4, 4, 64)     256         conv2d_41[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_47 (BatchNo (None, 4, 4, 32)     128         max_pooling2d_6[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_40 (Conv2D)              (None, 4, 4, 64)     36928       batch_normalization_43[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_42 (Conv2D)              (None, 4, 4, 64)     36928       batch_normalization_45[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_43 (Conv2D)              (None, 4, 4, 64)     2112        batch_normalization_47[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_44 (Conv2D)              (None, 4, 4, 64)     2112        batch_normalization_41[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_44 (BatchNo (None, 4, 4, 64)     256         conv2d_40[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_46 (BatchNo (None, 4, 4, 64)     256         conv2d_42[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_48 (BatchNo (None, 4, 4, 64)     256         conv2d_43[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_49 (BatchNo (None, 4, 4, 64)     256         conv2d_44[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_3 (Concatenate)     (None, 4, 4, 256)    0           batch_normalization_44[0][0]     \n",
      "                                                                 batch_normalization_46[0][0]     \n",
      "                                                                 batch_normalization_48[0][0]     \n",
      "                                                                 batch_normalization_49[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_45 (Conv2D)              (None, 4, 4, 128)    32896       concatenate_3[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_50 (BatchNo (None, 4, 4, 128)    512         conv2d_45[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_46 (Conv2D)              (None, 4, 4, 128)    114816      batch_normalization_50[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_51 (BatchNo (None, 4, 4, 128)    512         conv2d_46[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_47 (Conv2D)              (None, 4, 4, 128)    114816      batch_normalization_51[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_50 (Conv2D)              (None, 4, 4, 128)    32896       concatenate_3[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling2d_7 (MaxPooling2D)  (None, 4, 4, 256)    0           concatenate_3[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_52 (BatchNo (None, 4, 4, 128)    512         conv2d_47[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_55 (BatchNo (None, 4, 4, 128)    512         conv2d_50[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_58 (BatchNo (None, 4, 4, 256)    1024        max_pooling2d_7[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_48 (Conv2D)              (None, 4, 4, 128)    114816      batch_normalization_52[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_51 (Conv2D)              (None, 4, 4, 128)    114816      batch_normalization_55[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_53 (Conv2D)              (None, 4, 4, 128)    32896       batch_normalization_58[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_53 (BatchNo (None, 4, 4, 128)    512         conv2d_48[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_56 (BatchNo (None, 4, 4, 128)    512         conv2d_51[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_59 (BatchNo (None, 4, 4, 128)    512         conv2d_53[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_49 (Conv2D)              (None, 4, 4, 128)    114816      batch_normalization_53[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_52 (Conv2D)              (None, 4, 4, 128)    114816      batch_normalization_56[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_54 (Conv2D)              (None, 4, 4, 128)    16512       batch_normalization_59[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_54 (BatchNo (None, 4, 4, 128)    512         conv2d_49[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_57 (BatchNo (None, 4, 4, 128)    512         conv2d_52[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_60 (BatchNo (None, 4, 4, 128)    512         conv2d_54[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_4 (Concatenate)     (None, 4, 4, 512)    0           batch_normalization_54[0][0]     \n",
      "                                                                 batch_normalization_57[0][0]     \n",
      "                                                                 batch_normalization_59[0][0]     \n",
      "                                                                 batch_normalization_60[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_55 (Conv2D)              (None, 4, 4, 256)    131328      concatenate_4[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_61 (BatchNo (None, 4, 4, 256)    1024        conv2d_55[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_56 (Conv2D)              (None, 4, 4, 256)    590080      batch_normalization_61[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_59 (Conv2D)              (None, 4, 4, 256)    131328      concatenate_4[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_62 (BatchNo (None, 4, 4, 256)    1024        conv2d_56[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_65 (BatchNo (None, 4, 4, 256)    1024        conv2d_59[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling2d_8 (MaxPooling2D)  (None, 4, 4, 512)    0           concatenate_4[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_57 (Conv2D)              (None, 4, 4, 256)    196864      batch_normalization_62[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_58 (Conv2D)              (None, 4, 4, 256)    196864      batch_normalization_62[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_60 (Conv2D)              (None, 4, 4, 256)    196864      batch_normalization_65[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_61 (Conv2D)              (None, 4, 4, 256)    196864      batch_normalization_65[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_62 (Conv2D)              (None, 4, 4, 256)    131328      max_pooling2d_8[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_63 (Conv2D)              (None, 4, 4, 256)    131328      concatenate_4[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_63 (BatchNo (None, 4, 4, 256)    1024        conv2d_57[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_64 (BatchNo (None, 4, 4, 256)    1024        conv2d_58[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_66 (BatchNo (None, 4, 4, 256)    1024        conv2d_60[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_67 (BatchNo (None, 4, 4, 256)    1024        conv2d_61[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_68 (BatchNo (None, 4, 4, 256)    1024        conv2d_62[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_69 (BatchNo (None, 4, 4, 256)    1024        conv2d_63[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_5 (Concatenate)     (None, 4, 4, 1536)   0           batch_normalization_63[0][0]     \n",
      "                                                                 batch_normalization_64[0][0]     \n",
      "                                                                 batch_normalization_66[0][0]     \n",
      "                                                                 batch_normalization_67[0][0]     \n",
      "                                                                 batch_normalization_68[0][0]     \n",
      "                                                                 batch_normalization_69[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling2d_9 (MaxPooling2D)  (None, 4, 4, 1536)   0           concatenate_5[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_70 (BatchNo (None, 4, 4, 1536)   6144        max_pooling2d_9[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "flatten_1 (Flatten)             (None, 24576)        0           batch_normalization_70[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "dense_2 (Dense)                 (None, 256)          6291712     flatten_1[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_71 (BatchNo (None, 256)          1024        dense_2[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "dense_3 (Dense)                 (None, 100)          25700       batch_normalization_71[0][0]     \n",
      "==================================================================================================\n",
      "Total params: 9,215,940\n",
      "Trainable params: 9,203,332\n",
      "Non-trainable params: 12,608\n",
      "__________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'Node' object has no attribute 'output_masks'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-3-57af7a21f54a>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     37\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     38\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 39\u001b[1;33m \u001b[0ma\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mConv2D\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfilters\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m32\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mkernel_size\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m3\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;36m3\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mpadding\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m\"same\"\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mstrides\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mactivation\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mLeakyReLU\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0malpha\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m0.1\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0ma\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     40\u001b[0m \u001b[0ma\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mlayers\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mBatchNormalization\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0ma\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     41\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\keras\\engine\\base_layer.py\u001b[0m in \u001b[0;36m__call__\u001b[1;34m(self, inputs, **kwargs)\u001b[0m\n\u001b[0;32m    473\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    474\u001b[0m             \u001b[1;31m# Handle mask propagation.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 475\u001b[1;33m             \u001b[0mprevious_mask\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0m_collect_previous_mask\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    476\u001b[0m             \u001b[0muser_kwargs\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcopy\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    477\u001b[0m             \u001b[1;32mif\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[0mis_all_none\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mprevious_mask\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\keras\\engine\\base_layer.py\u001b[0m in \u001b[0;36m_collect_previous_mask\u001b[1;34m(input_tensors)\u001b[0m\n\u001b[0;32m   1439\u001b[0m             \u001b[0minbound_layer\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mnode_index\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtensor_index\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mx\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_keras_history\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1440\u001b[0m             \u001b[0mnode\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0minbound_layer\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_inbound_nodes\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mnode_index\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1441\u001b[1;33m             \u001b[0mmask\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnode\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0moutput_masks\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mtensor_index\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1442\u001b[0m             \u001b[0mmasks\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmask\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1443\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mAttributeError\u001b[0m: 'Node' object has no attribute 'output_masks'"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from tensorflow.keras.models import Sequential,Model\n",
    "from tensorflow.keras.layers import Dense,Flatten,Input,Conv2D,MaxPool2D,concatenate,BatchNormalization\n",
    "from keras.layers.convolutional import MaxPooling2D\n",
    "from keras.layers.merge import concatenate\n",
    "from keras.layers import Concatenate, Dense, LSTM, Input, concatenate\n",
    "from keras.layers import Conv2D\n",
    "\n",
    "from tensorflow.keras import layers\n",
    "\n",
    "import tensorflow.keras as keras\n",
    "\n",
    "import tensorflow.keras as keras\n",
    "\n",
    "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
    "\n",
    "from tensorflow.keras.layers import LeakyReLU\n",
    "\n",
    "from tensorflow.keras.datasets import cifar100\n",
    "from tensorflow.keras.utils import to_categorical\n",
    "\n",
    "from tensorflow.keras.callbacks import EarlyStopping,ModelCheckpoint\n",
    "\n",
    "from keras.optimizers import Adam,SGD\n",
    "\n",
    "(X_train,Y_train),(X_test,Y_test)=cifar100.load_data()\n",
    "y_train,y_test=to_categorical(Y_train),to_categorical(Y_test)\n",
    "X_train,X_test=X_train/255,X_test/255\n",
    "\n",
    "\n",
    "inputs=keras.Input(shape=(32,32,3))\n",
    "\n",
    "a=layers.Conv2D(filters=32,kernel_size=(3,3),padding=\"same\",strides=2,activation=LeakyReLU(alpha=0.1))(inputs)\n",
    "a=layers.BatchNormalization()(a)\n",
    "\n",
    "\n",
    "a=layers.Conv2D(filters=32,kernel_size=(3,3),padding=\"same\",strides=1,activation=LeakyReLU(alpha=0.1))(a)\n",
    "a=layers.BatchNormalization()(a)\n",
    "\n",
    "\n",
    "a=layers.Conv2D(filters=32, kernel_size=(3,3), padding=\"same\",strides=1,activation=LeakyReLU(alpha=0.1))(a)\n",
    "a=layers.BatchNormalization()(a)\n",
    "\n",
    "a=layers.MaxPool2D(pool_size=(3,3),strides=(2,2))(a)\n",
    "\n",
    "\n",
    "a=layers.Conv2D(filters=32, kernel_size=(3,3), padding=\"same\",strides=1,activation=LeakyReLU(alpha=0.1))(a)\n",
    "a=layers.BatchNormalization()(a)\n",
    "\n",
    "\n",
    "a=layers.Conv2D(filters=32, kernel_size=(3,3), padding=\"same\",strides=2,activation=LeakyReLU(alpha=0.1))(a)\n",
    "a=layers.BatchNormalization()(a)\n",
    "\n",
    "\n",
    "a=Conv2D(filters=32, kernel_size=(3,3), padding=\"same\",strides=1,activation=LeakyReLU(alpha=0.1))(a)\n",
    "a=layers.BatchNormalization()(a)\n",
    "\n",
    "\n",
    "#inception-1\n",
    "b=layers.Conv2D(64, (1,1), padding='same',strides=1,activation=LeakyReLU(alpha=0.1))(a)\n",
    "b=layers.BatchNormalization()(b)\n",
    "\n",
    "b=layers.Conv2D(64, (3,3), padding='same',strides=1 ,activation=LeakyReLU(alpha=0.1))(b)\n",
    "b=layers.BatchNormalization()(b)\n",
    "\n",
    "b=layers.Conv2D(64, (3,3), padding='same', strides=1,activation=LeakyReLU(alpha=0.1))(b)\n",
    "b=layers.BatchNormalization()(b)\n",
    "\n",
    "\n",
    "\n",
    "c=layers.Conv2D(64, (1,1), padding='same', strides=1, activation=LeakyReLU(alpha=0.1))(a)\n",
    "c=layers.BatchNormalization()(c)\n",
    "\n",
    "c=layers.Conv2D(64, (3,3), padding='same', strides=1, activation=LeakyReLU(alpha=0.1))(c)\n",
    "c=layers.BatchNormalization()(c)\n",
    "\n",
    "\n",
    "d=layers.MaxPooling2D((3,3), strides=(1,1), padding='same')(a)\n",
    "d=layers.BatchNormalization()(d)\n",
    "\n",
    "d=layers.Conv2D(64, (1,1), padding='same', strides=1, activation=LeakyReLU(alpha=0.1))(d)\n",
    "d=layers.BatchNormalization()(d)\n",
    "\n",
    "\n",
    "e=layers.Conv2D(64, (1,1), padding='same', strides=1, activation=LeakyReLU(alpha=0.1))(a)\n",
    "e=layers.BatchNormalization()(e)\n",
    "\n",
    "\n",
    "a=layers.concatenate([b, c, d,e],axis=3)\n",
    "\n",
    "#inception-2\n",
    "f=layers.Conv2D(128, (1,1), padding='same', strides=1, activation=LeakyReLU(alpha=0.1))(a)\n",
    "f=layers.BatchNormalization()(f)\n",
    "\n",
    "f=layers.Conv2D(128, (1,7), padding='same', strides=1, activation=LeakyReLU(alpha=0.1))(f)\n",
    "f=layers.BatchNormalization()(f)\n",
    "\n",
    "\n",
    "f=layers.Conv2D(128, (7,1), padding='same', strides=1, activation=LeakyReLU(alpha=0.1))(f)\n",
    "f=layers.BatchNormalization()(f)\n",
    "\n",
    "f=layers.Conv2D(128, (1,7), padding='same', strides=1, activation=LeakyReLU(alpha=0.1))(f)\n",
    "f=layers.BatchNormalization()(f)\n",
    "\n",
    "\n",
    "f=layers.Conv2D(128, (7,1), padding='same', strides=1, activation=LeakyReLU(alpha=0.1))(f)\n",
    "f=layers.BatchNormalization()(f)\n",
    "\n",
    "\n",
    "g=layers.Conv2D(128, (1,1), padding='same', strides=1, activation=LeakyReLU(alpha=0.1))(a)\n",
    "g=layers.BatchNormalization()(g)\n",
    "\n",
    "g=layers.Conv2D(128, (1,7), padding='same', strides=1,activation=LeakyReLU(alpha=0.1))(g)\n",
    "g=layers.BatchNormalization()(g)\n",
    "\n",
    "g=layers.Conv2D(128, (7,1), padding='same', strides=1, activation=LeakyReLU(alpha=0.1))(g)\n",
    "\n",
    "g=layers.BatchNormalization()(g)\n",
    "\n",
    "\n",
    "h=layers.MaxPooling2D((3,3), strides=(1,1), padding='same')(a)\n",
    "h=layers.BatchNormalization()(h)\n",
    "\n",
    "h=layers.Conv2D(128, (1,1), padding='same', strides=1, activation=LeakyReLU(alpha=0.1))(h)\n",
    "h=layers.BatchNormalization()(h)\n",
    "\n",
    "\n",
    "i=layers.Conv2D(128, (1,1), padding='same', strides=1, activation=LeakyReLU(alpha=0.1))(h)\n",
    "i=layers.BatchNormalization()(i)\n",
    "\n",
    "\n",
    "a=layers.concatenate([f,g,h,i],axis=3)\n",
    "\n",
    "#inception-3\n",
    "j=layers.Conv2D(256, (1,1), padding='same', strides=1, activation=LeakyReLU(alpha=0.1))(a)\n",
    "j=layers.BatchNormalization()(j)\n",
    "\n",
    "\n",
    "j=layers.Conv2D(256, (3,3), padding='same', strides=1, activation=LeakyReLU(alpha=0.1))(j)\n",
    "j=layers.BatchNormalization()(j)\n",
    "\n",
    "k=layers.Conv2D(256, (1,3), padding='same', strides=1, activation=LeakyReLU(alpha=0.1))(j)\n",
    "#k=layers.BatchNormalization()(k)\n",
    "\n",
    "l=layers.Conv2D(256, (3,1), padding='same', strides=1, activation=LeakyReLU(alpha=0.1))(j)\n",
    "#l=layers.BatchNormalization()(l)\n",
    "\n",
    "\n",
    "m=layers.Conv2D(256, (1,1), padding='same', activation=LeakyReLU(alpha=0.1))(a)\n",
    "m=layers.BatchNormalization()(m)\n",
    "\n",
    "n=layers.Conv2D(256, (1,3), padding='same', activation=LeakyReLU(alpha=0.1))(m)\n",
    "#n=layers.BatchNormalization()(n)\n",
    "\n",
    "o=layers.Conv2D(256, (3,1), padding='same', activation=LeakyReLU(alpha=0.1))(m)\n",
    "#o=layers.BatchNormalization()(o)\n",
    "\n",
    "\n",
    "p=layers.MaxPool2D((3,3), strides=(1,1), padding='same')(a)\n",
    "p=layers.Conv2D(256, (1,1), padding='same', strides=1, activation=LeakyReLU(alpha=0.1))(p)\n",
    "p=layers.BatchNormalization()(p)\n",
    "\n",
    "\n",
    "q=layers.Conv2D(256, (1,1), padding='same', strides=1, activation=LeakyReLU(alpha=0.1))(a)\n",
    "q=layers.BatchNormalization()(q)\n",
    "\n",
    "\n",
    "a=layers.concatenate([k,l,n,o,p,q],axis=3)\n",
    "\n",
    "a=layers.MaxPooling2D((8,8), strides=(1,1), padding='same')(a)\n",
    "a=layers.BatchNormalization()(a)\n",
    "\n",
    "\n",
    "a=layers.Flatten()(a)\n",
    "\n",
    "a=layers.Dense(units=256,activation=LeakyReLU(alpha=0.3))(a)\n",
    "a=layers.BatchNormalization()(a)\n",
    "\n",
    "\n",
    "outputs=layers.Dense(y_train.shape[1],activation=\"softmax\")(a)\n",
    "\n",
    "model=keras.Model(inputs,outputs)\n",
    "\n",
    "#adam=Adam(learning_rate=0.001,clipnorm=0.7)\n",
    "\n",
    "model.load_weights(\"../weights/Inception_SGD_Batchnorm_regularization.hdf5\")\n",
    "\n",
    "y_true = y_test.argmax(-1)\n",
    "y_pred = model.predict(X_test).argmax(-1)\n",
    "\n",
    "from sklearn.metrics import classification_report,accuracy_score,recall_score,precision_score\n",
    "print(classification_report(Y_test,y_pred))\n",
    "print(\"Accuracy is {}\".format(accuracy_score(Y_test,y_pred)))\n",
    "print(\"Precision is {}\".format(precision_score(Y_test,y_pred,average=\"weighted\")))\n",
    "print(\"Recall is {}\".format(recall_score(Y_test,y_pred,average=\"weighted\")))"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "name": "Inception_SGD_Batchnorm_regularization.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
