{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 51
    },
    "id": "mHcTJJNd7Pk0",
    "outputId": "1aec8d17-13ec-42de-933f-d99d4edfde17"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading data from https://www.cs.toronto.edu/~kriz/cifar-100-python.tar.gz\n",
      "169009152/169001437 [==============================] - 4s 0us/step\n"
     ]
    }
   ],
   "source": [
    "from keras.callbacks import ModelCheckpoint, EarlyStopping\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Conv2D, MaxPool2D, Flatten, Dense,BatchNormalization,Activation,Dropout,LeakyReLU\n",
    "from tensorflow.keras.optimizers import SGD,Adam\n",
    "from keras.preprocessing.image import ImageDataGenerator\n",
    "from keras.utils import to_categorical\n",
    "from tensorflow.keras.datasets import cifar100\n",
    "\n",
    "\n",
    "\n",
    "(x_train, Y_train), (x_test, Y_test) = cifar100.load_data()\n",
    "#x_train = x_train.astype('float32') / 255\n",
    "#x_test = x_test.astype('float32') / 255\n",
    "\n",
    "y_train = to_categorical(Y_train)\n",
    "y_test = to_categorical(Y_test)\n",
    "\n",
    "aug_data=ImageDataGenerator(rotation_range=20,horizontal_flip=True,width_shift_range=0.1,shear_range = 0.2,height_shift_range=0.1,zoom_range=0.2,brightness_range = (0.5, 1.5))\n",
    "aug_data.fit(x_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "id": "KVG0KRTc7RbC",
    "outputId": "83caa217-8199-4e1b-a5a9-d5118fbfa768"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:`period` argument is deprecated. Please use `save_freq` to specify the frequency in number of batches seen.\n"
     ]
    }
   ],
   "source": [
    "\n",
    "#import tensorflow as \n",
    "model = Sequential()\n",
    "\n",
    "# Creating first block- (2 Convolution + 1 Max pool)\n",
    "model.add(Conv2D(filters= 64, kernel_size= (3,3), strides= (1,1), padding='same', input_shape= (32, 32, 3)))\n",
    "model.add(LeakyReLU(alpha=0.1))\n",
    "model.add(Conv2D(filters= 64, kernel_size= (3,3), strides= (1,1), padding='same'))\n",
    "model.add(LeakyReLU(alpha=0.1))\n",
    "model.add(MaxPool2D(pool_size= (2,2), strides=(1,1)))\n",
    "\n",
    "# Creating second block- (2 Convolution + 1 Max pool)\n",
    "model.add(Conv2D(filters= 128, kernel_size= (3,3), strides= (1,1), padding='same'))\n",
    "model.add(LeakyReLU(alpha=0.1))\n",
    "model.add(Conv2D(filters= 128, kernel_size= (3,3), strides= (1,1), padding='same'))\n",
    "model.add(LeakyReLU(alpha=0.1))\n",
    "model.add(MaxPool2D(pool_size= (2,2), strides=(1,1)))\n",
    "\n",
    "\n",
    "# Creating third block- (3 Convolution + 1 Max pool)\n",
    "model.add(Conv2D(filters= 256, kernel_size= (3,3), strides= (1,1), padding='same'))\n",
    "model.add(LeakyReLU(alpha=0.1))\n",
    "model.add(Conv2D(filters= 256, kernel_size= (3,3), strides= (1,1), padding='same'))\n",
    "model.add(LeakyReLU(alpha=0.1))\n",
    "model.add(Conv2D(filters= 256, kernel_size= (3,3), strides= (1,1), padding='same'))\n",
    "model.add(LeakyReLU(alpha=0.1))\n",
    "model.add(MaxPool2D(pool_size= (2,2), strides=(1,1)))\n",
    "\n",
    "# Creating fourth block- (3 Convolution + 1 Max pool)\n",
    "model.add(Conv2D(filters= 512, kernel_size= (3,3), strides= (1,1), padding='same'))\n",
    "model.add(LeakyReLU(alpha=0.1))\n",
    "model.add(Conv2D(filters= 512, kernel_size= (3,3), strides= (1,1), padding='same'))\n",
    "model.add(LeakyReLU(alpha=0.1))\n",
    "model.add(Conv2D(filters= 512, kernel_size= (3,3), strides= (1,1), padding='same'))\n",
    "model.add(LeakyReLU(alpha=0.1))\n",
    "model.add(MaxPool2D(pool_size= (2,2), strides=(2,2)))\n",
    "\n",
    "# Creating fifth block- (3 Convolution + 1 Max pool)\n",
    "model.add(Conv2D(filters= 512, kernel_size= (3,3), strides= (1,1), padding='same'))\n",
    "model.add(LeakyReLU(alpha=0.1))\n",
    "model.add(Conv2D(filters= 512, kernel_size= (3,3), strides= (1,1), padding='same'))\n",
    "model.add(LeakyReLU(alpha=0.1))\n",
    "model.add(Conv2D(filters= 512, kernel_size= (3,3), strides= (1,1), padding='same'))\n",
    "model.add(LeakyReLU(alpha=0.1))\n",
    "model.add(MaxPool2D(pool_size= (2,2), strides=(1,1)))\n",
    "\n",
    "# Flattening the pooled image pixels\n",
    "model.add(Flatten())\n",
    "\n",
    "# Creating 2 Dense Layers\n",
    "model.add(Dense(units= 512))\n",
    "model.add(LeakyReLU(alpha=0.1))\n",
    "\n",
    "model.add(Dense(units= 100, activation='softmax'))\n",
    "adam=Adam(learning_rate=0.0001,clipnorm=1,name='adam')\n",
    "\n",
    "\n",
    "model.compile(optimizer=adam,loss='categorical_crossentropy',metrics=['accuracy'])\n",
    "\n",
    "checkpoint = ModelCheckpoint(\"VGG_Adam_no_reg_weights_amit.hdf5\", monitor='val_accuracy', verbose=1, save_best_only=True, save_weights_only=True, mode='auto', period=1)\n",
    "early = EarlyStopping(monitor='val_accuracy', min_delta=0, patience=5, verbose=1, mode='auto',restore_best_weights=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "id": "hn2LFvo57TxA",
    "outputId": "7d9d7700-4bf3-4097-f7af-802e8e1a8086"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100\n",
      "  2/391 [..............................] - ETA: 2:21 - loss: 5.5832 - accuracy: 0.0156WARNING:tensorflow:Callbacks method `on_train_batch_end` is slow compared to the batch time (batch time: 0.2699s vs `on_train_batch_end` time: 0.4580s). Check your callbacks.\n",
      "391/391 [==============================] - ETA: 0s - loss: 3.9238 - accuracy: 0.1094\n",
      "Epoch 00001: val_accuracy improved from -inf to 0.21810, saving model to VGG_Adam_no_reg_weights_amit.hdf5\n",
      "391/391 [==============================] - 335s 858ms/step - loss: 3.9238 - accuracy: 0.1094 - val_loss: 3.2499 - val_accuracy: 0.2181\n",
      "Epoch 2/100\n",
      "391/391 [==============================] - ETA: 0s - loss: 3.1673 - accuracy: 0.2418\n",
      "Epoch 00002: val_accuracy improved from 0.21810 to 0.31050, saving model to VGG_Adam_no_reg_weights_amit.hdf5\n",
      "391/391 [==============================] - 333s 852ms/step - loss: 3.1673 - accuracy: 0.2418 - val_loss: 2.7756 - val_accuracy: 0.3105\n",
      "Epoch 3/100\n",
      "391/391 [==============================] - ETA: 0s - loss: 2.7751 - accuracy: 0.3157\n",
      "Epoch 00003: val_accuracy improved from 0.31050 to 0.36800, saving model to VGG_Adam_no_reg_weights_amit.hdf5\n",
      "391/391 [==============================] - 333s 852ms/step - loss: 2.7751 - accuracy: 0.3157 - val_loss: 2.4980 - val_accuracy: 0.3680\n",
      "Epoch 4/100\n",
      "391/391 [==============================] - ETA: 0s - loss: 2.4704 - accuracy: 0.3753\n",
      "Epoch 00004: val_accuracy improved from 0.36800 to 0.43140, saving model to VGG_Adam_no_reg_weights_amit.hdf5\n",
      "391/391 [==============================] - 334s 854ms/step - loss: 2.4704 - accuracy: 0.3753 - val_loss: 2.1970 - val_accuracy: 0.4314\n",
      "Epoch 5/100\n",
      "391/391 [==============================] - ETA: 0s - loss: 2.2248 - accuracy: 0.4270\n",
      "Epoch 00005: val_accuracy improved from 0.43140 to 0.46440, saving model to VGG_Adam_no_reg_weights_amit.hdf5\n",
      "391/391 [==============================] - 334s 855ms/step - loss: 2.2248 - accuracy: 0.4270 - val_loss: 2.0573 - val_accuracy: 0.4644\n",
      "Epoch 6/100\n",
      "391/391 [==============================] - ETA: 0s - loss: 2.0253 - accuracy: 0.4676\n",
      "Epoch 00006: val_accuracy improved from 0.46440 to 0.51100, saving model to VGG_Adam_no_reg_weights_amit.hdf5\n",
      "391/391 [==============================] - 334s 854ms/step - loss: 2.0253 - accuracy: 0.4676 - val_loss: 1.8573 - val_accuracy: 0.5110\n",
      "Epoch 7/100\n",
      "391/391 [==============================] - ETA: 0s - loss: 1.8617 - accuracy: 0.5063\n",
      "Epoch 00007: val_accuracy improved from 0.51100 to 0.53570, saving model to VGG_Adam_no_reg_weights_amit.hdf5\n",
      "391/391 [==============================] - 334s 853ms/step - loss: 1.8617 - accuracy: 0.5063 - val_loss: 1.7514 - val_accuracy: 0.5357\n",
      "Epoch 8/100\n",
      "391/391 [==============================] - ETA: 0s - loss: 1.7162 - accuracy: 0.5376\n",
      "Epoch 00008: val_accuracy did not improve from 0.53570\n",
      "391/391 [==============================] - 334s 854ms/step - loss: 1.7162 - accuracy: 0.5376 - val_loss: 1.8021 - val_accuracy: 0.5278\n",
      "Epoch 9/100\n",
      "391/391 [==============================] - ETA: 0s - loss: 1.5882 - accuracy: 0.5702\n",
      "Epoch 00009: val_accuracy improved from 0.53570 to 0.55140, saving model to VGG_Adam_no_reg_weights_amit.hdf5\n",
      "391/391 [==============================] - 333s 851ms/step - loss: 1.5882 - accuracy: 0.5702 - val_loss: 1.7123 - val_accuracy: 0.5514\n",
      "Epoch 10/100\n",
      "391/391 [==============================] - ETA: 0s - loss: 1.4879 - accuracy: 0.5925\n",
      "Epoch 00010: val_accuracy improved from 0.55140 to 0.55870, saving model to VGG_Adam_no_reg_weights_amit.hdf5\n",
      "391/391 [==============================] - 333s 852ms/step - loss: 1.4879 - accuracy: 0.5925 - val_loss: 1.6874 - val_accuracy: 0.5587\n",
      "Epoch 11/100\n",
      "391/391 [==============================] - ETA: 0s - loss: 1.3959 - accuracy: 0.6131\n",
      "Epoch 00011: val_accuracy improved from 0.55870 to 0.58120, saving model to VGG_Adam_no_reg_weights_amit.hdf5\n",
      "391/391 [==============================] - 334s 854ms/step - loss: 1.3959 - accuracy: 0.6131 - val_loss: 1.5745 - val_accuracy: 0.5812\n",
      "Epoch 12/100\n",
      "391/391 [==============================] - ETA: 0s - loss: 1.3058 - accuracy: 0.6328\n",
      "Epoch 00012: val_accuracy did not improve from 0.58120\n",
      "391/391 [==============================] - 334s 854ms/step - loss: 1.3058 - accuracy: 0.6328 - val_loss: 1.7160 - val_accuracy: 0.5647\n",
      "Epoch 13/100\n",
      "391/391 [==============================] - ETA: 0s - loss: 1.2158 - accuracy: 0.6575\n",
      "Epoch 00013: val_accuracy improved from 0.58120 to 0.58170, saving model to VGG_Adam_no_reg_weights_amit.hdf5\n",
      "391/391 [==============================] - 334s 854ms/step - loss: 1.2158 - accuracy: 0.6575 - val_loss: 1.6304 - val_accuracy: 0.5817\n",
      "Epoch 14/100\n",
      "391/391 [==============================] - ETA: 0s - loss: 1.1463 - accuracy: 0.6751\n",
      "Epoch 00014: val_accuracy improved from 0.58170 to 0.59100, saving model to VGG_Adam_no_reg_weights_amit.hdf5\n",
      "391/391 [==============================] - 334s 854ms/step - loss: 1.1463 - accuracy: 0.6751 - val_loss: 1.6432 - val_accuracy: 0.5910\n",
      "Epoch 15/100\n",
      "391/391 [==============================] - ETA: 0s - loss: 1.0931 - accuracy: 0.6884\n",
      "Epoch 00015: val_accuracy improved from 0.59100 to 0.60340, saving model to VGG_Adam_no_reg_weights_amit.hdf5\n",
      "391/391 [==============================] - 334s 854ms/step - loss: 1.0931 - accuracy: 0.6884 - val_loss: 1.5801 - val_accuracy: 0.6034\n",
      "Epoch 16/100\n",
      "391/391 [==============================] - ETA: 0s - loss: 1.0230 - accuracy: 0.7042\n",
      "Epoch 00016: val_accuracy did not improve from 0.60340\n",
      "391/391 [==============================] - 333s 852ms/step - loss: 1.0230 - accuracy: 0.7042 - val_loss: 1.5497 - val_accuracy: 0.6014\n",
      "Epoch 17/100\n",
      "391/391 [==============================] - ETA: 0s - loss: 0.9630 - accuracy: 0.7214\n",
      "Epoch 00017: val_accuracy did not improve from 0.60340\n",
      "391/391 [==============================] - 333s 851ms/step - loss: 0.9630 - accuracy: 0.7214 - val_loss: 1.6544 - val_accuracy: 0.6029\n",
      "Epoch 18/100\n",
      "391/391 [==============================] - ETA: 0s - loss: 0.9142 - accuracy: 0.7354\n",
      "Epoch 00018: val_accuracy improved from 0.60340 to 0.62410, saving model to VGG_Adam_no_reg_weights_amit.hdf5\n",
      "391/391 [==============================] - 333s 852ms/step - loss: 0.9142 - accuracy: 0.7354 - val_loss: 1.5262 - val_accuracy: 0.6241\n",
      "Epoch 19/100\n",
      "391/391 [==============================] - ETA: 0s - loss: 0.8712 - accuracy: 0.7457\n",
      "Epoch 00019: val_accuracy did not improve from 0.62410\n",
      "391/391 [==============================] - 333s 852ms/step - loss: 0.8712 - accuracy: 0.7457 - val_loss: 1.5691 - val_accuracy: 0.6087\n",
      "Epoch 20/100\n",
      "391/391 [==============================] - ETA: 0s - loss: 0.8261 - accuracy: 0.7556\n",
      "Epoch 00020: val_accuracy did not improve from 0.62410\n",
      "391/391 [==============================] - 333s 851ms/step - loss: 0.8261 - accuracy: 0.7556 - val_loss: 1.6591 - val_accuracy: 0.6159\n",
      "Epoch 21/100\n",
      "391/391 [==============================] - ETA: 0s - loss: 0.7841 - accuracy: 0.7682\n",
      "Epoch 00021: val_accuracy did not improve from 0.62410\n",
      "391/391 [==============================] - 333s 851ms/step - loss: 0.7841 - accuracy: 0.7682 - val_loss: 1.6586 - val_accuracy: 0.6053\n",
      "Epoch 22/100\n",
      "391/391 [==============================] - ETA: 0s - loss: 0.7478 - accuracy: 0.7787\n",
      "Epoch 00022: val_accuracy did not improve from 0.62410\n",
      "391/391 [==============================] - 333s 851ms/step - loss: 0.7478 - accuracy: 0.7787 - val_loss: 1.5905 - val_accuracy: 0.6232\n",
      "Epoch 23/100\n",
      "391/391 [==============================] - ETA: 0s - loss: 0.7048 - accuracy: 0.7907\n",
      "Epoch 00023: val_accuracy did not improve from 0.62410\n",
      "Restoring model weights from the end of the best epoch.\n",
      "391/391 [==============================] - 333s 851ms/step - loss: 0.7048 - accuracy: 0.7907 - val_loss: 1.6975 - val_accuracy: 0.6174\n",
      "Epoch 00023: early stopping\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.keras.callbacks.History at 0x7fb044ab8908>"
      ]
     },
     "execution_count": 3,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.fit(aug_data.flow(x_train, y_train, batch_size=128), batch_size=128, epochs=100, verbose=1, validation_data=(x_test, y_test),callbacks=[checkpoint,early])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "id": "THA5F5_K7V7a",
    "outputId": "1870fc2b-9acb-443f-f4b1-054770b82fba"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From <ipython-input-4-44ed652cd446>:1: Sequential.predict_classes (from tensorflow.python.keras.engine.sequential) is deprecated and will be removed after 2021-01-01.\n",
      "Instructions for updating:\n",
      "Please use instead:* `np.argmax(model.predict(x), axis=-1)`,   if your model does multi-class classification   (e.g. if it uses a `softmax` last-layer activation).* `(model.predict(x) > 0.5).astype(\"int32\")`,   if your model does binary classification   (e.g. if it uses a `sigmoid` last-layer activation).\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.81      0.81      0.81       100\n",
      "           1       0.69      0.83      0.75       100\n",
      "           2       0.57      0.56      0.57       100\n",
      "           3       0.48      0.42      0.45       100\n",
      "           4       0.43      0.47      0.45       100\n",
      "           5       0.65      0.67      0.66       100\n",
      "           6       0.74      0.70      0.72       100\n",
      "           7       0.54      0.71      0.61       100\n",
      "           8       0.67      0.78      0.72       100\n",
      "           9       0.70      0.67      0.68       100\n",
      "          10       0.48      0.44      0.46       100\n",
      "          11       0.40      0.61      0.48       100\n",
      "          12       0.62      0.77      0.69       100\n",
      "          13       0.60      0.63      0.61       100\n",
      "          14       0.61      0.65      0.63       100\n",
      "          15       0.55      0.63      0.59       100\n",
      "          16       0.68      0.64      0.66       100\n",
      "          17       0.78      0.76      0.77       100\n",
      "          18       0.71      0.49      0.58       100\n",
      "          19       0.62      0.58      0.60       100\n",
      "          20       0.83      0.78      0.80       100\n",
      "          21       0.85      0.72      0.78       100\n",
      "          22       0.59      0.58      0.59       100\n",
      "          23       0.70      0.70      0.70       100\n",
      "          24       0.86      0.68      0.76       100\n",
      "          25       0.53      0.46      0.49       100\n",
      "          26       0.61      0.70      0.65       100\n",
      "          27       0.52      0.52      0.52       100\n",
      "          28       0.68      0.76      0.72       100\n",
      "          29       0.53      0.65      0.58       100\n",
      "          30       0.52      0.60      0.56       100\n",
      "          31       0.57      0.61      0.59       100\n",
      "          32       0.65      0.57      0.61       100\n",
      "          33       0.52      0.62      0.57       100\n",
      "          34       0.60      0.67      0.63       100\n",
      "          35       0.41      0.32      0.36       100\n",
      "          36       0.83      0.67      0.74       100\n",
      "          37       0.55      0.61      0.58       100\n",
      "          38       0.44      0.63      0.52       100\n",
      "          39       0.54      0.74      0.62       100\n",
      "          40       0.51      0.54      0.53       100\n",
      "          41       0.84      0.81      0.83       100\n",
      "          42       0.57      0.60      0.58       100\n",
      "          43       0.70      0.64      0.67       100\n",
      "          44       0.42      0.21      0.28       100\n",
      "          45       0.59      0.42      0.49       100\n",
      "          46       0.49      0.37      0.42       100\n",
      "          47       0.67      0.55      0.60       100\n",
      "          48       0.68      0.90      0.78       100\n",
      "          49       0.80      0.78      0.79       100\n",
      "          50       0.59      0.40      0.48       100\n",
      "          51       0.55      0.67      0.61       100\n",
      "          52       0.54      0.77      0.63       100\n",
      "          53       0.80      0.84      0.82       100\n",
      "          54       0.77      0.63      0.69       100\n",
      "          55       0.35      0.24      0.28       100\n",
      "          56       0.83      0.80      0.82       100\n",
      "          57       0.62      0.74      0.67       100\n",
      "          58       0.82      0.68      0.74       100\n",
      "          59       0.58      0.62      0.60       100\n",
      "          60       0.73      0.78      0.75       100\n",
      "          61       0.59      0.68      0.63       100\n",
      "          62       0.78      0.69      0.73       100\n",
      "          63       0.70      0.56      0.62       100\n",
      "          64       0.51      0.39      0.44       100\n",
      "          65       0.51      0.39      0.44       100\n",
      "          66       0.52      0.88      0.66       100\n",
      "          67       0.48      0.44      0.46       100\n",
      "          68       0.82      0.90      0.86       100\n",
      "          69       0.70      0.80      0.74       100\n",
      "          70       0.62      0.73      0.67       100\n",
      "          71       0.66      0.81      0.73       100\n",
      "          72       0.44      0.25      0.32       100\n",
      "          73       0.60      0.37      0.46       100\n",
      "          74       0.45      0.49      0.47       100\n",
      "          75       0.86      0.80      0.83       100\n",
      "          76       0.81      0.78      0.80       100\n",
      "          77       0.58      0.60      0.59       100\n",
      "          78       0.38      0.54      0.44       100\n",
      "          79       0.65      0.71      0.68       100\n",
      "          80       0.60      0.33      0.43       100\n",
      "          81       0.53      0.74      0.62       100\n",
      "          82       0.82      0.86      0.84       100\n",
      "          83       0.78      0.47      0.59       100\n",
      "          84       0.61      0.54      0.57       100\n",
      "          85       0.74      0.71      0.72       100\n",
      "          86       0.52      0.65      0.58       100\n",
      "          87       0.62      0.66      0.64       100\n",
      "          88       0.71      0.68      0.69       100\n",
      "          89       0.64      0.71      0.67       100\n",
      "          90       0.67      0.64      0.65       100\n",
      "          91       0.78      0.72      0.75       100\n",
      "          92       0.63      0.52      0.57       100\n",
      "          93       0.55      0.34      0.42       100\n",
      "          94       0.77      0.86      0.82       100\n",
      "          95       0.73      0.61      0.66       100\n",
      "          96       0.52      0.44      0.48       100\n",
      "          97       0.81      0.59      0.68       100\n",
      "          98       0.48      0.41      0.44       100\n",
      "          99       0.63      0.72      0.67       100\n",
      "\n",
      "    accuracy                           0.62     10000\n",
      "   macro avg       0.63      0.62      0.62     10000\n",
      "weighted avg       0.63      0.62      0.62     10000\n",
      "\n",
      "Accuracy is 0.6241\n",
      "Precision is 0.6288195377309753\n",
      "Recall is 0.6241\n"
     ]
    }
   ],
   "source": [
    "y_pred=model.predict_classes(x_test)\n",
    "from sklearn.metrics import classification_report,accuracy_score,recall_score,precision_score\n",
    "print(classification_report(Y_test,y_pred))\n",
    "print(\"Accuracy is {}\".format(accuracy_score(Y_test,y_pred)))\n",
    "print(\"Precision is {}\".format(precision_score(Y_test,y_pred,average=\"weighted\")))\n",
    "print(\"Recall is {}\".format(recall_score(Y_test,y_pred,average=\"weighted\")))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "iagCxNQb7YHT"
   },
   "outputs": [],
   "source": [
    "YY=model.predict_classes(x_train)\n",
    "print(\"Accuracy is {}\".format(accuracy_score(Y_train,YY)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "icainBKXvTos"
   },
   "outputs": [],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From C:\\Users\\uamit\\Anaconda3\\lib\\site-packages\\keras\\backend\\tensorflow_backend.py:4070: The name tf.nn.max_pool is deprecated. Please use tf.nn.max_pool2d instead.\n",
      "\n",
      "WARNING:tensorflow:From C:\\Users\\uamit\\Anaconda3\\lib\\site-packages\\keras\\backend\\tensorflow_backend.py:422: The name tf.global_variables is deprecated. Please use tf.compat.v1.global_variables instead.\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.81      0.81      0.81       100\n",
      "           1       0.69      0.83      0.75       100\n",
      "           2       0.57      0.56      0.57       100\n",
      "           3       0.48      0.42      0.45       100\n",
      "           4       0.43      0.47      0.45       100\n",
      "           5       0.65      0.67      0.66       100\n",
      "           6       0.74      0.70      0.72       100\n",
      "           7       0.54      0.71      0.61       100\n",
      "           8       0.67      0.78      0.72       100\n",
      "           9       0.70      0.67      0.68       100\n",
      "          10       0.48      0.44      0.46       100\n",
      "          11       0.40      0.61      0.48       100\n",
      "          12       0.62      0.77      0.69       100\n",
      "          13       0.60      0.63      0.61       100\n",
      "          14       0.61      0.65      0.63       100\n",
      "          15       0.55      0.63      0.59       100\n",
      "          16       0.68      0.64      0.66       100\n",
      "          17       0.78      0.76      0.77       100\n",
      "          18       0.71      0.49      0.58       100\n",
      "          19       0.62      0.58      0.60       100\n",
      "          20       0.83      0.78      0.80       100\n",
      "          21       0.85      0.72      0.78       100\n",
      "          22       0.59      0.58      0.59       100\n",
      "          23       0.70      0.70      0.70       100\n",
      "          24       0.86      0.68      0.76       100\n",
      "          25       0.53      0.46      0.49       100\n",
      "          26       0.61      0.70      0.65       100\n",
      "          27       0.52      0.52      0.52       100\n",
      "          28       0.68      0.76      0.72       100\n",
      "          29       0.53      0.65      0.58       100\n",
      "          30       0.52      0.60      0.56       100\n",
      "          31       0.57      0.61      0.59       100\n",
      "          32       0.65      0.57      0.61       100\n",
      "          33       0.52      0.62      0.57       100\n",
      "          34       0.60      0.67      0.63       100\n",
      "          35       0.41      0.32      0.36       100\n",
      "          36       0.83      0.67      0.74       100\n",
      "          37       0.55      0.61      0.58       100\n",
      "          38       0.44      0.63      0.52       100\n",
      "          39       0.54      0.74      0.62       100\n",
      "          40       0.51      0.54      0.53       100\n",
      "          41       0.84      0.81      0.83       100\n",
      "          42       0.57      0.60      0.58       100\n",
      "          43       0.70      0.64      0.67       100\n",
      "          44       0.42      0.21      0.28       100\n",
      "          45       0.59      0.42      0.49       100\n",
      "          46       0.49      0.37      0.42       100\n",
      "          47       0.67      0.55      0.60       100\n",
      "          48       0.68      0.90      0.78       100\n",
      "          49       0.80      0.78      0.79       100\n",
      "          50       0.59      0.40      0.48       100\n",
      "          51       0.55      0.67      0.61       100\n",
      "          52       0.54      0.77      0.63       100\n",
      "          53       0.80      0.84      0.82       100\n",
      "          54       0.77      0.63      0.69       100\n",
      "          55       0.35      0.24      0.28       100\n",
      "          56       0.83      0.80      0.82       100\n",
      "          57       0.62      0.74      0.67       100\n",
      "          58       0.82      0.68      0.74       100\n",
      "          59       0.58      0.62      0.60       100\n",
      "          60       0.73      0.78      0.75       100\n",
      "          61       0.59      0.68      0.63       100\n",
      "          62       0.78      0.69      0.73       100\n",
      "          63       0.70      0.56      0.62       100\n",
      "          64       0.51      0.39      0.44       100\n",
      "          65       0.51      0.39      0.44       100\n",
      "          66       0.52      0.88      0.66       100\n",
      "          67       0.48      0.44      0.46       100\n",
      "          68       0.82      0.90      0.86       100\n",
      "          69       0.70      0.80      0.74       100\n",
      "          70       0.62      0.73      0.67       100\n",
      "          71       0.66      0.81      0.73       100\n",
      "          72       0.44      0.25      0.32       100\n",
      "          73       0.60      0.37      0.46       100\n",
      "          74       0.45      0.49      0.47       100\n",
      "          75       0.86      0.80      0.83       100\n",
      "          76       0.81      0.78      0.80       100\n",
      "          77       0.58      0.60      0.59       100\n",
      "          78       0.38      0.54      0.44       100\n",
      "          79       0.65      0.71      0.68       100\n",
      "          80       0.60      0.33      0.43       100\n",
      "          81       0.53      0.74      0.62       100\n",
      "          82       0.82      0.86      0.84       100\n",
      "          83       0.78      0.47      0.59       100\n",
      "          84       0.61      0.54      0.57       100\n",
      "          85       0.74      0.71      0.72       100\n",
      "          86       0.52      0.65      0.58       100\n",
      "          87       0.62      0.66      0.64       100\n",
      "          88       0.71      0.68      0.69       100\n",
      "          89       0.64      0.71      0.67       100\n",
      "          90       0.67      0.64      0.65       100\n",
      "          91       0.78      0.72      0.75       100\n",
      "          92       0.63      0.52      0.57       100\n",
      "          93       0.55      0.34      0.42       100\n",
      "          94       0.77      0.86      0.82       100\n",
      "          95       0.73      0.61      0.66       100\n",
      "          96       0.52      0.44      0.48       100\n",
      "          97       0.81      0.59      0.68       100\n",
      "          98       0.48      0.41      0.44       100\n",
      "          99       0.63      0.72      0.67       100\n",
      "\n",
      "    accuracy                           0.62     10000\n",
      "   macro avg       0.63      0.62      0.62     10000\n",
      "weighted avg       0.63      0.62      0.62     10000\n",
      "\n",
      "Accuracy is 0.6241\n",
      "Precision is 0.6288195377309753\n",
      "Recall is 0.6241\n"
     ]
    }
   ],
   "source": [
    "from keras.callbacks import ModelCheckpoint, EarlyStopping\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Conv2D, MaxPool2D, Flatten, Dense,BatchNormalization,Activation,Dropout,LeakyReLU\n",
    "from tensorflow.keras.optimizers import SGD,Adam\n",
    "from keras.preprocessing.image import ImageDataGenerator\n",
    "from keras.utils import to_categorical\n",
    "from tensorflow.keras.datasets import cifar100\n",
    "\n",
    "\n",
    "\n",
    "(x_train, Y_train), (x_test, Y_test) = cifar100.load_data()\n",
    "#x_train = x_train.astype('float32') / 255\n",
    "#x_test = x_test.astype('float32') / 255\n",
    "\n",
    "y_train = to_categorical(Y_train)\n",
    "y_test = to_categorical(Y_test)\n",
    "\n",
    "aug_data=ImageDataGenerator(rotation_range=20,horizontal_flip=True,width_shift_range=0.1,shear_range = 0.2,height_shift_range=0.1,zoom_range=0.2,brightness_range = (0.5, 1.5))\n",
    "aug_data.fit(x_train)\n",
    "\n",
    "\n",
    "#import tensorflow as \n",
    "model = Sequential()\n",
    "\n",
    "# Creating first block- (2 Convolution + 1 Max pool)\n",
    "model.add(Conv2D(filters= 64, kernel_size= (3,3), strides= (1,1), padding='same', input_shape= (32, 32, 3)))\n",
    "model.add(LeakyReLU(alpha=0.1))\n",
    "model.add(Conv2D(filters= 64, kernel_size= (3,3), strides= (1,1), padding='same'))\n",
    "model.add(LeakyReLU(alpha=0.1))\n",
    "model.add(MaxPool2D(pool_size= (2,2), strides=(1,1)))\n",
    "\n",
    "# Creating second block- (2 Convolution + 1 Max pool)\n",
    "model.add(Conv2D(filters= 128, kernel_size= (3,3), strides= (1,1), padding='same'))\n",
    "model.add(LeakyReLU(alpha=0.1))\n",
    "model.add(Conv2D(filters= 128, kernel_size= (3,3), strides= (1,1), padding='same'))\n",
    "model.add(LeakyReLU(alpha=0.1))\n",
    "model.add(MaxPool2D(pool_size= (2,2), strides=(1,1)))\n",
    "\n",
    "\n",
    "# Creating third block- (3 Convolution + 1 Max pool)\n",
    "model.add(Conv2D(filters= 256, kernel_size= (3,3), strides= (1,1), padding='same'))\n",
    "model.add(LeakyReLU(alpha=0.1))\n",
    "model.add(Conv2D(filters= 256, kernel_size= (3,3), strides= (1,1), padding='same'))\n",
    "model.add(LeakyReLU(alpha=0.1))\n",
    "model.add(Conv2D(filters= 256, kernel_size= (3,3), strides= (1,1), padding='same'))\n",
    "model.add(LeakyReLU(alpha=0.1))\n",
    "model.add(MaxPool2D(pool_size= (2,2), strides=(1,1)))\n",
    "\n",
    "# Creating fourth block- (3 Convolution + 1 Max pool)\n",
    "model.add(Conv2D(filters= 512, kernel_size= (3,3), strides= (1,1), padding='same'))\n",
    "model.add(LeakyReLU(alpha=0.1))\n",
    "model.add(Conv2D(filters= 512, kernel_size= (3,3), strides= (1,1), padding='same'))\n",
    "model.add(LeakyReLU(alpha=0.1))\n",
    "model.add(Conv2D(filters= 512, kernel_size= (3,3), strides= (1,1), padding='same'))\n",
    "model.add(LeakyReLU(alpha=0.1))\n",
    "model.add(MaxPool2D(pool_size= (2,2), strides=(2,2)))\n",
    "\n",
    "# Creating fifth block- (3 Convolution + 1 Max pool)\n",
    "model.add(Conv2D(filters= 512, kernel_size= (3,3), strides= (1,1), padding='same'))\n",
    "model.add(LeakyReLU(alpha=0.1))\n",
    "model.add(Conv2D(filters= 512, kernel_size= (3,3), strides= (1,1), padding='same'))\n",
    "model.add(LeakyReLU(alpha=0.1))\n",
    "model.add(Conv2D(filters= 512, kernel_size= (3,3), strides= (1,1), padding='same'))\n",
    "model.add(LeakyReLU(alpha=0.1))\n",
    "model.add(MaxPool2D(pool_size= (2,2), strides=(1,1)))\n",
    "\n",
    "# Flattening the pooled image pixels\n",
    "model.add(Flatten())\n",
    "\n",
    "# Creating 2 Dense Layers\n",
    "model.add(Dense(units= 512))\n",
    "model.add(LeakyReLU(alpha=0.1))\n",
    "\n",
    "model.add(Dense(units= 100, activation='softmax'))\n",
    "\n",
    "\n",
    "#adam=Adam(learning_rate=0.0001,clipnorm=1,name='adam')\n",
    "\n",
    "model.load_weights('../weights/VGG_Adam_no_reg_weights_amit.hdf5')\n",
    "\n",
    "y_pred=model.predict_classes(x_test)\n",
    "from sklearn.metrics import classification_report,accuracy_score,recall_score,precision_score\n",
    "print(classification_report(Y_test,y_pred))\n",
    "print(\"Accuracy is {}\".format(accuracy_score(Y_test,y_pred)))\n",
    "print(\"Precision is {}\".format(precision_score(Y_test,y_pred,average=\"weighted\")))\n",
    "print(\"Recall is {}\".format(recall_score(Y_test,y_pred,average=\"weighted\")))\n",
    "\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "name": "VGG_Adam_no_reg_weights_amit.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
