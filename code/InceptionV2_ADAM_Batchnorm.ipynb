{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "id": "XeTmuGgr5lDw"
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from tensorflow.keras.models import Sequential,Model\n",
    "from tensorflow.keras.layers import Dense,Flatten,Input,Conv2D,MaxPool2D,concatenate,BatchNormalization\n",
    "from keras.layers.convolutional import MaxPooling2D\n",
    "from keras.layers.merge import concatenate\n",
    "from keras.layers import Concatenate, Dense, LSTM, Input, concatenate\n",
    "from keras.layers import Conv2D\n",
    "\n",
    "from tensorflow.keras import layers\n",
    "\n",
    "import tensorflow.keras as keras\n",
    "\n",
    "import tensorflow.keras as keras\n",
    "\n",
    "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
    "\n",
    "from tensorflow.keras.layers import LeakyReLU\n",
    "\n",
    "from tensorflow.keras.datasets import cifar100\n",
    "from tensorflow.keras.utils import to_categorical\n",
    "\n",
    "from tensorflow.keras.callbacks import EarlyStopping,ModelCheckpoint\n",
    "\n",
    "from keras.optimizers import Adam,SGD\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "id": "pI2HWz025vav"
   },
   "outputs": [],
   "source": [
    "(X_train,Y_train),(X_test,Y_test)=cifar100.load_data()\n",
    "y_train,y_test=to_categorical(Y_train),to_categorical(Y_test)\n",
    "X_train,X_test=X_train/255,X_test/255\n",
    "\n",
    "aug_data=ImageDataGenerator(rotation_range=15,horizontal_flip=True,width_shift_range=0.1,height_shift_range=0.1,zoom_range=0.3)\n",
    "aug_data.fit(X_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "id": "Dc1L_Wvf5yy5",
    "outputId": "01225104-ed26-48ec-b0a0-4d829f5b8887"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:`period` argument is deprecated. Please use `save_freq` to specify the frequency in number of batches seen.\n"
     ]
    }
   ],
   "source": [
    "\n",
    "inputs=keras.Input(shape=(32,32,3))\n",
    "\n",
    "a=Conv2D(filters=32,kernel_size=(3,3),padding=\"same\",strides=2,activation=LeakyReLU(alpha=0.1))(inputs)\n",
    "a=layers.BatchNormalization()(a)\n",
    "\n",
    "\n",
    "a=Conv2D(filters=32,kernel_size=(3,3),padding=\"same\",strides=1,activation=LeakyReLU(alpha=0.1))(a)\n",
    "a=layers.BatchNormalization()(a)\n",
    "\n",
    "\n",
    "a=Conv2D(filters=32, kernel_size=(3,3), padding=\"same\",strides=1,activation=LeakyReLU(alpha=0.1))(a)\n",
    "a=layers.BatchNormalization()(a)\n",
    "\n",
    "a=(MaxPool2D(pool_size=(3,3),strides=(2,2)))(a)\n",
    "\n",
    "\n",
    "a=Conv2D(filters=32, kernel_size=(3,3), padding=\"same\",strides=1,activation=LeakyReLU(alpha=0.1))(a)\n",
    "a=layers.BatchNormalization()(a)\n",
    "\n",
    "\n",
    "a=Conv2D(filters=32, kernel_size=(3,3), padding=\"same\",strides=2,activation=LeakyReLU(alpha=0.1))(a)\n",
    "a=layers.BatchNormalization()(a)\n",
    "\n",
    "\n",
    "a=Conv2D(filters=32, kernel_size=(3,3), padding=\"same\",strides=1,activation=LeakyReLU(alpha=0.1))(a)\n",
    "a=layers.BatchNormalization()(a)\n",
    "\n",
    "\n",
    "#inception-1\n",
    "b=Conv2D(64, (1,1), padding='same',strides=1,activation=LeakyReLU(alpha=0.1))(a)\n",
    "b=layers.BatchNormalization()(b)\n",
    "\n",
    "b=Conv2D(64, (3,3), padding='same',strides=1 ,activation=LeakyReLU(alpha=0.1))(b)\n",
    "b=layers.BatchNormalization()(b)\n",
    "\n",
    "b=Conv2D(64, (3,3), padding='same', strides=1,activation=LeakyReLU(alpha=0.1))(b)\n",
    "b=layers.BatchNormalization()(b)\n",
    "\n",
    "\n",
    "\n",
    "c=Conv2D(64, (1,1), padding='same', strides=1, activation=LeakyReLU(alpha=0.1))(a)\n",
    "c=layers.BatchNormalization()(c)\n",
    "\n",
    "c=Conv2D(64, (3,3), padding='same', strides=1, activation=LeakyReLU(alpha=0.1))(c)\n",
    "c=layers.BatchNormalization()(c)\n",
    "\n",
    "\n",
    "d=MaxPooling2D((3,3), strides=(1,1), padding='same')(a)\n",
    "d=layers.BatchNormalization()(d)\n",
    "\n",
    "d=Conv2D(64, (1,1), padding='same', strides=1, activation=LeakyReLU(alpha=0.1))(d)\n",
    "d=layers.BatchNormalization()(d)\n",
    "\n",
    "\n",
    "e=Conv2D(64, (1,1), padding='same', strides=1, activation=LeakyReLU(alpha=0.1))(a)\n",
    "e=layers.BatchNormalization()(e)\n",
    "\n",
    "\n",
    "a=concatenate([b, c, d,e],axis=3)\n",
    "\n",
    "#inception-2\n",
    "f=Conv2D(128, (1,1), padding='same', strides=1, activation=LeakyReLU(alpha=0.1))(a)\n",
    "f=layers.BatchNormalization()(f)\n",
    "\n",
    "f=Conv2D(128, (1,7), padding='same', strides=1, activation=LeakyReLU(alpha=0.1))(f)\n",
    "f=layers.BatchNormalization()(f)\n",
    "\n",
    "\n",
    "f=Conv2D(128, (7,1), padding='same', strides=1, activation=LeakyReLU(alpha=0.1))(f)\n",
    "f=layers.BatchNormalization()(f)\n",
    "\n",
    "f=Conv2D(128, (1,7), padding='same', strides=1, activation=LeakyReLU(alpha=0.1))(f)\n",
    "f=layers.BatchNormalization()(f)\n",
    "\n",
    "\n",
    "f=Conv2D(128, (7,1), padding='same', strides=1, activation=LeakyReLU(alpha=0.1))(f)\n",
    "f=layers.BatchNormalization()(f)\n",
    "\n",
    "\n",
    "g=Conv2D(128, (1,1), padding='same', strides=1, activation=LeakyReLU(alpha=0.1))(a)\n",
    "g=layers.BatchNormalization()(g)\n",
    "\n",
    "g=Conv2D(128, (1,7), padding='same', strides=1,activation=LeakyReLU(alpha=0.1))(g)\n",
    "g=layers.BatchNormalization()(g)\n",
    "\n",
    "g=Conv2D(128, (7,1), padding='same', strides=1, activation=LeakyReLU(alpha=0.1))(g)\n",
    "\n",
    "g=layers.BatchNormalization()(g)\n",
    "\n",
    "\n",
    "h=MaxPooling2D((3,3), strides=(1,1), padding='same')(a)\n",
    "h=layers.BatchNormalization()(h)\n",
    "\n",
    "h=Conv2D(128, (1,1), padding='same', strides=1, activation=LeakyReLU(alpha=0.1))(h)\n",
    "h=layers.BatchNormalization()(h)\n",
    "\n",
    "\n",
    "i=Conv2D(128, (1,1), padding='same', strides=1, activation=LeakyReLU(alpha=0.1))(h)\n",
    "i=layers.BatchNormalization()(i)\n",
    "\n",
    "\n",
    "a=concatenate([f,g,h,i],axis=3)\n",
    "\n",
    "#inception-3\n",
    "j=Conv2D(256, (1,1), padding='same', strides=1, activation=LeakyReLU(alpha=0.1))(a)\n",
    "j=layers.BatchNormalization()(j)\n",
    "\n",
    "\n",
    "j=Conv2D(256, (3,3), padding='same', strides=1, activation=LeakyReLU(alpha=0.1))(j)\n",
    "j=layers.BatchNormalization()(j)\n",
    "\n",
    "k=Conv2D(256, (1,3), padding='same', strides=1, activation=LeakyReLU(alpha=0.1))(j)\n",
    "k=layers.BatchNormalization()(k)\n",
    "\n",
    "l=Conv2D(256, (3,1), padding='same', strides=1, activation=LeakyReLU(alpha=0.1))(j)\n",
    "l=layers.BatchNormalization()(l)\n",
    "\n",
    "\n",
    "m=Conv2D(256, (1,1), padding='same', activation=LeakyReLU(alpha=0.1))(a)\n",
    "m=layers.BatchNormalization()(m)\n",
    "\n",
    "n=Conv2D(256, (1,3), padding='same', activation=LeakyReLU(alpha=0.1))(m)\n",
    "n=layers.BatchNormalization()(n)\n",
    "\n",
    "o=Conv2D(256, (3,1), padding='same', activation=LeakyReLU(alpha=0.1))(m)\n",
    "o=layers.BatchNormalization()(o)\n",
    "\n",
    "\n",
    "p=MaxPool2D((3,3), strides=(1,1), padding='same')(a)\n",
    "p=Conv2D(256, (1,1), padding='same', strides=1, activation=LeakyReLU(alpha=0.1))(p)\n",
    "p=layers.BatchNormalization()(p)\n",
    "\n",
    "\n",
    "q=Conv2D(256, (1,1), padding='same', strides=1, activation=LeakyReLU(alpha=0.1))(a)\n",
    "q=layers.BatchNormalization()(q)\n",
    "\n",
    "\n",
    "a=concatenate([k,l,n,o,p,q],axis=3)\n",
    "\n",
    "a=MaxPooling2D((8,8), strides=(1,1), padding='same')(a)\n",
    "a=layers.BatchNormalization()(a)\n",
    "\n",
    "\n",
    "a=Flatten()(a)\n",
    "\n",
    "a=Dense(units=256,activation=LeakyReLU(alpha=0.3))(a)\n",
    "a=layers.BatchNormalization()(a)\n",
    "\n",
    "\n",
    "outputs=Dense(y_train.shape[1],activation=\"softmax\")(a)\n",
    "\n",
    "model=keras.Model(inputs,outputs)\n",
    "\n",
    "adam=Adam(learning_rate=0.001,clipnorm=0.7)\n",
    "\n",
    "model.compile(loss=\"categorical_crossentropy\",optimizer=adam,metrics=['accuracy'])\n",
    "\n",
    "\n",
    "call=EarlyStopping(monitor=\"val_loss\",verbose=1,mode=\"auto\",patience=10,restore_best_weights=\"true\")\n",
    "\n",
    "checkpoint=ModelCheckpoint('Inception_adam_Batchnorm_regularization.hdf5',monitor='val_accuracy',verbose=1,save_best_only=True,save_weights_only=True,model='auto',period=1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "id": "Fu1NT8Qk53Wy",
    "outputId": "ca774a70-f80d-46e0-911b-4eb85de4295e"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/80\n",
      "391/390 [==============================] - ETA: 0s - loss: 3.7397 - accuracy: 0.1288\n",
      "Epoch 00001: val_accuracy improved from -inf to 0.07970, saving model to Inception_adam_Batchnorm_regularization.hdf5\n",
      "391/390 [==============================] - 30s 77ms/step - loss: 3.7397 - accuracy: 0.1288 - val_loss: 4.3203 - val_accuracy: 0.0797\n",
      "Epoch 2/80\n",
      "391/390 [==============================] - ETA: 0s - loss: 3.2357 - accuracy: 0.2083\n",
      "Epoch 00002: val_accuracy improved from 0.07970 to 0.21660, saving model to Inception_adam_Batchnorm_regularization.hdf5\n",
      "391/390 [==============================] - 29s 75ms/step - loss: 3.2357 - accuracy: 0.2083 - val_loss: 3.3158 - val_accuracy: 0.2166\n",
      "Epoch 3/80\n",
      "391/390 [==============================] - ETA: 0s - loss: 2.9750 - accuracy: 0.2568\n",
      "Epoch 00003: val_accuracy improved from 0.21660 to 0.25650, saving model to Inception_adam_Batchnorm_regularization.hdf5\n",
      "391/390 [==============================] - 30s 76ms/step - loss: 2.9750 - accuracy: 0.2568 - val_loss: 3.0908 - val_accuracy: 0.2565\n",
      "Epoch 4/80\n",
      "391/390 [==============================] - ETA: 0s - loss: 2.7872 - accuracy: 0.2927\n",
      "Epoch 00004: val_accuracy improved from 0.25650 to 0.29640, saving model to Inception_adam_Batchnorm_regularization.hdf5\n",
      "391/390 [==============================] - 30s 75ms/step - loss: 2.7872 - accuracy: 0.2927 - val_loss: 2.9032 - val_accuracy: 0.2964\n",
      "Epoch 5/80\n",
      "391/390 [==============================] - ETA: 0s - loss: 2.6323 - accuracy: 0.3239\n",
      "Epoch 00005: val_accuracy improved from 0.29640 to 0.30300, saving model to Inception_adam_Batchnorm_regularization.hdf5\n",
      "391/390 [==============================] - 30s 76ms/step - loss: 2.6323 - accuracy: 0.3239 - val_loss: 2.8757 - val_accuracy: 0.3030\n",
      "Epoch 6/80\n",
      "391/390 [==============================] - ETA: 0s - loss: 2.5030 - accuracy: 0.3527\n",
      "Epoch 00006: val_accuracy improved from 0.30300 to 0.35240, saving model to Inception_adam_Batchnorm_regularization.hdf5\n",
      "391/390 [==============================] - 30s 77ms/step - loss: 2.5030 - accuracy: 0.3527 - val_loss: 2.5722 - val_accuracy: 0.3524\n",
      "Epoch 7/80\n",
      "391/390 [==============================] - ETA: 0s - loss: 2.3942 - accuracy: 0.3746\n",
      "Epoch 00007: val_accuracy improved from 0.35240 to 0.35880, saving model to Inception_adam_Batchnorm_regularization.hdf5\n",
      "391/390 [==============================] - 30s 76ms/step - loss: 2.3942 - accuracy: 0.3746 - val_loss: 2.5624 - val_accuracy: 0.3588\n",
      "Epoch 8/80\n",
      "391/390 [==============================] - ETA: 0s - loss: 2.2992 - accuracy: 0.3947\n",
      "Epoch 00008: val_accuracy improved from 0.35880 to 0.39900, saving model to Inception_adam_Batchnorm_regularization.hdf5\n",
      "391/390 [==============================] - 30s 76ms/step - loss: 2.2992 - accuracy: 0.3947 - val_loss: 2.3331 - val_accuracy: 0.3990\n",
      "Epoch 9/80\n",
      "391/390 [==============================] - ETA: 0s - loss: 2.2135 - accuracy: 0.4108\n",
      "Epoch 00009: val_accuracy did not improve from 0.39900\n",
      "391/390 [==============================] - 29s 75ms/step - loss: 2.2135 - accuracy: 0.4108 - val_loss: 2.3264 - val_accuracy: 0.3896\n",
      "Epoch 10/80\n",
      "391/390 [==============================] - ETA: 0s - loss: 2.1360 - accuracy: 0.4289\n",
      "Epoch 00010: val_accuracy did not improve from 0.39900\n",
      "391/390 [==============================] - 30s 76ms/step - loss: 2.1360 - accuracy: 0.4289 - val_loss: 2.3774 - val_accuracy: 0.3979\n",
      "Epoch 11/80\n",
      "391/390 [==============================] - ETA: 0s - loss: 2.0589 - accuracy: 0.4472\n",
      "Epoch 00011: val_accuracy improved from 0.39900 to 0.41350, saving model to Inception_adam_Batchnorm_regularization.hdf5\n",
      "391/390 [==============================] - 30s 76ms/step - loss: 2.0589 - accuracy: 0.4472 - val_loss: 2.3047 - val_accuracy: 0.4135\n",
      "Epoch 12/80\n",
      "391/390 [==============================] - ETA: 0s - loss: 1.9879 - accuracy: 0.4632\n",
      "Epoch 00012: val_accuracy improved from 0.41350 to 0.43960, saving model to Inception_adam_Batchnorm_regularization.hdf5\n",
      "391/390 [==============================] - 30s 76ms/step - loss: 1.9879 - accuracy: 0.4632 - val_loss: 2.1952 - val_accuracy: 0.4396\n",
      "Epoch 13/80\n",
      "391/390 [==============================] - ETA: 0s - loss: 1.9287 - accuracy: 0.4760\n",
      "Epoch 00013: val_accuracy improved from 0.43960 to 0.45310, saving model to Inception_adam_Batchnorm_regularization.hdf5\n",
      "391/390 [==============================] - 30s 77ms/step - loss: 1.9287 - accuracy: 0.4760 - val_loss: 2.1037 - val_accuracy: 0.4531\n",
      "Epoch 14/80\n",
      "391/390 [==============================] - ETA: 0s - loss: 1.8753 - accuracy: 0.4889\n",
      "Epoch 00014: val_accuracy did not improve from 0.45310\n",
      "391/390 [==============================] - 29s 75ms/step - loss: 1.8753 - accuracy: 0.4889 - val_loss: 2.1184 - val_accuracy: 0.4516\n",
      "Epoch 15/80\n",
      "391/390 [==============================] - ETA: 0s - loss: 1.8150 - accuracy: 0.5014\n",
      "Epoch 00015: val_accuracy improved from 0.45310 to 0.46720, saving model to Inception_adam_Batchnorm_regularization.hdf5\n",
      "391/390 [==============================] - 30s 76ms/step - loss: 1.8150 - accuracy: 0.5014 - val_loss: 2.0605 - val_accuracy: 0.4672\n",
      "Epoch 16/80\n",
      "391/390 [==============================] - ETA: 0s - loss: 1.7645 - accuracy: 0.5136\n",
      "Epoch 00016: val_accuracy improved from 0.46720 to 0.47520, saving model to Inception_adam_Batchnorm_regularization.hdf5\n",
      "391/390 [==============================] - 30s 76ms/step - loss: 1.7645 - accuracy: 0.5136 - val_loss: 2.0413 - val_accuracy: 0.4752\n",
      "Epoch 17/80\n",
      "391/390 [==============================] - ETA: 0s - loss: 1.7099 - accuracy: 0.5267\n",
      "Epoch 00017: val_accuracy did not improve from 0.47520\n",
      "391/390 [==============================] - 30s 76ms/step - loss: 1.7099 - accuracy: 0.5267 - val_loss: 2.1003 - val_accuracy: 0.4672\n",
      "Epoch 18/80\n",
      "391/390 [==============================] - ETA: 0s - loss: 1.6752 - accuracy: 0.5340\n",
      "Epoch 00018: val_accuracy improved from 0.47520 to 0.47730, saving model to Inception_adam_Batchnorm_regularization.hdf5\n",
      "391/390 [==============================] - 30s 76ms/step - loss: 1.6752 - accuracy: 0.5340 - val_loss: 2.0468 - val_accuracy: 0.4773\n",
      "Epoch 19/80\n",
      "391/390 [==============================] - ETA: 0s - loss: 1.6347 - accuracy: 0.5459\n",
      "Epoch 00019: val_accuracy improved from 0.47730 to 0.48760, saving model to Inception_adam_Batchnorm_regularization.hdf5\n",
      "391/390 [==============================] - 30s 76ms/step - loss: 1.6347 - accuracy: 0.5459 - val_loss: 1.9871 - val_accuracy: 0.4876\n",
      "Epoch 20/80\n",
      "391/390 [==============================] - ETA: 0s - loss: 1.5775 - accuracy: 0.5581\n",
      "Epoch 00020: val_accuracy did not improve from 0.48760\n",
      "391/390 [==============================] - 29s 75ms/step - loss: 1.5775 - accuracy: 0.5581 - val_loss: 1.9970 - val_accuracy: 0.4871\n",
      "Epoch 21/80\n",
      "391/390 [==============================] - ETA: 0s - loss: 1.5430 - accuracy: 0.5644\n",
      "Epoch 00021: val_accuracy improved from 0.48760 to 0.49030, saving model to Inception_adam_Batchnorm_regularization.hdf5\n",
      "391/390 [==============================] - 30s 75ms/step - loss: 1.5430 - accuracy: 0.5644 - val_loss: 1.9846 - val_accuracy: 0.4903\n",
      "Epoch 22/80\n",
      "391/390 [==============================] - ETA: 0s - loss: 1.4901 - accuracy: 0.5786\n",
      "Epoch 00022: val_accuracy improved from 0.49030 to 0.49720, saving model to Inception_adam_Batchnorm_regularization.hdf5\n",
      "391/390 [==============================] - 30s 76ms/step - loss: 1.4901 - accuracy: 0.5786 - val_loss: 1.9674 - val_accuracy: 0.4972\n",
      "Epoch 23/80\n",
      "391/390 [==============================] - ETA: 0s - loss: 1.4607 - accuracy: 0.5869\n",
      "Epoch 00023: val_accuracy improved from 0.49720 to 0.50430, saving model to Inception_adam_Batchnorm_regularization.hdf5\n",
      "391/390 [==============================] - 30s 76ms/step - loss: 1.4607 - accuracy: 0.5869 - val_loss: 1.9399 - val_accuracy: 0.5043\n",
      "Epoch 24/80\n",
      "391/390 [==============================] - ETA: 0s - loss: 1.4215 - accuracy: 0.5949\n",
      "Epoch 00024: val_accuracy improved from 0.50430 to 0.50870, saving model to Inception_adam_Batchnorm_regularization.hdf5\n",
      "391/390 [==============================] - 30s 76ms/step - loss: 1.4215 - accuracy: 0.5949 - val_loss: 1.9366 - val_accuracy: 0.5087\n",
      "Epoch 25/80\n",
      "391/390 [==============================] - ETA: 0s - loss: 1.3749 - accuracy: 0.6063\n",
      "Epoch 00025: val_accuracy did not improve from 0.50870\n",
      "391/390 [==============================] - 29s 75ms/step - loss: 1.3749 - accuracy: 0.6063 - val_loss: 1.9995 - val_accuracy: 0.4996\n",
      "Epoch 26/80\n",
      "391/390 [==============================] - ETA: 0s - loss: 1.3436 - accuracy: 0.6155\n",
      "Epoch 00026: val_accuracy did not improve from 0.50870\n",
      "391/390 [==============================] - 29s 75ms/step - loss: 1.3436 - accuracy: 0.6155 - val_loss: 1.9870 - val_accuracy: 0.5051\n",
      "Epoch 27/80\n",
      "391/390 [==============================] - ETA: 0s - loss: 1.3122 - accuracy: 0.6197\n",
      "Epoch 00027: val_accuracy improved from 0.50870 to 0.51030, saving model to Inception_adam_Batchnorm_regularization.hdf5\n",
      "391/390 [==============================] - 29s 75ms/step - loss: 1.3122 - accuracy: 0.6197 - val_loss: 1.9504 - val_accuracy: 0.5103\n",
      "Epoch 28/80\n",
      "391/390 [==============================] - ETA: 0s - loss: 1.2767 - accuracy: 0.6305\n",
      "Epoch 00028: val_accuracy improved from 0.51030 to 0.51810, saving model to Inception_adam_Batchnorm_regularization.hdf5\n",
      "391/390 [==============================] - 29s 75ms/step - loss: 1.2767 - accuracy: 0.6305 - val_loss: 1.9275 - val_accuracy: 0.5181\n",
      "Epoch 29/80\n",
      "391/390 [==============================] - ETA: 0s - loss: 1.2347 - accuracy: 0.6425\n",
      "Epoch 00029: val_accuracy did not improve from 0.51810\n",
      "391/390 [==============================] - 29s 75ms/step - loss: 1.2347 - accuracy: 0.6425 - val_loss: 1.9975 - val_accuracy: 0.5088\n",
      "Epoch 30/80\n",
      "391/390 [==============================] - ETA: 0s - loss: 1.2155 - accuracy: 0.6452\n",
      "Epoch 00030: val_accuracy did not improve from 0.51810\n",
      "391/390 [==============================] - 29s 75ms/step - loss: 1.2155 - accuracy: 0.6452 - val_loss: 2.0007 - val_accuracy: 0.5143\n",
      "Epoch 31/80\n",
      "391/390 [==============================] - ETA: 0s - loss: 1.1731 - accuracy: 0.6576\n",
      "Epoch 00031: val_accuracy did not improve from 0.51810\n",
      "391/390 [==============================] - 29s 75ms/step - loss: 1.1731 - accuracy: 0.6576 - val_loss: 2.0172 - val_accuracy: 0.5086\n",
      "Epoch 32/80\n",
      "391/390 [==============================] - ETA: 0s - loss: 1.1453 - accuracy: 0.6639\n",
      "Epoch 00032: val_accuracy did not improve from 0.51810\n",
      "391/390 [==============================] - 29s 75ms/step - loss: 1.1453 - accuracy: 0.6639 - val_loss: 2.0102 - val_accuracy: 0.5132\n",
      "Epoch 33/80\n",
      "391/390 [==============================] - ETA: 0s - loss: 1.1190 - accuracy: 0.6701\n",
      "Epoch 00033: val_accuracy improved from 0.51810 to 0.52170, saving model to Inception_adam_Batchnorm_regularization.hdf5\n",
      "391/390 [==============================] - 29s 75ms/step - loss: 1.1190 - accuracy: 0.6701 - val_loss: 2.0058 - val_accuracy: 0.5217\n",
      "Epoch 34/80\n",
      "391/390 [==============================] - ETA: 0s - loss: 1.0776 - accuracy: 0.6780\n",
      "Epoch 00034: val_accuracy did not improve from 0.52170\n",
      "391/390 [==============================] - 29s 75ms/step - loss: 1.0776 - accuracy: 0.6780 - val_loss: 2.0827 - val_accuracy: 0.5123\n",
      "Epoch 35/80\n",
      "391/390 [==============================] - ETA: 0s - loss: 1.0599 - accuracy: 0.6871\n",
      "Epoch 00035: val_accuracy did not improve from 0.52170\n",
      "391/390 [==============================] - 29s 75ms/step - loss: 1.0599 - accuracy: 0.6871 - val_loss: 2.1201 - val_accuracy: 0.5038\n",
      "Epoch 36/80\n",
      "391/390 [==============================] - ETA: 0s - loss: 1.0244 - accuracy: 0.6961\n",
      "Epoch 00036: val_accuracy did not improve from 0.52170\n",
      "391/390 [==============================] - 29s 74ms/step - loss: 1.0244 - accuracy: 0.6961 - val_loss: 2.0759 - val_accuracy: 0.5136\n",
      "Epoch 37/80\n",
      "391/390 [==============================] - ETA: 0s - loss: 1.0053 - accuracy: 0.6973\n",
      "Epoch 00037: val_accuracy did not improve from 0.52170\n",
      "391/390 [==============================] - 29s 74ms/step - loss: 1.0053 - accuracy: 0.6973 - val_loss: 2.1276 - val_accuracy: 0.5089\n",
      "Epoch 38/80\n",
      "391/390 [==============================] - ETA: 0s - loss: 0.9733 - accuracy: 0.7066Restoring model weights from the end of the best epoch.\n",
      "\n",
      "Epoch 00038: val_accuracy did not improve from 0.52170\n",
      "391/390 [==============================] - 29s 74ms/step - loss: 0.9733 - accuracy: 0.7066 - val_loss: 2.1138 - val_accuracy: 0.5131\n",
      "Epoch 00038: early stopping\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.keras.callbacks.History at 0x7fd945da3978>"
      ]
     },
     "execution_count": 23,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.fit_generator(aug_data.flow(X_train,y_train,batch_size=128),steps_per_epoch=len(X_train)/128,epochs=80,validation_data=(X_test,y_test),verbose=1,callbacks=[call,checkpoint])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "id": "G7efzg6zC1A8"
   },
   "outputs": [],
   "source": [
    "y_true = y_test.argmax(-1)\n",
    "y_pred = model.predict(X_test).argmax(-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "id": "Up1W-eGzC1sA",
    "outputId": "2af21f3b-93f3-446e-d132-49c02a8516ca"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.88      0.65      0.75       100\n",
      "           1       0.71      0.59      0.64       100\n",
      "           2       0.53      0.40      0.45       100\n",
      "           3       0.36      0.31      0.34       100\n",
      "           4       0.37      0.30      0.33       100\n",
      "           5       0.47      0.55      0.51       100\n",
      "           6       0.57      0.50      0.53       100\n",
      "           7       0.66      0.64      0.65       100\n",
      "           8       0.45      0.63      0.53       100\n",
      "           9       0.65      0.62      0.63       100\n",
      "          10       0.38      0.27      0.32       100\n",
      "          11       0.33      0.37      0.35       100\n",
      "          12       0.56      0.58      0.57       100\n",
      "          13       0.43      0.39      0.41       100\n",
      "          14       0.49      0.51      0.50       100\n",
      "          15       0.59      0.46      0.52       100\n",
      "          16       0.58      0.48      0.52       100\n",
      "          17       0.64      0.62      0.63       100\n",
      "          18       0.51      0.46      0.48       100\n",
      "          19       0.47      0.45      0.46       100\n",
      "          20       0.75      0.76      0.76       100\n",
      "          21       0.63      0.73      0.68       100\n",
      "          22       0.32      0.51      0.40       100\n",
      "          23       0.70      0.72      0.71       100\n",
      "          24       0.87      0.66      0.75       100\n",
      "          25       0.46      0.32      0.38       100\n",
      "          26       0.39      0.51      0.44       100\n",
      "          27       0.35      0.33      0.34       100\n",
      "          28       0.75      0.64      0.69       100\n",
      "          29       0.44      0.44      0.44       100\n",
      "          30       0.53      0.31      0.39       100\n",
      "          31       0.53      0.59      0.56       100\n",
      "          32       0.53      0.56      0.55       100\n",
      "          33       0.54      0.40      0.46       100\n",
      "          34       0.47      0.45      0.46       100\n",
      "          35       0.35      0.28      0.31       100\n",
      "          36       0.54      0.40      0.46       100\n",
      "          37       0.53      0.38      0.44       100\n",
      "          38       0.39      0.37      0.38       100\n",
      "          39       0.50      0.82      0.62       100\n",
      "          40       0.59      0.45      0.51       100\n",
      "          41       0.75      0.71      0.73       100\n",
      "          42       0.48      0.61      0.54       100\n",
      "          43       0.60      0.52      0.56       100\n",
      "          44       0.39      0.15      0.22       100\n",
      "          45       0.33      0.35      0.34       100\n",
      "          46       0.33      0.28      0.30       100\n",
      "          47       0.66      0.46      0.54       100\n",
      "          48       0.40      0.91      0.55       100\n",
      "          49       0.50      0.80      0.62       100\n",
      "          50       0.30      0.35      0.32       100\n",
      "          51       0.61      0.43      0.51       100\n",
      "          52       0.42      0.90      0.58       100\n",
      "          53       0.74      0.86      0.80       100\n",
      "          54       0.66      0.47      0.55       100\n",
      "          55       0.35      0.18      0.24       100\n",
      "          56       0.73      0.76      0.75       100\n",
      "          57       0.62      0.60      0.61       100\n",
      "          58       0.53      0.63      0.57       100\n",
      "          59       0.54      0.42      0.47       100\n",
      "          60       0.75      0.78      0.76       100\n",
      "          61       0.59      0.50      0.54       100\n",
      "          62       0.59      0.62      0.60       100\n",
      "          63       0.50      0.49      0.49       100\n",
      "          64       0.32      0.33      0.33       100\n",
      "          65       0.44      0.22      0.29       100\n",
      "          66       0.42      0.44      0.43       100\n",
      "          67       0.41      0.51      0.46       100\n",
      "          68       0.90      0.80      0.85       100\n",
      "          69       0.76      0.67      0.71       100\n",
      "          70       0.73      0.33      0.46       100\n",
      "          71       0.76      0.55      0.64       100\n",
      "          72       0.22      0.25      0.23       100\n",
      "          73       0.50      0.40      0.44       100\n",
      "          74       0.34      0.39      0.36       100\n",
      "          75       0.74      0.81      0.78       100\n",
      "          76       0.84      0.73      0.78       100\n",
      "          77       0.51      0.36      0.42       100\n",
      "          78       0.42      0.42      0.42       100\n",
      "          79       0.72      0.46      0.56       100\n",
      "          80       0.46      0.17      0.25       100\n",
      "          81       0.41      0.66      0.51       100\n",
      "          82       0.78      0.87      0.82       100\n",
      "          83       0.39      0.50      0.44       100\n",
      "          84       0.59      0.44      0.51       100\n",
      "          85       0.67      0.64      0.65       100\n",
      "          86       0.35      0.51      0.42       100\n",
      "          87       0.46      0.69      0.55       100\n",
      "          88       0.35      0.67      0.46       100\n",
      "          89       0.65      0.55      0.60       100\n",
      "          90       0.40      0.61      0.49       100\n",
      "          91       0.67      0.64      0.65       100\n",
      "          92       0.35      0.46      0.40       100\n",
      "          93       0.30      0.26      0.28       100\n",
      "          94       0.81      0.78      0.80       100\n",
      "          95       0.43      0.76      0.55       100\n",
      "          96       0.55      0.42      0.48       100\n",
      "          97       0.59      0.50      0.54       100\n",
      "          98       0.35      0.31      0.33       100\n",
      "          99       0.55      0.51      0.53       100\n",
      "\n",
      "    accuracy                           0.52     10000\n",
      "   macro avg       0.53      0.52      0.51     10000\n",
      "weighted avg       0.53      0.52      0.51     10000\n",
      "\n",
      "Accuracy is 0.5181\n",
      "Precision is 0.533273641060228\n",
      "Recall is 0.5181\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import classification_report,accuracy_score,recall_score,precision_score\n",
    "print(classification_report(Y_test,y_pred))\n",
    "print(\"Accuracy is {}\".format(accuracy_score(Y_test,y_pred)))\n",
    "print(\"Precision is {}\".format(precision_score(Y_test,y_pred,average=\"weighted\")))\n",
    "print(\"Recall is {}\".format(recall_score(Y_test,y_pred,average=\"weighted\")))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "id": "b89sN7VRC4R7",
    "outputId": "3b5ed759-cf1b-4efa-ab52-12cc58fe2c15"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"functional_11\"\n",
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "input_7 (InputLayer)            [(None, 32, 32, 3)]  0                                            \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_161 (Conv2D)             (None, 16, 16, 32)   896         input_7[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_142 (BatchN (None, 16, 16, 32)   128         conv2d_161[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_162 (Conv2D)             (None, 16, 16, 32)   9248        batch_normalization_142[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_143 (BatchN (None, 16, 16, 32)   128         conv2d_162[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_163 (Conv2D)             (None, 16, 16, 32)   9248        batch_normalization_143[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_144 (BatchN (None, 16, 16, 32)   128         conv2d_163[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling2d_25 (MaxPooling2D) (None, 7, 7, 32)     0           batch_normalization_144[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_164 (Conv2D)             (None, 7, 7, 32)     9248        max_pooling2d_25[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_145 (BatchN (None, 7, 7, 32)     128         conv2d_164[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_165 (Conv2D)             (None, 4, 4, 32)     9248        batch_normalization_145[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_146 (BatchN (None, 4, 4, 32)     128         conv2d_165[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_166 (Conv2D)             (None, 4, 4, 32)     9248        batch_normalization_146[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_147 (BatchN (None, 4, 4, 32)     128         conv2d_166[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_167 (Conv2D)             (None, 4, 4, 64)     2112        batch_normalization_147[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_148 (BatchN (None, 4, 4, 64)     256         conv2d_167[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_168 (Conv2D)             (None, 4, 4, 64)     36928       batch_normalization_148[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_170 (Conv2D)             (None, 4, 4, 64)     2112        batch_normalization_147[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling2d_26 (MaxPooling2D) (None, 4, 4, 32)     0           batch_normalization_147[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_149 (BatchN (None, 4, 4, 64)     256         conv2d_168[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_151 (BatchN (None, 4, 4, 64)     256         conv2d_170[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_153 (BatchN (None, 4, 4, 32)     128         max_pooling2d_26[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_169 (Conv2D)             (None, 4, 4, 64)     36928       batch_normalization_149[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_171 (Conv2D)             (None, 4, 4, 64)     36928       batch_normalization_151[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_172 (Conv2D)             (None, 4, 4, 64)     2112        batch_normalization_153[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_173 (Conv2D)             (None, 4, 4, 64)     2112        batch_normalization_147[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_150 (BatchN (None, 4, 4, 64)     256         conv2d_169[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_152 (BatchN (None, 4, 4, 64)     256         conv2d_171[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_154 (BatchN (None, 4, 4, 64)     256         conv2d_172[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_155 (BatchN (None, 4, 4, 64)     256         conv2d_173[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_15 (Concatenate)    (None, 4, 4, 256)    0           batch_normalization_150[0][0]    \n",
      "                                                                 batch_normalization_152[0][0]    \n",
      "                                                                 batch_normalization_154[0][0]    \n",
      "                                                                 batch_normalization_155[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_174 (Conv2D)             (None, 4, 4, 128)    32896       concatenate_15[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_156 (BatchN (None, 4, 4, 128)    512         conv2d_174[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_175 (Conv2D)             (None, 4, 4, 128)    114816      batch_normalization_156[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_157 (BatchN (None, 4, 4, 128)    512         conv2d_175[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_176 (Conv2D)             (None, 4, 4, 128)    114816      batch_normalization_157[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_179 (Conv2D)             (None, 4, 4, 128)    32896       concatenate_15[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling2d_27 (MaxPooling2D) (None, 4, 4, 256)    0           concatenate_15[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_158 (BatchN (None, 4, 4, 128)    512         conv2d_176[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_161 (BatchN (None, 4, 4, 128)    512         conv2d_179[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_164 (BatchN (None, 4, 4, 256)    1024        max_pooling2d_27[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_177 (Conv2D)             (None, 4, 4, 128)    114816      batch_normalization_158[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_180 (Conv2D)             (None, 4, 4, 128)    114816      batch_normalization_161[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_182 (Conv2D)             (None, 4, 4, 128)    32896       batch_normalization_164[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_159 (BatchN (None, 4, 4, 128)    512         conv2d_177[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_162 (BatchN (None, 4, 4, 128)    512         conv2d_180[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_165 (BatchN (None, 4, 4, 128)    512         conv2d_182[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_178 (Conv2D)             (None, 4, 4, 128)    114816      batch_normalization_159[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_181 (Conv2D)             (None, 4, 4, 128)    114816      batch_normalization_162[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_183 (Conv2D)             (None, 4, 4, 128)    16512       batch_normalization_165[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_160 (BatchN (None, 4, 4, 128)    512         conv2d_178[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_163 (BatchN (None, 4, 4, 128)    512         conv2d_181[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_166 (BatchN (None, 4, 4, 128)    512         conv2d_183[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_16 (Concatenate)    (None, 4, 4, 512)    0           batch_normalization_160[0][0]    \n",
      "                                                                 batch_normalization_163[0][0]    \n",
      "                                                                 batch_normalization_165[0][0]    \n",
      "                                                                 batch_normalization_166[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_184 (Conv2D)             (None, 4, 4, 256)    131328      concatenate_16[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_167 (BatchN (None, 4, 4, 256)    1024        conv2d_184[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_185 (Conv2D)             (None, 4, 4, 256)    590080      batch_normalization_167[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_188 (Conv2D)             (None, 4, 4, 256)    131328      concatenate_16[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_168 (BatchN (None, 4, 4, 256)    1024        conv2d_185[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_171 (BatchN (None, 4, 4, 256)    1024        conv2d_188[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling2d_28 (MaxPooling2D) (None, 4, 4, 512)    0           concatenate_16[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_186 (Conv2D)             (None, 4, 4, 256)    196864      batch_normalization_168[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_187 (Conv2D)             (None, 4, 4, 256)    196864      batch_normalization_168[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_189 (Conv2D)             (None, 4, 4, 256)    196864      batch_normalization_171[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_190 (Conv2D)             (None, 4, 4, 256)    196864      batch_normalization_171[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_191 (Conv2D)             (None, 4, 4, 256)    131328      max_pooling2d_28[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_192 (Conv2D)             (None, 4, 4, 256)    131328      concatenate_16[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_169 (BatchN (None, 4, 4, 256)    1024        conv2d_186[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_170 (BatchN (None, 4, 4, 256)    1024        conv2d_187[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_172 (BatchN (None, 4, 4, 256)    1024        conv2d_189[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_173 (BatchN (None, 4, 4, 256)    1024        conv2d_190[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_174 (BatchN (None, 4, 4, 256)    1024        conv2d_191[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_175 (BatchN (None, 4, 4, 256)    1024        conv2d_192[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_17 (Concatenate)    (None, 4, 4, 1536)   0           batch_normalization_169[0][0]    \n",
      "                                                                 batch_normalization_170[0][0]    \n",
      "                                                                 batch_normalization_172[0][0]    \n",
      "                                                                 batch_normalization_173[0][0]    \n",
      "                                                                 batch_normalization_174[0][0]    \n",
      "                                                                 batch_normalization_175[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling2d_29 (MaxPooling2D) (None, 4, 4, 1536)   0           concatenate_17[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_176 (BatchN (None, 4, 4, 1536)   6144        max_pooling2d_29[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "flatten_5 (Flatten)             (None, 24576)        0           batch_normalization_176[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "dense_10 (Dense)                (None, 256)          6291712     flatten_5[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_177 (BatchN (None, 256)          1024        dense_10[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "dense_11 (Dense)                (None, 100)          25700       batch_normalization_177[0][0]    \n",
      "==================================================================================================\n",
      "Total params: 9,215,940\n",
      "Trainable params: 9,203,332\n",
      "Non-trainable params: 12,608\n",
      "__________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.73      0.83      0.78       100\n",
      "           1       0.79      0.58      0.67       100\n",
      "           2       0.42      0.37      0.39       100\n",
      "           3       0.28      0.26      0.27       100\n",
      "           4       0.39      0.30      0.34       100\n",
      "           5       0.49      0.46      0.47       100\n",
      "           6       0.38      0.74      0.50       100\n",
      "           7       0.62      0.58      0.60       100\n",
      "           8       0.52      0.69      0.59       100\n",
      "           9       0.82      0.55      0.66       100\n",
      "          10       0.38      0.24      0.29       100\n",
      "          11       0.41      0.23      0.29       100\n",
      "          12       0.50      0.54      0.52       100\n",
      "          13       0.35      0.51      0.41       100\n",
      "          14       0.44      0.48      0.46       100\n",
      "          15       0.47      0.48      0.48       100\n",
      "          16       0.52      0.55      0.53       100\n",
      "          17       0.73      0.61      0.66       100\n",
      "          18       0.39      0.60      0.47       100\n",
      "          19       0.47      0.36      0.41       100\n",
      "          20       0.88      0.75      0.81       100\n",
      "          21       0.59      0.71      0.64       100\n",
      "          22       0.22      0.53      0.32       100\n",
      "          23       0.80      0.73      0.76       100\n",
      "          24       0.88      0.61      0.72       100\n",
      "          25       0.48      0.27      0.35       100\n",
      "          26       0.34      0.43      0.38       100\n",
      "          27       0.37      0.40      0.38       100\n",
      "          28       0.74      0.70      0.72       100\n",
      "          29       0.45      0.47      0.46       100\n",
      "          30       0.53      0.48      0.51       100\n",
      "          31       0.59      0.54      0.57       100\n",
      "          32       0.66      0.42      0.51       100\n",
      "          33       0.55      0.47      0.51       100\n",
      "          34       0.49      0.49      0.49       100\n",
      "          35       0.34      0.47      0.39       100\n",
      "          36       0.63      0.39      0.48       100\n",
      "          37       0.43      0.52      0.47       100\n",
      "          38       0.49      0.34      0.40       100\n",
      "          39       0.57      0.75      0.65       100\n",
      "          40       0.51      0.45      0.48       100\n",
      "          41       0.91      0.64      0.75       100\n",
      "          42       0.39      0.67      0.49       100\n",
      "          43       0.58      0.61      0.60       100\n",
      "          44       0.24      0.33      0.28       100\n",
      "          45       0.29      0.41      0.34       100\n",
      "          46       0.44      0.34      0.38       100\n",
      "          47       0.56      0.53      0.55       100\n",
      "          48       0.57      0.78      0.66       100\n",
      "          49       0.54      0.78      0.64       100\n",
      "          50       0.37      0.28      0.32       100\n",
      "          51       0.50      0.60      0.55       100\n",
      "          52       0.49      0.81      0.61       100\n",
      "          53       0.63      0.92      0.74       100\n",
      "          54       0.50      0.57      0.53       100\n",
      "          55       0.32      0.22      0.26       100\n",
      "          56       0.80      0.78      0.79       100\n",
      "          57       0.77      0.46      0.57       100\n",
      "          58       0.54      0.49      0.51       100\n",
      "          59       0.60      0.43      0.50       100\n",
      "          60       0.78      0.77      0.77       100\n",
      "          61       0.51      0.55      0.53       100\n",
      "          62       0.71      0.59      0.64       100\n",
      "          63       0.57      0.47      0.52       100\n",
      "          64       0.41      0.35      0.38       100\n",
      "          65       0.51      0.32      0.39       100\n",
      "          66       0.35      0.37      0.36       100\n",
      "          67       0.48      0.27      0.35       100\n",
      "          68       0.91      0.82      0.86       100\n",
      "          69       0.75      0.66      0.70       100\n",
      "          70       0.60      0.52      0.56       100\n",
      "          71       0.72      0.78      0.75       100\n",
      "          72       0.38      0.23      0.29       100\n",
      "          73       0.47      0.44      0.45       100\n",
      "          74       0.41      0.31      0.35       100\n",
      "          75       0.89      0.63      0.74       100\n",
      "          76       0.79      0.68      0.73       100\n",
      "          77       0.41      0.36      0.39       100\n",
      "          78       0.30      0.39      0.34       100\n",
      "          79       0.62      0.56      0.59       100\n",
      "          80       0.46      0.17      0.25       100\n",
      "          81       0.40      0.68      0.50       100\n",
      "          82       0.78      0.75      0.77       100\n",
      "          83       0.59      0.43      0.50       100\n",
      "          84       0.44      0.58      0.50       100\n",
      "          85       0.70      0.57      0.63       100\n",
      "          86       0.49      0.58      0.53       100\n",
      "          87       0.60      0.61      0.61       100\n",
      "          88       0.46      0.56      0.51       100\n",
      "          89       0.48      0.70      0.57       100\n",
      "          90       0.44      0.65      0.52       100\n",
      "          91       0.61      0.67      0.64       100\n",
      "          92       0.42      0.35      0.38       100\n",
      "          93       0.46      0.26      0.33       100\n",
      "          94       0.84      0.73      0.78       100\n",
      "          95       0.64      0.63      0.64       100\n",
      "          96       0.60      0.38      0.47       100\n",
      "          97       0.49      0.56      0.52       100\n",
      "          98       0.45      0.18      0.26       100\n",
      "          99       0.49      0.53      0.51       100\n",
      "\n",
      "    accuracy                           0.52     10000\n",
      "   macro avg       0.54      0.52      0.52     10000\n",
      "weighted avg       0.54      0.52      0.52     10000\n",
      "\n",
      "Accuracy is 0.5217\n",
      "Precision is 0.5406710891870593\n",
      "Recall is 0.5217\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from tensorflow.keras.models import Sequential,Model\n",
    "from tensorflow.keras.layers import Dense,Flatten,Input,Conv2D,MaxPool2D,concatenate,BatchNormalization\n",
    "from keras.layers.convolutional import MaxPooling2D\n",
    "from keras.layers.merge import concatenate\n",
    "from keras.layers import Concatenate, Dense, LSTM, Input, concatenate\n",
    "from keras.layers import Conv2D\n",
    "\n",
    "from tensorflow.keras import layers\n",
    "\n",
    "import tensorflow.keras as keras\n",
    "\n",
    "import tensorflow.keras as keras\n",
    "\n",
    "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
    "\n",
    "from tensorflow.keras.layers import LeakyReLU\n",
    "\n",
    "from tensorflow.keras.datasets import cifar100\n",
    "from tensorflow.keras.utils import to_categorical\n",
    "\n",
    "from tensorflow.keras.callbacks import EarlyStopping,ModelCheckpoint\n",
    "\n",
    "from keras.optimizers import Adam,SGD\n",
    "\n",
    "(X_train,Y_train),(X_test,Y_test)=cifar100.load_data()\n",
    "y_train,y_test=to_categorical(Y_train),to_categorical(Y_test)\n",
    "X_train,X_test=X_train/255,X_test/255\n",
    "\n",
    "\n",
    "inputs=keras.Input(shape=(32,32,3))\n",
    "\n",
    "a=layers.Conv2D(filters=32,kernel_size=(3,3),padding=\"same\",strides=2,activation=LeakyReLU(alpha=0.1))(inputs)\n",
    "a=layers.BatchNormalization()(a)\n",
    "\n",
    "\n",
    "a=layers.Conv2D(filters=32,kernel_size=(3,3),padding=\"same\",strides=1,activation=LeakyReLU(alpha=0.1))(a)\n",
    "a=layers.BatchNormalization()(a)\n",
    "\n",
    "\n",
    "a=layers.Conv2D(filters=32, kernel_size=(3,3), padding=\"same\",strides=1,activation=LeakyReLU(alpha=0.1))(a)\n",
    "a=layers.BatchNormalization()(a)\n",
    "\n",
    "a=(MaxPool2D(pool_size=(3,3),strides=(2,2)))(a)\n",
    "\n",
    "\n",
    "a=layers.Conv2D(filters=32, kernel_size=(3,3), padding=\"same\",strides=1,activation=LeakyReLU(alpha=0.1))(a)\n",
    "a=layers.BatchNormalization()(a)\n",
    "\n",
    "\n",
    "a=layers.Conv2D(filters=32, kernel_size=(3,3), padding=\"same\",strides=2,activation=LeakyReLU(alpha=0.1))(a)\n",
    "a=layers.BatchNormalization()(a)\n",
    "\n",
    "\n",
    "a=layers.Conv2D(filters=32, kernel_size=(3,3), padding=\"same\",strides=1,activation=LeakyReLU(alpha=0.1))(a)\n",
    "a=layers.BatchNormalization()(a)\n",
    "\n",
    "\n",
    "#inception-1\n",
    "b=layers.Conv2D(64, (1,1), padding='same',strides=1,activation=LeakyReLU(alpha=0.1))(a)\n",
    "b=layers.BatchNormalization()(b)\n",
    "\n",
    "b=layers.Conv2D(64, (3,3), padding='same',strides=1 ,activation=LeakyReLU(alpha=0.1))(b)\n",
    "b=layers.BatchNormalization()(b)\n",
    "\n",
    "b=layers.Conv2D(64, (3,3), padding='same', strides=1,activation=LeakyReLU(alpha=0.1))(b)\n",
    "b=layers.BatchNormalization()(b)\n",
    "\n",
    "\n",
    "\n",
    "c=layers.Conv2D(64, (1,1), padding='same', strides=1, activation=LeakyReLU(alpha=0.1))(a)\n",
    "c=layers.BatchNormalization()(c)\n",
    "\n",
    "c=layers.Conv2D(64, (3,3), padding='same', strides=1, activation=LeakyReLU(alpha=0.1))(c)\n",
    "c=layers.BatchNormalization()(c)\n",
    "\n",
    "\n",
    "d=layers.MaxPooling2D((3,3), strides=(1,1), padding='same')(a)\n",
    "d=layers.BatchNormalization()(d)\n",
    "\n",
    "d=layers.Conv2D(64, (1,1), padding='same', strides=1, activation=LeakyReLU(alpha=0.1))(d)\n",
    "d=layers.BatchNormalization()(d)\n",
    "\n",
    "\n",
    "e=layers.Conv2D(64, (1,1), padding='same', strides=1, activation=LeakyReLU(alpha=0.1))(a)\n",
    "e=layers.BatchNormalization()(e)\n",
    "\n",
    "\n",
    "a=layers.concatenate([b, c, d,e],axis=3)\n",
    "\n",
    "#inception-2\n",
    "f=layers.Conv2D(128, (1,1), padding='same', strides=1, activation=LeakyReLU(alpha=0.1))(a)\n",
    "f=layers.BatchNormalization()(f)\n",
    "\n",
    "f=layers.Conv2D(128, (1,7), padding='same', strides=1, activation=LeakyReLU(alpha=0.1))(f)\n",
    "f=layers.BatchNormalization()(f)\n",
    "\n",
    "\n",
    "f=layers.Conv2D(128, (7,1), padding='same', strides=1, activation=LeakyReLU(alpha=0.1))(f)\n",
    "f=layers.BatchNormalization()(f)\n",
    "\n",
    "f=layers.Conv2D(128, (1,7), padding='same', strides=1, activation=LeakyReLU(alpha=0.1))(f)\n",
    "f=layers.BatchNormalization()(f)\n",
    "\n",
    "\n",
    "f=layers.Conv2D(128, (7,1), padding='same', strides=1, activation=LeakyReLU(alpha=0.1))(f)\n",
    "f=layers.BatchNormalization()(f)\n",
    "\n",
    "\n",
    "g=layers.Conv2D(128, (1,1), padding='same', strides=1, activation=LeakyReLU(alpha=0.1))(a)\n",
    "g=layers.BatchNormalization()(g)\n",
    "\n",
    "g=layers.Conv2D(128, (1,7), padding='same', strides=1,activation=LeakyReLU(alpha=0.1))(g)\n",
    "g=layers.BatchNormalization()(g)\n",
    "\n",
    "g=layers.Conv2D(128, (7,1), padding='same', strides=1, activation=LeakyReLU(alpha=0.1))(g)\n",
    "\n",
    "g=layers.BatchNormalization()(g)\n",
    "\n",
    "\n",
    "h=layers.MaxPooling2D((3,3), strides=(1,1), padding='same')(a)\n",
    "h=layers.BatchNormalization()(h)\n",
    "\n",
    "h=layers.Conv2D(128, (1,1), padding='same', strides=1, activation=LeakyReLU(alpha=0.1))(h)\n",
    "h=layers.BatchNormalization()(h)\n",
    "\n",
    "\n",
    "i=layers.Conv2D(128, (1,1), padding='same', strides=1, activation=LeakyReLU(alpha=0.1))(h)\n",
    "i=layers.BatchNormalization()(i)\n",
    "\n",
    "\n",
    "a=layers.concatenate([f,g,h,i],axis=3)\n",
    "\n",
    "#inception-3\n",
    "j=layers.Conv2D(256, (1,1), padding='same', strides=1, activation=LeakyReLU(alpha=0.1))(a)\n",
    "j=layers.BatchNormalization()(j)\n",
    "\n",
    "\n",
    "j=layers.Conv2D(256, (3,3), padding='same', strides=1, activation=LeakyReLU(alpha=0.1))(j)\n",
    "j=layers.BatchNormalization()(j)\n",
    "\n",
    "k=layers.Conv2D(256, (1,3), padding='same', strides=1, activation=LeakyReLU(alpha=0.1))(j)\n",
    "k=layers.BatchNormalization()(k)\n",
    "\n",
    "l=layers.Conv2D(256, (3,1), padding='same', strides=1, activation=LeakyReLU(alpha=0.1))(j)\n",
    "l=layers.BatchNormalization()(l)\n",
    "\n",
    "\n",
    "m=layers.Conv2D(256, (1,1), padding='same', activation=LeakyReLU(alpha=0.1))(a)\n",
    "m=layers.BatchNormalization()(m)\n",
    "\n",
    "n=layers.Conv2D(256, (1,3), padding='same', activation=LeakyReLU(alpha=0.1))(m)\n",
    "n=layers.BatchNormalization()(n)\n",
    "\n",
    "o=layers.Conv2D(256, (3,1), padding='same', activation=LeakyReLU(alpha=0.1))(m)\n",
    "o=layers.BatchNormalization()(o)\n",
    "\n",
    "\n",
    "p=layers.MaxPool2D((3,3), strides=(1,1), padding='same')(a)\n",
    "p=layers.Conv2D(256, (1,1), padding='same', strides=1, activation=LeakyReLU(alpha=0.1))(p)\n",
    "p=layers.BatchNormalization()(p)\n",
    "\n",
    "\n",
    "q=layers.Conv2D(256, (1,1), padding='same', strides=1, activation=LeakyReLU(alpha=0.1))(a)\n",
    "q=layers.BatchNormalization()(q)\n",
    "\n",
    "\n",
    "a=layers.concatenate([k,l,n,o,p,q],axis=3)\n",
    "\n",
    "a=layers.MaxPooling2D((8,8), strides=(1,1), padding='same')(a)\n",
    "a=layers.BatchNormalization()(a)\n",
    "\n",
    "\n",
    "a=layers.Flatten()(a)\n",
    "\n",
    "a=layers.Dense(units=256,activation=LeakyReLU(alpha=0.3))(a)\n",
    "a=layers.BatchNormalization()(a)\n",
    "\n",
    "\n",
    "outputs=layers.Dense(y_train.shape[1],activation=\"softmax\")(a)\n",
    "\n",
    "model=keras.Model(inputs,outputs)\n",
    "\n",
    "model.load_weights(\"../weights/Inception_adam_Batchnorm_regularization.hdf5\")\n",
    "\n",
    "y_true = y_test.argmax(-1)\n",
    "y_pred = model.predict(X_test).argmax(-1)\n",
    "\n",
    "from sklearn.metrics import classification_report,accuracy_score,recall_score,precision_score\n",
    "print(classification_report(Y_test,y_pred))\n",
    "print(\"Accuracy is {}\".format(accuracy_score(Y_test,y_pred)))\n",
    "print(\"Precision is {}\".format(precision_score(Y_test,y_pred,average=\"weighted\")))\n",
    "print(\"Recall is {}\".format(recall_score(Y_test,y_pred,average=\"weighted\")))\n"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "name": "Adam_with_Bach_norm_inception.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
