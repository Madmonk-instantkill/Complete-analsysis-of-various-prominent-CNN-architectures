{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 51
    },
    "id": "TbuoZJLQ74pI",
    "outputId": "3f4fdf95-c4cb-4bb5-9072-4c5feac0b0ba"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading data from https://www.cs.toronto.edu/~kriz/cifar-100-python.tar.gz\n",
      "169009152/169001437 [==============================] - 6s 0us/step\n"
     ]
    }
   ],
   "source": [
    "from keras.callbacks import ModelCheckpoint, EarlyStopping\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Conv2D, MaxPool2D, Flatten, Dense,BatchNormalization,Activation,Dropout,LeakyReLU\n",
    "from tensorflow.keras.optimizers import SGD,Adam\n",
    "from keras.preprocessing.image import ImageDataGenerator\n",
    "from keras.utils import to_categorical\n",
    "from tensorflow.keras.datasets import cifar100\n",
    "\n",
    "\n",
    "\n",
    "(x_train, Y_train), (x_test, Y_test) = cifar100.load_data()\n",
    "#x_train = x_train.astype('float32') / 255\n",
    "#x_test = x_test.astype('float32') / 255\n",
    "\n",
    "y_train = to_categorical(Y_train)\n",
    "y_test = to_categorical(Y_test)\n",
    "\n",
    "aug_data=ImageDataGenerator(rotation_range=20,horizontal_flip=True,width_shift_range=0.1,shear_range = 0.2,height_shift_range=0.1,zoom_range=0.2,brightness_range = (0.5, 1.5))\n",
    "aug_data.fit(x_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "id": "4dzZ1Lnl7654",
    "outputId": "53fef2e5-1b8e-43a1-8f7d-e423be14b7cd"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:`period` argument is deprecated. Please use `save_freq` to specify the frequency in number of batches seen.\n"
     ]
    }
   ],
   "source": [
    "\n",
    "#import tensorflow as \n",
    "model = Sequential()\n",
    "\n",
    "# Creating first block- (2 Convolution + 1 Max pool)\n",
    "model.add(Conv2D(filters= 64, kernel_size= (3,3), strides= (1,1), padding='same', input_shape= (32, 32, 3)))\n",
    "model.add(LeakyReLU(alpha=0.1))\n",
    "model.add(Conv2D(filters= 64, kernel_size= (3,3), strides= (1,1), padding='same'))\n",
    "model.add(LeakyReLU(alpha=0.1))\n",
    "model.add(MaxPool2D(pool_size= (2,2), strides=(1,1)))\n",
    "\n",
    "# Creating second block- (2 Convolution + 1 Max pool)\n",
    "model.add(Conv2D(filters= 128, kernel_size= (3,3), strides= (1,1), padding='same'))\n",
    "model.add(LeakyReLU(alpha=0.1))\n",
    "model.add(Conv2D(filters= 128, kernel_size= (3,3), strides= (1,1), padding='same'))\n",
    "model.add(LeakyReLU(alpha=0.1))\n",
    "model.add(MaxPool2D(pool_size= (2,2), strides=(1,1)))\n",
    "\n",
    "\n",
    "# Creating third block- (3 Convolution + 1 Max pool)\n",
    "model.add(Conv2D(filters= 256, kernel_size= (3,3), strides= (1,1), padding='same'))\n",
    "model.add(LeakyReLU(alpha=0.1))\n",
    "model.add(Conv2D(filters= 256, kernel_size= (3,3), strides= (1,1), padding='same'))\n",
    "model.add(LeakyReLU(alpha=0.1))\n",
    "model.add(Conv2D(filters= 256, kernel_size= (3,3), strides= (1,1), padding='same'))\n",
    "model.add(LeakyReLU(alpha=0.1))\n",
    "model.add(MaxPool2D(pool_size= (2,2), strides=(1,1)))\n",
    "\n",
    "# Creating fourth block- (3 Convolution + 1 Max pool)\n",
    "model.add(Conv2D(filters= 512, kernel_size= (3,3), strides= (1,1), padding='same'))\n",
    "model.add(LeakyReLU(alpha=0.1))\n",
    "model.add(Conv2D(filters= 512, kernel_size= (3,3), strides= (1,1), padding='same'))\n",
    "model.add(LeakyReLU(alpha=0.1))\n",
    "model.add(Conv2D(filters= 512, kernel_size= (3,3), strides= (1,1), padding='same'))\n",
    "model.add(LeakyReLU(alpha=0.1))\n",
    "model.add(MaxPool2D(pool_size= (2,2), strides=(2,2)))\n",
    "\n",
    "# Creating fifth block- (3 Convolution + 1 Max pool)\n",
    "model.add(Conv2D(filters= 512, kernel_size= (3,3), strides= (1,1), padding='same'))\n",
    "model.add(LeakyReLU(alpha=0.1))\n",
    "model.add(Conv2D(filters= 512, kernel_size= (3,3), strides= (1,1), padding='same'))\n",
    "model.add(LeakyReLU(alpha=0.1))\n",
    "model.add(Conv2D(filters= 512, kernel_size= (3,3), strides= (1,1), padding='same'))\n",
    "model.add(LeakyReLU(alpha=0.1))\n",
    "model.add(MaxPool2D(pool_size= (2,2), strides=(1,1)))\n",
    "\n",
    "# Flattening the pooled image pixels\n",
    "model.add(Flatten())\n",
    "\n",
    "# Creating 2 Dense Layers\n",
    "model.add(Dense(units= 512))\n",
    "model.add(LeakyReLU(alpha=0.1))\n",
    "\n",
    "model.add(Dense(units= 100, activation='softmax'))\n",
    "\n",
    "\n",
    "#adam=Adam(learning_rate=0.01,clipnorm=1,name='adam')\n",
    "sgd=SGD(learning_rate=0.01,momentum=0.9,clipnorm=1)\n",
    "\n",
    "\n",
    "\n",
    "model.compile(optimizer=sgd,loss='categorical_crossentropy',metrics=['accuracy'])\n",
    "\n",
    "checkpoint = ModelCheckpoint(\"VGG_SGD_no_reg_weights_amit.hdf5\", monitor='val_accuracy', verbose=1, save_best_only=True, save_weights_only=True, mode='auto', period=1)\n",
    "early = EarlyStopping(monitor='val_accuracy', min_delta=0, patience=5, verbose=1, mode='auto',restore_best_weights=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "YlY1aYkv7_nZ",
    "outputId": "ed31338e-2f2e-4d0f-ef6d-b67e971608b5"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100\n",
      "  2/391 [..............................] - ETA: 2:08 - loss: 4.6717 - accuracy: 0.0039WARNING:tensorflow:Callbacks method `on_train_batch_end` is slow compared to the batch time (batch time: 0.2389s vs `on_train_batch_end` time: 0.4226s). Check your callbacks.\n",
      "391/391 [==============================] - ETA: 0s - loss: 4.2166 - accuracy: 0.0595\n",
      "Epoch 00001: val_accuracy improved from -inf to 0.11750, saving model to VGG_SGD_no_reg_weights_amit.hdf5\n",
      "391/391 [==============================] - 310s 792ms/step - loss: 4.2166 - accuracy: 0.0595 - val_loss: 3.7561 - val_accuracy: 0.1175\n",
      "Epoch 2/100\n",
      "391/391 [==============================] - ETA: 0s - loss: 3.6302 - accuracy: 0.1511\n",
      "Epoch 00002: val_accuracy improved from 0.11750 to 0.22570, saving model to VGG_SGD_no_reg_weights_amit.hdf5\n",
      "391/391 [==============================] - 323s 826ms/step - loss: 3.6302 - accuracy: 0.1511 - val_loss: 3.2376 - val_accuracy: 0.2257\n",
      "Epoch 3/100\n",
      "391/391 [==============================] - ETA: 0s - loss: 3.2428 - accuracy: 0.2191\n",
      "Epoch 00003: val_accuracy improved from 0.22570 to 0.27380, saving model to VGG_SGD_no_reg_weights_amit.hdf5\n",
      "391/391 [==============================] - 323s 825ms/step - loss: 3.2428 - accuracy: 0.2191 - val_loss: 2.9444 - val_accuracy: 0.2738\n",
      "Epoch 4/100\n",
      "391/391 [==============================] - ETA: 0s - loss: 2.9468 - accuracy: 0.2746\n",
      "Epoch 00004: val_accuracy improved from 0.27380 to 0.32900, saving model to VGG_SGD_no_reg_weights_amit.hdf5\n",
      "391/391 [==============================] - 323s 825ms/step - loss: 2.9468 - accuracy: 0.2746 - val_loss: 2.6814 - val_accuracy: 0.3290\n",
      "Epoch 5/100\n",
      "391/391 [==============================] - ETA: 0s - loss: 2.7113 - accuracy: 0.3210\n",
      "Epoch 00005: val_accuracy improved from 0.32900 to 0.37780, saving model to VGG_SGD_no_reg_weights_amit.hdf5\n",
      "391/391 [==============================] - 323s 825ms/step - loss: 2.7113 - accuracy: 0.3210 - val_loss: 2.4336 - val_accuracy: 0.3778\n",
      "Epoch 6/100\n",
      "391/391 [==============================] - ETA: 0s - loss: 2.4900 - accuracy: 0.3634\n",
      "Epoch 00006: val_accuracy improved from 0.37780 to 0.41620, saving model to VGG_SGD_no_reg_weights_amit.hdf5\n",
      "391/391 [==============================] - 323s 825ms/step - loss: 2.4900 - accuracy: 0.3634 - val_loss: 2.2975 - val_accuracy: 0.4162\n",
      "Epoch 7/100\n",
      "391/391 [==============================] - ETA: 0s - loss: 2.3144 - accuracy: 0.4020\n",
      "Epoch 00007: val_accuracy improved from 0.41620 to 0.45710, saving model to VGG_SGD_no_reg_weights_amit.hdf5\n",
      "391/391 [==============================] - 323s 825ms/step - loss: 2.3144 - accuracy: 0.4020 - val_loss: 2.0635 - val_accuracy: 0.4571\n",
      "Epoch 8/100\n",
      "391/391 [==============================] - ETA: 0s - loss: 2.1596 - accuracy: 0.4347\n",
      "Epoch 00008: val_accuracy improved from 0.45710 to 0.47860, saving model to VGG_SGD_no_reg_weights_amit.hdf5\n",
      "391/391 [==============================] - 323s 825ms/step - loss: 2.1596 - accuracy: 0.4347 - val_loss: 2.0105 - val_accuracy: 0.4786\n",
      "Epoch 9/100\n",
      "391/391 [==============================] - ETA: 0s - loss: 2.0255 - accuracy: 0.4666\n",
      "Epoch 00009: val_accuracy improved from 0.47860 to 0.48700, saving model to VGG_SGD_no_reg_weights_amit.hdf5\n",
      "391/391 [==============================] - 323s 826ms/step - loss: 2.0255 - accuracy: 0.4666 - val_loss: 1.9164 - val_accuracy: 0.4870\n",
      "Epoch 10/100\n",
      "391/391 [==============================] - ETA: 0s - loss: 1.9043 - accuracy: 0.4920\n",
      "Epoch 00010: val_accuracy improved from 0.48700 to 0.50030, saving model to VGG_SGD_no_reg_weights_amit.hdf5\n",
      "391/391 [==============================] - 323s 825ms/step - loss: 1.9043 - accuracy: 0.4920 - val_loss: 1.8983 - val_accuracy: 0.5003\n",
      "Epoch 11/100\n",
      "391/391 [==============================] - ETA: 0s - loss: 1.8076 - accuracy: 0.5142\n",
      "Epoch 00011: val_accuracy improved from 0.50030 to 0.50980, saving model to VGG_SGD_no_reg_weights_amit.hdf5\n",
      "391/391 [==============================] - 323s 826ms/step - loss: 1.8076 - accuracy: 0.5142 - val_loss: 1.8781 - val_accuracy: 0.5098\n",
      "Epoch 12/100\n",
      "391/391 [==============================] - ETA: 0s - loss: 1.7137 - accuracy: 0.5319\n",
      "Epoch 00012: val_accuracy improved from 0.50980 to 0.54620, saving model to VGG_SGD_no_reg_weights_amit.hdf5\n",
      "391/391 [==============================] - 323s 827ms/step - loss: 1.7137 - accuracy: 0.5319 - val_loss: 1.7310 - val_accuracy: 0.5462\n",
      "Epoch 13/100\n",
      "391/391 [==============================] - ETA: 0s - loss: 1.6239 - accuracy: 0.5562\n",
      "Epoch 00013: val_accuracy did not improve from 0.54620\n",
      "391/391 [==============================] - 323s 826ms/step - loss: 1.6239 - accuracy: 0.5562 - val_loss: 1.6964 - val_accuracy: 0.5450\n",
      "Epoch 14/100\n",
      "391/391 [==============================] - ETA: 0s - loss: 1.5487 - accuracy: 0.5739\n",
      "Epoch 00014: val_accuracy improved from 0.54620 to 0.55910, saving model to VGG_SGD_no_reg_weights_amit.hdf5\n",
      "391/391 [==============================] - 323s 826ms/step - loss: 1.5487 - accuracy: 0.5739 - val_loss: 1.6984 - val_accuracy: 0.5591\n",
      "Epoch 15/100\n",
      "391/391 [==============================] - ETA: 0s - loss: 1.4815 - accuracy: 0.5889\n",
      "Epoch 00015: val_accuracy did not improve from 0.55910\n",
      "391/391 [==============================] - 323s 826ms/step - loss: 1.4815 - accuracy: 0.5889 - val_loss: 1.6590 - val_accuracy: 0.5559\n",
      "Epoch 16/100\n",
      "391/391 [==============================] - ETA: 0s - loss: 1.4148 - accuracy: 0.6040\n",
      "Epoch 00016: val_accuracy did not improve from 0.55910\n",
      "391/391 [==============================] - 323s 827ms/step - loss: 1.4148 - accuracy: 0.6040 - val_loss: 1.7459 - val_accuracy: 0.5579\n",
      "Epoch 17/100\n",
      "391/391 [==============================] - ETA: 0s - loss: 1.3463 - accuracy: 0.6231\n",
      "Epoch 00017: val_accuracy improved from 0.55910 to 0.58330, saving model to VGG_SGD_no_reg_weights_amit.hdf5\n",
      "391/391 [==============================] - 324s 828ms/step - loss: 1.3463 - accuracy: 0.6231 - val_loss: 1.5945 - val_accuracy: 0.5833\n",
      "Epoch 18/100\n",
      "391/391 [==============================] - ETA: 0s - loss: 1.2934 - accuracy: 0.6363\n",
      "Epoch 00018: val_accuracy did not improve from 0.58330\n",
      "391/391 [==============================] - 323s 827ms/step - loss: 1.2934 - accuracy: 0.6363 - val_loss: 1.6178 - val_accuracy: 0.5726\n",
      "Epoch 19/100\n",
      "391/391 [==============================] - ETA: 0s - loss: 1.2488 - accuracy: 0.6486\n",
      "Epoch 00019: val_accuracy did not improve from 0.58330\n",
      "391/391 [==============================] - 324s 828ms/step - loss: 1.2488 - accuracy: 0.6486 - val_loss: 1.5951 - val_accuracy: 0.5802\n",
      "Epoch 20/100\n",
      "391/391 [==============================] - ETA: 0s - loss: 1.1972 - accuracy: 0.6601\n",
      "Epoch 00020: val_accuracy did not improve from 0.58330\n",
      "391/391 [==============================] - 323s 826ms/step - loss: 1.1972 - accuracy: 0.6601 - val_loss: 1.6822 - val_accuracy: 0.5825\n",
      "Epoch 21/100\n",
      "391/391 [==============================] - ETA: 0s - loss: 1.1524 - accuracy: 0.6697\n",
      "Epoch 00021: val_accuracy improved from 0.58330 to 0.60440, saving model to VGG_SGD_no_reg_weights_amit.hdf5\n",
      "391/391 [==============================] - 323s 827ms/step - loss: 1.1524 - accuracy: 0.6697 - val_loss: 1.5151 - val_accuracy: 0.6044\n",
      "Epoch 22/100\n",
      "391/391 [==============================] - ETA: 0s - loss: 1.1060 - accuracy: 0.6820\n",
      "Epoch 00022: val_accuracy did not improve from 0.60440\n",
      "391/391 [==============================] - 323s 827ms/step - loss: 1.1060 - accuracy: 0.6820 - val_loss: 1.5701 - val_accuracy: 0.5963\n",
      "Epoch 23/100\n",
      "391/391 [==============================] - ETA: 0s - loss: 1.0765 - accuracy: 0.6886\n",
      "Epoch 00023: val_accuracy did not improve from 0.60440\n",
      "391/391 [==============================] - 323s 826ms/step - loss: 1.0765 - accuracy: 0.6886 - val_loss: 1.5855 - val_accuracy: 0.6003\n",
      "Epoch 24/100\n",
      "391/391 [==============================] - ETA: 0s - loss: 1.0395 - accuracy: 0.6966\n",
      "Epoch 00024: val_accuracy improved from 0.60440 to 0.61010, saving model to VGG_SGD_no_reg_weights_amit.hdf5\n",
      "391/391 [==============================] - 323s 827ms/step - loss: 1.0395 - accuracy: 0.6966 - val_loss: 1.5259 - val_accuracy: 0.6101\n",
      "Epoch 25/100\n",
      "391/391 [==============================] - ETA: 0s - loss: 0.9946 - accuracy: 0.7113\n",
      "Epoch 00025: val_accuracy improved from 0.61010 to 0.61130, saving model to VGG_SGD_no_reg_weights_amit.hdf5\n",
      "391/391 [==============================] - 324s 828ms/step - loss: 0.9946 - accuracy: 0.7113 - val_loss: 1.5196 - val_accuracy: 0.6113\n",
      "Epoch 26/100\n",
      "391/391 [==============================] - ETA: 0s - loss: 0.9568 - accuracy: 0.7198\n",
      "Epoch 00026: val_accuracy did not improve from 0.61130\n",
      "391/391 [==============================] - 323s 827ms/step - loss: 0.9568 - accuracy: 0.7198 - val_loss: 1.6020 - val_accuracy: 0.5957\n",
      "Epoch 27/100\n",
      "391/391 [==============================] - ETA: 0s - loss: 0.9281 - accuracy: 0.7290\n",
      "Epoch 00027: val_accuracy improved from 0.61130 to 0.62700, saving model to VGG_SGD_no_reg_weights_amit.hdf5\n",
      "391/391 [==============================] - 323s 827ms/step - loss: 0.9281 - accuracy: 0.7290 - val_loss: 1.5063 - val_accuracy: 0.6270\n",
      "Epoch 28/100\n",
      "391/391 [==============================] - ETA: 0s - loss: 0.9020 - accuracy: 0.7351\n",
      "Epoch 00028: val_accuracy did not improve from 0.62700\n",
      "391/391 [==============================] - 323s 826ms/step - loss: 0.9020 - accuracy: 0.7351 - val_loss: 1.6666 - val_accuracy: 0.6044\n",
      "Epoch 29/100\n",
      "391/391 [==============================] - ETA: 0s - loss: 0.8745 - accuracy: 0.7410\n",
      "Epoch 00029: val_accuracy improved from 0.62700 to 0.62810, saving model to VGG_SGD_no_reg_weights_amit.hdf5\n",
      "391/391 [==============================] - 323s 827ms/step - loss: 0.8745 - accuracy: 0.7410 - val_loss: 1.4594 - val_accuracy: 0.6281\n",
      "Epoch 30/100\n",
      "391/391 [==============================] - ETA: 0s - loss: 0.8484 - accuracy: 0.7485\n",
      "Epoch 00030: val_accuracy did not improve from 0.62810\n",
      "391/391 [==============================] - 323s 825ms/step - loss: 0.8484 - accuracy: 0.7485 - val_loss: 1.7714 - val_accuracy: 0.6011\n",
      "Epoch 31/100\n",
      "391/391 [==============================] - ETA: 0s - loss: 0.8167 - accuracy: 0.7570\n",
      "Epoch 00031: val_accuracy did not improve from 0.62810\n",
      "391/391 [==============================] - 323s 826ms/step - loss: 0.8167 - accuracy: 0.7570 - val_loss: 1.4895 - val_accuracy: 0.6257\n",
      "Epoch 32/100\n",
      "391/391 [==============================] - ETA: 0s - loss: 0.7887 - accuracy: 0.7663\n",
      "Epoch 00032: val_accuracy did not improve from 0.62810\n",
      "391/391 [==============================] - 323s 825ms/step - loss: 0.7887 - accuracy: 0.7663 - val_loss: 1.6469 - val_accuracy: 0.6100\n",
      "Epoch 33/100\n",
      "391/391 [==============================] - ETA: 0s - loss: 0.7686 - accuracy: 0.7721\n",
      "Epoch 00033: val_accuracy did not improve from 0.62810\n",
      "391/391 [==============================] - 323s 827ms/step - loss: 0.7686 - accuracy: 0.7721 - val_loss: 1.5903 - val_accuracy: 0.6203\n",
      "Epoch 34/100\n",
      "391/391 [==============================] - ETA: 0s - loss: 0.7462 - accuracy: 0.7777\n",
      "Epoch 00034: val_accuracy improved from 0.62810 to 0.62970, saving model to VGG_SGD_no_reg_weights_amit.hdf5\n",
      "391/391 [==============================] - 324s 828ms/step - loss: 0.7462 - accuracy: 0.7777 - val_loss: 1.5664 - val_accuracy: 0.6297\n",
      "Epoch 35/100\n",
      "391/391 [==============================] - ETA: 0s - loss: 0.7292 - accuracy: 0.7820\n",
      "Epoch 00035: val_accuracy did not improve from 0.62970\n",
      "391/391 [==============================] - 323s 825ms/step - loss: 0.7292 - accuracy: 0.7820 - val_loss: 1.6924 - val_accuracy: 0.6147\n",
      "Epoch 36/100\n",
      "391/391 [==============================] - ETA: 0s - loss: 0.7065 - accuracy: 0.7883\n",
      "Epoch 00036: val_accuracy did not improve from 0.62970\n",
      "391/391 [==============================] - 323s 825ms/step - loss: 0.7065 - accuracy: 0.7883 - val_loss: 1.6677 - val_accuracy: 0.6182\n",
      "Epoch 37/100\n",
      "391/391 [==============================] - ETA: 0s - loss: 0.6784 - accuracy: 0.7968\n",
      "Epoch 00037: val_accuracy did not improve from 0.62970\n",
      "391/391 [==============================] - 323s 825ms/step - loss: 0.6784 - accuracy: 0.7968 - val_loss: 1.5919 - val_accuracy: 0.6253\n",
      "Epoch 38/100\n",
      "391/391 [==============================] - ETA: 0s - loss: 0.6706 - accuracy: 0.7999\n",
      "Epoch 00038: val_accuracy did not improve from 0.62970\n",
      "391/391 [==============================] - 323s 825ms/step - loss: 0.6706 - accuracy: 0.7999 - val_loss: 1.6647 - val_accuracy: 0.6282\n",
      "Epoch 39/100\n",
      "391/391 [==============================] - ETA: 0s - loss: 0.6538 - accuracy: 0.8031\n",
      "Epoch 00039: val_accuracy did not improve from 0.62970\n",
      "Restoring model weights from the end of the best epoch.\n",
      "391/391 [==============================] - 323s 826ms/step - loss: 0.6538 - accuracy: 0.8031 - val_loss: 1.6981 - val_accuracy: 0.6244\n",
      "Epoch 00039: early stopping\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.keras.callbacks.History at 0x7f0aa72cc828>"
      ]
     },
     "execution_count": 3,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.fit(aug_data.flow(x_train, y_train, batch_size=128), batch_size=128, epochs=100, verbose=1, validation_data=(x_test, y_test),callbacks=[checkpoint,early])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "id": "4zXe3HN_8An0",
    "outputId": "9bfa3def-19a6-495a-d4ff-da17d94e5cd3"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From <ipython-input-4-44ed652cd446>:1: Sequential.predict_classes (from tensorflow.python.keras.engine.sequential) is deprecated and will be removed after 2021-01-01.\n",
      "Instructions for updating:\n",
      "Please use instead:* `np.argmax(model.predict(x), axis=-1)`,   if your model does multi-class classification   (e.g. if it uses a `softmax` last-layer activation).* `(model.predict(x) > 0.5).astype(\"int32\")`,   if your model does binary classification   (e.g. if it uses a `sigmoid` last-layer activation).\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.94      0.72      0.81       100\n",
      "           1       0.90      0.69      0.78       100\n",
      "           2       0.51      0.55      0.53       100\n",
      "           3       0.56      0.42      0.48       100\n",
      "           4       0.51      0.34      0.41       100\n",
      "           5       0.73      0.59      0.65       100\n",
      "           6       0.70      0.57      0.63       100\n",
      "           7       0.76      0.62      0.68       100\n",
      "           8       0.68      0.84      0.75       100\n",
      "           9       0.71      0.77      0.74       100\n",
      "          10       0.52      0.29      0.37       100\n",
      "          11       0.35      0.54      0.42       100\n",
      "          12       0.63      0.75      0.68       100\n",
      "          13       0.59      0.47      0.53       100\n",
      "          14       0.51      0.69      0.58       100\n",
      "          15       0.58      0.60      0.59       100\n",
      "          16       0.52      0.68      0.59       100\n",
      "          17       0.78      0.76      0.77       100\n",
      "          18       0.60      0.49      0.54       100\n",
      "          19       0.70      0.51      0.59       100\n",
      "          20       0.82      0.84      0.83       100\n",
      "          21       0.73      0.80      0.77       100\n",
      "          22       0.63      0.62      0.62       100\n",
      "          23       0.76      0.68      0.72       100\n",
      "          24       0.86      0.84      0.85       100\n",
      "          25       0.74      0.48      0.58       100\n",
      "          26       0.50      0.78      0.61       100\n",
      "          27       0.55      0.41      0.47       100\n",
      "          28       0.74      0.77      0.75       100\n",
      "          29       0.68      0.60      0.64       100\n",
      "          30       0.61      0.58      0.59       100\n",
      "          31       0.54      0.65      0.59       100\n",
      "          32       0.65      0.60      0.62       100\n",
      "          33       0.72      0.49      0.58       100\n",
      "          34       0.69      0.66      0.67       100\n",
      "          35       0.45      0.47      0.46       100\n",
      "          36       0.85      0.66      0.74       100\n",
      "          37       0.64      0.62      0.63       100\n",
      "          38       0.42      0.57      0.49       100\n",
      "          39       0.63      0.79      0.70       100\n",
      "          40       0.53      0.49      0.51       100\n",
      "          41       0.76      0.90      0.82       100\n",
      "          42       0.45      0.77      0.56       100\n",
      "          43       0.78      0.74      0.76       100\n",
      "          44       0.36      0.41      0.38       100\n",
      "          45       0.67      0.38      0.48       100\n",
      "          46       0.45      0.45      0.45       100\n",
      "          47       0.54      0.67      0.60       100\n",
      "          48       0.71      0.90      0.79       100\n",
      "          49       0.80      0.69      0.74       100\n",
      "          50       0.35      0.47      0.40       100\n",
      "          51       0.58      0.66      0.62       100\n",
      "          52       0.60      0.59      0.59       100\n",
      "          53       0.83      0.76      0.79       100\n",
      "          54       0.76      0.70      0.73       100\n",
      "          55       0.39      0.40      0.39       100\n",
      "          56       0.81      0.80      0.80       100\n",
      "          57       0.66      0.66      0.66       100\n",
      "          58       0.71      0.90      0.79       100\n",
      "          59       0.61      0.71      0.66       100\n",
      "          60       0.89      0.47      0.61       100\n",
      "          61       0.62      0.69      0.65       100\n",
      "          62       0.74      0.60      0.66       100\n",
      "          63       0.77      0.57      0.66       100\n",
      "          64       0.59      0.51      0.55       100\n",
      "          65       0.53      0.42      0.47       100\n",
      "          66       0.55      0.84      0.67       100\n",
      "          67       0.58      0.49      0.53       100\n",
      "          68       0.83      0.86      0.85       100\n",
      "          69       0.74      0.76      0.75       100\n",
      "          70       0.68      0.73      0.71       100\n",
      "          71       0.75      0.72      0.73       100\n",
      "          72       0.47      0.29      0.36       100\n",
      "          73       0.56      0.51      0.53       100\n",
      "          74       0.39      0.54      0.45       100\n",
      "          75       0.77      0.86      0.81       100\n",
      "          76       0.74      0.83      0.78       100\n",
      "          77       0.78      0.43      0.55       100\n",
      "          78       0.33      0.67      0.45       100\n",
      "          79       0.76      0.64      0.70       100\n",
      "          80       0.51      0.44      0.47       100\n",
      "          81       0.55      0.79      0.65       100\n",
      "          82       0.93      0.82      0.87       100\n",
      "          83       0.69      0.53      0.60       100\n",
      "          84       0.62      0.53      0.57       100\n",
      "          85       0.80      0.67      0.73       100\n",
      "          86       0.61      0.71      0.65       100\n",
      "          87       0.56      0.75      0.64       100\n",
      "          88       0.79      0.62      0.70       100\n",
      "          89       0.78      0.72      0.75       100\n",
      "          90       0.67      0.62      0.65       100\n",
      "          91       0.80      0.68      0.74       100\n",
      "          92       0.50      0.61      0.55       100\n",
      "          93       0.60      0.45      0.51       100\n",
      "          94       0.77      0.83      0.80       100\n",
      "          95       0.65      0.71      0.68       100\n",
      "          96       0.73      0.41      0.53       100\n",
      "          97       0.69      0.77      0.73       100\n",
      "          98       0.47      0.43      0.45       100\n",
      "          99       0.61      0.60      0.61       100\n",
      "\n",
      "    accuracy                           0.63     10000\n",
      "   macro avg       0.65      0.63      0.63     10000\n",
      "weighted avg       0.65      0.63      0.63     10000\n",
      "\n",
      "Accuracy is 0.6297\n",
      "Precision is 0.6468087651919263\n",
      "Recall is 0.6297\n"
     ]
    }
   ],
   "source": [
    "y_pred=model.predict_classes(x_test)\n",
    "from sklearn.metrics import classification_report,accuracy_score,recall_score,precision_score\n",
    "print(classification_report(Y_test,y_pred))\n",
    "print(\"Accuracy is {}\".format(accuracy_score(Y_test,y_pred)))\n",
    "print(\"Precision is {}\".format(precision_score(Y_test,y_pred,average=\"weighted\")))\n",
    "print(\"Recall is {}\".format(recall_score(Y_test,y_pred,average=\"weighted\")))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "id": "yv4sZ5OK8DdM",
    "outputId": "16d901f8-f5c8-4976-fac1-be534de4f64c"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy is 0.8882\n"
     ]
    }
   ],
   "source": [
    "YY=model.predict_classes(x_train)\n",
    "print(\"Accuracy is {}\".format(accuracy_score(Y_train,YY)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "id": "3_QZ2FvaCoaZ",
    "outputId": "6d54937b-3b3e-4f85-96ff-566a2b277709"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "conv2d (Conv2D)              (None, 32, 32, 64)        1792      \n",
      "_________________________________________________________________\n",
      "leaky_re_lu (LeakyReLU)      (None, 32, 32, 64)        0         \n",
      "_________________________________________________________________\n",
      "conv2d_1 (Conv2D)            (None, 32, 32, 64)        36928     \n",
      "_________________________________________________________________\n",
      "leaky_re_lu_1 (LeakyReLU)    (None, 32, 32, 64)        0         \n",
      "_________________________________________________________________\n",
      "max_pooling2d (MaxPooling2D) (None, 31, 31, 64)        0         \n",
      "_________________________________________________________________\n",
      "conv2d_2 (Conv2D)            (None, 31, 31, 128)       73856     \n",
      "_________________________________________________________________\n",
      "leaky_re_lu_2 (LeakyReLU)    (None, 31, 31, 128)       0         \n",
      "_________________________________________________________________\n",
      "conv2d_3 (Conv2D)            (None, 31, 31, 128)       147584    \n",
      "_________________________________________________________________\n",
      "leaky_re_lu_3 (LeakyReLU)    (None, 31, 31, 128)       0         \n",
      "_________________________________________________________________\n",
      "max_pooling2d_1 (MaxPooling2 (None, 30, 30, 128)       0         \n",
      "_________________________________________________________________\n",
      "conv2d_4 (Conv2D)            (None, 30, 30, 256)       295168    \n",
      "_________________________________________________________________\n",
      "leaky_re_lu_4 (LeakyReLU)    (None, 30, 30, 256)       0         \n",
      "_________________________________________________________________\n",
      "conv2d_5 (Conv2D)            (None, 30, 30, 256)       590080    \n",
      "_________________________________________________________________\n",
      "leaky_re_lu_5 (LeakyReLU)    (None, 30, 30, 256)       0         \n",
      "_________________________________________________________________\n",
      "conv2d_6 (Conv2D)            (None, 30, 30, 256)       590080    \n",
      "_________________________________________________________________\n",
      "leaky_re_lu_6 (LeakyReLU)    (None, 30, 30, 256)       0         \n",
      "_________________________________________________________________\n",
      "max_pooling2d_2 (MaxPooling2 (None, 29, 29, 256)       0         \n",
      "_________________________________________________________________\n",
      "conv2d_7 (Conv2D)            (None, 29, 29, 512)       1180160   \n",
      "_________________________________________________________________\n",
      "leaky_re_lu_7 (LeakyReLU)    (None, 29, 29, 512)       0         \n",
      "_________________________________________________________________\n",
      "conv2d_8 (Conv2D)            (None, 29, 29, 512)       2359808   \n",
      "_________________________________________________________________\n",
      "leaky_re_lu_8 (LeakyReLU)    (None, 29, 29, 512)       0         \n",
      "_________________________________________________________________\n",
      "conv2d_9 (Conv2D)            (None, 29, 29, 512)       2359808   \n",
      "_________________________________________________________________\n",
      "leaky_re_lu_9 (LeakyReLU)    (None, 29, 29, 512)       0         \n",
      "_________________________________________________________________\n",
      "max_pooling2d_3 (MaxPooling2 (None, 14, 14, 512)       0         \n",
      "_________________________________________________________________\n",
      "conv2d_10 (Conv2D)           (None, 14, 14, 512)       2359808   \n",
      "_________________________________________________________________\n",
      "leaky_re_lu_10 (LeakyReLU)   (None, 14, 14, 512)       0         \n",
      "_________________________________________________________________\n",
      "conv2d_11 (Conv2D)           (None, 14, 14, 512)       2359808   \n",
      "_________________________________________________________________\n",
      "leaky_re_lu_11 (LeakyReLU)   (None, 14, 14, 512)       0         \n",
      "_________________________________________________________________\n",
      "conv2d_12 (Conv2D)           (None, 14, 14, 512)       2359808   \n",
      "_________________________________________________________________\n",
      "leaky_re_lu_12 (LeakyReLU)   (None, 14, 14, 512)       0         \n",
      "_________________________________________________________________\n",
      "max_pooling2d_4 (MaxPooling2 (None, 13, 13, 512)       0         \n",
      "_________________________________________________________________\n",
      "flatten (Flatten)            (None, 86528)             0         \n",
      "_________________________________________________________________\n",
      "dense (Dense)                (None, 512)               44302848  \n",
      "_________________________________________________________________\n",
      "leaky_re_lu_13 (LeakyReLU)   (None, 512)               0         \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 100)               51300     \n",
      "=================================================================\n",
      "Total params: 59,068,836\n",
      "Trainable params: 59,068,836\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From C:\\Users\\uamit\\Anaconda3\\lib\\site-packages\\keras\\backend\\tensorflow_backend.py:4070: The name tf.nn.max_pool is deprecated. Please use tf.nn.max_pool2d instead.\n",
      "\n",
      "WARNING:tensorflow:From C:\\Users\\uamit\\Anaconda3\\lib\\site-packages\\keras\\backend\\tensorflow_backend.py:422: The name tf.global_variables is deprecated. Please use tf.compat.v1.global_variables instead.\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.94      0.72      0.81       100\n",
      "           1       0.90      0.69      0.78       100\n",
      "           2       0.51      0.55      0.53       100\n",
      "           3       0.56      0.42      0.48       100\n",
      "           4       0.51      0.34      0.41       100\n",
      "           5       0.73      0.59      0.65       100\n",
      "           6       0.70      0.57      0.63       100\n",
      "           7       0.76      0.62      0.68       100\n",
      "           8       0.68      0.84      0.75       100\n",
      "           9       0.71      0.77      0.74       100\n",
      "          10       0.52      0.29      0.37       100\n",
      "          11       0.35      0.54      0.42       100\n",
      "          12       0.63      0.75      0.68       100\n",
      "          13       0.59      0.47      0.53       100\n",
      "          14       0.51      0.69      0.58       100\n",
      "          15       0.58      0.60      0.59       100\n",
      "          16       0.52      0.68      0.59       100\n",
      "          17       0.78      0.76      0.77       100\n",
      "          18       0.60      0.49      0.54       100\n",
      "          19       0.70      0.51      0.59       100\n",
      "          20       0.82      0.84      0.83       100\n",
      "          21       0.73      0.80      0.77       100\n",
      "          22       0.63      0.62      0.62       100\n",
      "          23       0.76      0.68      0.72       100\n",
      "          24       0.86      0.84      0.85       100\n",
      "          25       0.74      0.48      0.58       100\n",
      "          26       0.50      0.78      0.61       100\n",
      "          27       0.55      0.41      0.47       100\n",
      "          28       0.74      0.77      0.75       100\n",
      "          29       0.68      0.60      0.64       100\n",
      "          30       0.61      0.58      0.59       100\n",
      "          31       0.54      0.65      0.59       100\n",
      "          32       0.65      0.60      0.62       100\n",
      "          33       0.72      0.49      0.58       100\n",
      "          34       0.69      0.66      0.67       100\n",
      "          35       0.45      0.47      0.46       100\n",
      "          36       0.85      0.66      0.74       100\n",
      "          37       0.64      0.62      0.63       100\n",
      "          38       0.42      0.57      0.49       100\n",
      "          39       0.63      0.79      0.70       100\n",
      "          40       0.53      0.49      0.51       100\n",
      "          41       0.76      0.90      0.82       100\n",
      "          42       0.45      0.77      0.56       100\n",
      "          43       0.78      0.74      0.76       100\n",
      "          44       0.36      0.41      0.38       100\n",
      "          45       0.67      0.38      0.48       100\n",
      "          46       0.45      0.45      0.45       100\n",
      "          47       0.54      0.67      0.60       100\n",
      "          48       0.71      0.90      0.79       100\n",
      "          49       0.80      0.69      0.74       100\n",
      "          50       0.35      0.47      0.40       100\n",
      "          51       0.58      0.66      0.62       100\n",
      "          52       0.60      0.59      0.59       100\n",
      "          53       0.83      0.76      0.79       100\n",
      "          54       0.76      0.70      0.73       100\n",
      "          55       0.39      0.40      0.39       100\n",
      "          56       0.81      0.80      0.80       100\n",
      "          57       0.66      0.66      0.66       100\n",
      "          58       0.71      0.90      0.79       100\n",
      "          59       0.61      0.71      0.66       100\n",
      "          60       0.89      0.47      0.61       100\n",
      "          61       0.62      0.69      0.65       100\n",
      "          62       0.74      0.60      0.66       100\n",
      "          63       0.77      0.57      0.66       100\n",
      "          64       0.59      0.51      0.55       100\n",
      "          65       0.53      0.42      0.47       100\n",
      "          66       0.55      0.84      0.67       100\n",
      "          67       0.58      0.49      0.53       100\n",
      "          68       0.83      0.86      0.85       100\n",
      "          69       0.74      0.76      0.75       100\n",
      "          70       0.68      0.73      0.71       100\n",
      "          71       0.75      0.72      0.73       100\n",
      "          72       0.47      0.29      0.36       100\n",
      "          73       0.56      0.51      0.53       100\n",
      "          74       0.39      0.54      0.45       100\n",
      "          75       0.77      0.86      0.81       100\n",
      "          76       0.74      0.83      0.78       100\n",
      "          77       0.78      0.43      0.55       100\n",
      "          78       0.33      0.67      0.45       100\n",
      "          79       0.76      0.64      0.70       100\n",
      "          80       0.51      0.44      0.47       100\n",
      "          81       0.55      0.79      0.65       100\n",
      "          82       0.93      0.82      0.87       100\n",
      "          83       0.69      0.53      0.60       100\n",
      "          84       0.62      0.53      0.57       100\n",
      "          85       0.80      0.67      0.73       100\n",
      "          86       0.61      0.71      0.65       100\n",
      "          87       0.56      0.75      0.64       100\n",
      "          88       0.79      0.62      0.70       100\n",
      "          89       0.78      0.72      0.75       100\n",
      "          90       0.67      0.62      0.65       100\n",
      "          91       0.80      0.68      0.74       100\n",
      "          92       0.50      0.61      0.55       100\n",
      "          93       0.60      0.45      0.51       100\n",
      "          94       0.77      0.83      0.80       100\n",
      "          95       0.65      0.71      0.68       100\n",
      "          96       0.73      0.41      0.53       100\n",
      "          97       0.69      0.77      0.73       100\n",
      "          98       0.47      0.43      0.45       100\n",
      "          99       0.61      0.60      0.61       100\n",
      "\n",
      "    accuracy                           0.63     10000\n",
      "   macro avg       0.65      0.63      0.63     10000\n",
      "weighted avg       0.65      0.63      0.63     10000\n",
      "\n",
      "Accuracy is 0.6297\n",
      "Precision is 0.6468087651919263\n",
      "Recall is 0.6297\n"
     ]
    }
   ],
   "source": [
    "from keras.callbacks import ModelCheckpoint, EarlyStopping\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Conv2D, MaxPool2D, Flatten, Dense,BatchNormalization,Activation,Dropout,LeakyReLU\n",
    "from tensorflow.keras.optimizers import SGD,Adam\n",
    "from keras.preprocessing.image import ImageDataGenerator\n",
    "from keras.utils import to_categorical\n",
    "from tensorflow.keras.datasets import cifar100\n",
    "\n",
    "\n",
    "\n",
    "(x_train, Y_train), (x_test, Y_test) = cifar100.load_data()\n",
    "#x_train = x_train.astype('float32') / 255\n",
    "#x_test = x_test.astype('float32') / 255\n",
    "\n",
    "y_train = to_categorical(Y_train)\n",
    "y_test = to_categorical(Y_test)\n",
    "\n",
    "\n",
    "#import tensorflow as \n",
    "model = Sequential()\n",
    "\n",
    "# Creating first block- (2 Convolution + 1 Max pool)\n",
    "model.add(Conv2D(filters= 64, kernel_size= (3,3), strides= (1,1), padding='same', input_shape= (32, 32, 3)))\n",
    "model.add(LeakyReLU(alpha=0.1))\n",
    "model.add(Conv2D(filters= 64, kernel_size= (3,3), strides= (1,1), padding='same'))\n",
    "model.add(LeakyReLU(alpha=0.1))\n",
    "model.add(MaxPool2D(pool_size= (2,2), strides=(1,1)))\n",
    "\n",
    "# Creating second block- (2 Convolution + 1 Max pool)\n",
    "model.add(Conv2D(filters= 128, kernel_size= (3,3), strides= (1,1), padding='same'))\n",
    "model.add(LeakyReLU(alpha=0.1))\n",
    "model.add(Conv2D(filters= 128, kernel_size= (3,3), strides= (1,1), padding='same'))\n",
    "model.add(LeakyReLU(alpha=0.1))\n",
    "model.add(MaxPool2D(pool_size= (2,2), strides=(1,1)))\n",
    "\n",
    "\n",
    "# Creating third block- (3 Convolution + 1 Max pool)\n",
    "model.add(Conv2D(filters= 256, kernel_size= (3,3), strides= (1,1), padding='same'))\n",
    "model.add(LeakyReLU(alpha=0.1))\n",
    "model.add(Conv2D(filters= 256, kernel_size= (3,3), strides= (1,1), padding='same'))\n",
    "model.add(LeakyReLU(alpha=0.1))\n",
    "model.add(Conv2D(filters= 256, kernel_size= (3,3), strides= (1,1), padding='same'))\n",
    "model.add(LeakyReLU(alpha=0.1))\n",
    "model.add(MaxPool2D(pool_size= (2,2), strides=(1,1)))\n",
    "\n",
    "# Creating fourth block- (3 Convolution + 1 Max pool)\n",
    "model.add(Conv2D(filters= 512, kernel_size= (3,3), strides= (1,1), padding='same'))\n",
    "model.add(LeakyReLU(alpha=0.1))\n",
    "model.add(Conv2D(filters= 512, kernel_size= (3,3), strides= (1,1), padding='same'))\n",
    "model.add(LeakyReLU(alpha=0.1))\n",
    "model.add(Conv2D(filters= 512, kernel_size= (3,3), strides= (1,1), padding='same'))\n",
    "model.add(LeakyReLU(alpha=0.1))\n",
    "model.add(MaxPool2D(pool_size= (2,2), strides=(2,2)))\n",
    "\n",
    "# Creating fifth block- (3 Convolution + 1 Max pool)\n",
    "model.add(Conv2D(filters= 512, kernel_size= (3,3), strides= (1,1), padding='same'))\n",
    "model.add(LeakyReLU(alpha=0.1))\n",
    "model.add(Conv2D(filters= 512, kernel_size= (3,3), strides= (1,1), padding='same'))\n",
    "model.add(LeakyReLU(alpha=0.1))\n",
    "model.add(Conv2D(filters= 512, kernel_size= (3,3), strides= (1,1), padding='same'))\n",
    "model.add(LeakyReLU(alpha=0.1))\n",
    "model.add(MaxPool2D(pool_size= (2,2), strides=(1,1)))\n",
    "\n",
    "# Flattening the pooled image pixels\n",
    "model.add(Flatten())\n",
    "\n",
    "# Creating 2 Dense Layers\n",
    "model.add(Dense(units= 512))\n",
    "model.add(LeakyReLU(alpha=0.1))\n",
    "\n",
    "model.add(Dense(units= 100, activation='softmax'))\n",
    "\n",
    "model.load_weights(\"../weights/VGG_SGD_no_reg_weights_amit.hdf5\")\n",
    "\n",
    "y_true = y_test.argmax(-1)\n",
    "y_pred = model.predict(x_test).argmax(-1)\n",
    "\n",
    "from sklearn.metrics import classification_report,accuracy_score,recall_score,precision_score\n",
    "print(classification_report(Y_test,y_pred))\n",
    "print(\"Accuracy is {}\".format(accuracy_score(Y_test,y_pred)))\n",
    "print(\"Precision is {}\".format(precision_score(Y_test,y_pred,average=\"weighted\")))\n",
    "print(\"Recall is {}\".format(recall_score(Y_test,y_pred,average=\"weighted\")))"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "name": "VGG_SGD_no_reg_weights_amit.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
