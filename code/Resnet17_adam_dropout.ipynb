{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "id": "lKx28KtBisLa"
   },
   "outputs": [],
   "source": [
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras.layers import Conv2D,Dense,MaxPool2D,AveragePooling2D,Flatten,Dropout\n",
    "import tensorflow.keras as keras\n",
    "from tensorflow.keras.datasets import cifar100\n",
    "from tensorflow.keras.utils import to_categorical\n",
    "from tensorflow.keras import layers\n",
    "\n",
    "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
    "from tensorflow.keras.optimizers import SGD,Adam\n",
    "\n",
    "from tensorflow.keras.callbacks import EarlyStopping,ModelCheckpoint\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "id": "zbwsICWgiwzY"
   },
   "outputs": [],
   "source": [
    "(X_train,Y_train),(X_test,Y_test)=cifar100.load_data()\n",
    "\n",
    "X_train,X_test=X_train/255,X_test/255\n",
    "y_train,y_test=to_categorical(Y_train),to_categorical(Y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "id": "7Vqn54fQiytj"
   },
   "outputs": [],
   "source": [
    "aug_data=ImageDataGenerator(rotation_range=15,horizontal_flip=True,width_shift_range=0.01,height_shift_range=0.01,zoom_range=0.3)\n",
    "aug_data.fit(X_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "id": "w3hGn1FWi08G"
   },
   "outputs": [],
   "source": [
    "shape=(32,32,3)\n",
    "\n",
    "inputs = keras.Input(shape=shape)\n",
    "\n",
    "a=layers.Conv2D(32,(3,3),strides=1,padding=\"same\",activation=\"relu\")(inputs)  #1\n",
    "a=layers.Dropout(0.2)(a)\n",
    "a=layers.Conv2D(32,(3,3),strides=1,padding=\"same\",activation=\"relu\")(a) \n",
    "a=layers.Dropout(0.2)(a)  #2\n",
    "a_temp=a\n",
    "a=layers.Conv2D(32,(3,3),strides=1,padding=\"same\",activation=\"relu\")(a)#\\3\n",
    "a=layers.Dropout(0.2)(a)\n",
    "a=layers.Conv2D(32,(3,3),strides=1,padding=\"same\")(a)#4\n",
    "a=layers.Dropout(0.2)(a)\n",
    "a=layers.Add()([a,a_temp])\n",
    "a=layers.Activation(\"relu\")(a)\n",
    "\n",
    "b=layers.Conv2D(64,(3,3),strides=1,padding=\"same\",activation=\"relu\")(a) #1\n",
    "                               \n",
    "#a_temp=a\n",
    "##########################################################################\n",
    "a=layers.Conv2D(64,(3,3),strides=1,padding=\"same\",activation=\"relu\")(a) #5\n",
    "a=layers.Dropout(0.2)(a)\n",
    "a=layers.Conv2D(64,(3,3),strides=1,padding=\"same\")(a) #6\n",
    "a=layers.Dropout(0.2)(a)\n",
    "a=layers.Add()([b,a])\n",
    "a=layers.Activation(\"relu\")(a)\n",
    "a_temp=a\n",
    "a=layers.Conv2D(64,(3,3),strides=1,padding=\"same\",activation=\"relu\")(a) #7\n",
    "a=layers.Dropout(0.2)(a)\n",
    "a=layers.Conv2D(64,(3,3),strides=1,padding=\"same\")(a) #8\n",
    "a=layers.Dropout(0.2)(a)\n",
    "a=layers.Add()([a,a_temp])\n",
    "a=layers.Activation(\"relu\")(a)\n",
    "\n",
    "b=layers.Conv2D(128,(3,3),strides=1,padding=\"same\",activation=\"relu\")(a) #2\n",
    "\n",
    "###############################################################################\n",
    "\n",
    "a=layers.Conv2D(128,(3,3),strides=1,padding=\"same\",activation=\"relu\")(a) #9\n",
    "a=layers.Dropout(0.2)(a)\n",
    "a=layers.Conv2D(128,(3,3),strides=1,padding=\"same\")(a) #10\n",
    "a=layers.Dropout(0.2)(a)\n",
    "a=layers.Add()([b,a])\n",
    "a=layers.Activation(\"relu\")(a)\n",
    "a_temp=a\n",
    "a=layers.Conv2D(128,(3,3),strides=1,padding=\"same\",activation=\"relu\")(a) #11\n",
    "a=layers.Dropout(0.2)(a)\n",
    "a=layers.Conv2D(128,(3,3),strides=1,padding=\"same\")(a) #12\n",
    "a=layers.Dropout(0.2)(a)\n",
    "a=layers.Add()([a,a_temp])\n",
    "a=layers.Activation(\"relu\")(a)\n",
    "\n",
    "b=layers.Conv2D(256,(3,3),strides=1,padding=\"same\",activation=\"relu\")(a) #3\n",
    "\n",
    "#################################################################################\n",
    "\n",
    "a=layers.Conv2D(256,(3,3),strides=1,padding=\"same\",activation=\"relu\")(a) #13\n",
    "a=layers.Dropout(0.2)(a)\n",
    "a=layers.Conv2D(256,(3,3),strides=1,padding=\"same\")(a) #14\n",
    "a=layers.Dropout(0.2)(a)\n",
    "a=layers.Add()([b,a])\n",
    "a=layers.Activation(\"relu\")(a)\n",
    "a_temp=a\n",
    "a=layers.Conv2D(256,(3,3),strides=1,padding=\"same\",activation=\"relu\")(a) #15\n",
    "a=layers.Dropout(0.2)(a)\n",
    "a=layers.Conv2D(256,(3,3),strides=1,padding=\"same\")(a) #16\n",
    "a=layers.Dropout(0.2)(a)\n",
    "a=layers.Add()([a,a_temp])\n",
    "a=layers.Activation(\"relu\")(a)\n",
    "############################################################3\n",
    "a=layers.AveragePooling2D(pool_size=(2,2),strides=1,padding=\"same\")(a)\n",
    "\n",
    "a=Flatten()(a)\n",
    "\n",
    "#############################################################\n",
    "\n",
    "a=layers.Dense(512,activation=\"relu\")(a)\n",
    "a=layers.Dropout(0.2)(a)\n",
    "outputs=layers.Dense(y_train.shape[1],activation=\"softmax\")(a)\n",
    "\n",
    "\n",
    "model = keras.Model(inputs, outputs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "id": "yTkIId7ti2qP",
    "outputId": "f44131e5-8d84-41f8-c78d-7abd21fea6d8"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:`period` argument is deprecated. Please use `save_freq` to specify the frequency in number of batches seen.\n"
     ]
    }
   ],
   "source": [
    "adam = Adam(learning_rate=0.001,clipvalue=0.6)\n",
    "\n",
    "\n",
    "model.compile(loss=\"categorical_crossentropy\",optimizer=adam,metrics=[\"accuracy\"])\n",
    "call=EarlyStopping(monitor=\"val_loss\",verbose=1,mode=\"auto\",patience=5,restore_best_weights=True)\n",
    "checkpoint=ModelCheckpoint('ADAM_WITH_DROPOUT_RESNET.hdf5',monitor='val_accuracy',verbose=1,save_best_only=True,save_weights_only=True,model='auto',period=1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "id": "0pyoZJKRi7R0",
    "outputId": "8fa73cc7-285b-4d28-e9c1-6bac4033d2e6"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/80\n",
      "391/390 [==============================] - ETA: 0s - loss: 4.4576 - accuracy: 0.0361\n",
      "Epoch 00001: val_accuracy improved from -inf to 0.07140, saving model to ADAM_WITH_DROPOUT_RESNET.hdf5\n",
      "391/390 [==============================] - 97s 248ms/step - loss: 4.4576 - accuracy: 0.0361 - val_loss: 4.0422 - val_accuracy: 0.0714\n",
      "Epoch 2/80\n",
      "391/390 [==============================] - ETA: 0s - loss: 3.9164 - accuracy: 0.0901\n",
      "Epoch 00002: val_accuracy improved from 0.07140 to 0.12900, saving model to ADAM_WITH_DROPOUT_RESNET.hdf5\n",
      "391/390 [==============================] - 96s 247ms/step - loss: 3.9164 - accuracy: 0.0901 - val_loss: 3.6997 - val_accuracy: 0.1290\n",
      "Epoch 3/80\n",
      "391/390 [==============================] - ETA: 0s - loss: 3.6887 - accuracy: 0.1315\n",
      "Epoch 00003: val_accuracy improved from 0.12900 to 0.13860, saving model to ADAM_WITH_DROPOUT_RESNET.hdf5\n",
      "391/390 [==============================] - 96s 247ms/step - loss: 3.6887 - accuracy: 0.1315 - val_loss: 3.6979 - val_accuracy: 0.1386\n",
      "Epoch 4/80\n",
      "391/390 [==============================] - ETA: 0s - loss: 3.4902 - accuracy: 0.1676\n",
      "Epoch 00004: val_accuracy improved from 0.13860 to 0.19820, saving model to ADAM_WITH_DROPOUT_RESNET.hdf5\n",
      "391/390 [==============================] - 97s 247ms/step - loss: 3.4902 - accuracy: 0.1676 - val_loss: 3.3700 - val_accuracy: 0.1982\n",
      "Epoch 5/80\n",
      "391/390 [==============================] - ETA: 0s - loss: 3.3347 - accuracy: 0.1948\n",
      "Epoch 00005: val_accuracy improved from 0.19820 to 0.22440, saving model to ADAM_WITH_DROPOUT_RESNET.hdf5\n",
      "391/390 [==============================] - 97s 247ms/step - loss: 3.3347 - accuracy: 0.1948 - val_loss: 3.2404 - val_accuracy: 0.2244\n",
      "Epoch 6/80\n",
      "391/390 [==============================] - ETA: 0s - loss: 3.2117 - accuracy: 0.2142\n",
      "Epoch 00006: val_accuracy improved from 0.22440 to 0.24720, saving model to ADAM_WITH_DROPOUT_RESNET.hdf5\n",
      "391/390 [==============================] - 97s 248ms/step - loss: 3.2117 - accuracy: 0.2142 - val_loss: 3.1256 - val_accuracy: 0.2472\n",
      "Epoch 7/80\n",
      "391/390 [==============================] - ETA: 0s - loss: 3.0980 - accuracy: 0.2362\n",
      "Epoch 00007: val_accuracy improved from 0.24720 to 0.27520, saving model to ADAM_WITH_DROPOUT_RESNET.hdf5\n",
      "391/390 [==============================] - 97s 248ms/step - loss: 3.0980 - accuracy: 0.2362 - val_loss: 2.9620 - val_accuracy: 0.2752\n",
      "Epoch 8/80\n",
      "391/390 [==============================] - ETA: 0s - loss: 3.0058 - accuracy: 0.2551\n",
      "Epoch 00008: val_accuracy did not improve from 0.27520\n",
      "391/390 [==============================] - 95s 243ms/step - loss: 3.0058 - accuracy: 0.2551 - val_loss: 2.9365 - val_accuracy: 0.2730\n",
      "Epoch 9/80\n",
      "391/390 [==============================] - ETA: 0s - loss: 2.9243 - accuracy: 0.2714\n",
      "Epoch 00009: val_accuracy improved from 0.27520 to 0.29150, saving model to ADAM_WITH_DROPOUT_RESNET.hdf5\n",
      "391/390 [==============================] - 97s 248ms/step - loss: 2.9243 - accuracy: 0.2714 - val_loss: 2.8503 - val_accuracy: 0.2915\n",
      "Epoch 10/80\n",
      "391/390 [==============================] - ETA: 0s - loss: 2.8363 - accuracy: 0.2886\n",
      "Epoch 00010: val_accuracy improved from 0.29150 to 0.31210, saving model to ADAM_WITH_DROPOUT_RESNET.hdf5\n",
      "391/390 [==============================] - 97s 248ms/step - loss: 2.8363 - accuracy: 0.2886 - val_loss: 2.7736 - val_accuracy: 0.3121\n",
      "Epoch 11/80\n",
      "391/390 [==============================] - ETA: 0s - loss: 2.7700 - accuracy: 0.3010\n",
      "Epoch 00011: val_accuracy improved from 0.31210 to 0.34420, saving model to ADAM_WITH_DROPOUT_RESNET.hdf5\n",
      "391/390 [==============================] - 97s 248ms/step - loss: 2.7700 - accuracy: 0.3010 - val_loss: 2.6216 - val_accuracy: 0.3442\n",
      "Epoch 12/80\n",
      "391/390 [==============================] - ETA: 0s - loss: 2.7069 - accuracy: 0.3156\n",
      "Epoch 00012: val_accuracy improved from 0.34420 to 0.35860, saving model to ADAM_WITH_DROPOUT_RESNET.hdf5\n",
      "391/390 [==============================] - 97s 248ms/step - loss: 2.7069 - accuracy: 0.3156 - val_loss: 2.5378 - val_accuracy: 0.3586\n",
      "Epoch 13/80\n",
      "391/390 [==============================] - ETA: 0s - loss: 2.6458 - accuracy: 0.3287\n",
      "Epoch 00013: val_accuracy improved from 0.35860 to 0.37700, saving model to ADAM_WITH_DROPOUT_RESNET.hdf5\n",
      "391/390 [==============================] - 97s 248ms/step - loss: 2.6458 - accuracy: 0.3287 - val_loss: 2.4601 - val_accuracy: 0.3770\n",
      "Epoch 14/80\n",
      "391/390 [==============================] - ETA: 0s - loss: 2.5937 - accuracy: 0.3375\n",
      "Epoch 00014: val_accuracy improved from 0.37700 to 0.38140, saving model to ADAM_WITH_DROPOUT_RESNET.hdf5\n",
      "391/390 [==============================] - 97s 248ms/step - loss: 2.5937 - accuracy: 0.3375 - val_loss: 2.3976 - val_accuracy: 0.3814\n",
      "Epoch 15/80\n",
      "391/390 [==============================] - ETA: 0s - loss: 2.5518 - accuracy: 0.3456\n",
      "Epoch 00015: val_accuracy improved from 0.38140 to 0.38380, saving model to ADAM_WITH_DROPOUT_RESNET.hdf5\n",
      "391/390 [==============================] - 97s 247ms/step - loss: 2.5518 - accuracy: 0.3456 - val_loss: 2.4109 - val_accuracy: 0.3838\n",
      "Epoch 16/80\n",
      "391/390 [==============================] - ETA: 0s - loss: 2.5068 - accuracy: 0.3531\n",
      "Epoch 00016: val_accuracy improved from 0.38380 to 0.40110, saving model to ADAM_WITH_DROPOUT_RESNET.hdf5\n",
      "391/390 [==============================] - 97s 248ms/step - loss: 2.5068 - accuracy: 0.3531 - val_loss: 2.3378 - val_accuracy: 0.4011\n",
      "Epoch 17/80\n",
      "391/390 [==============================] - ETA: 0s - loss: 2.4675 - accuracy: 0.3624\n",
      "Epoch 00017: val_accuracy improved from 0.40110 to 0.40510, saving model to ADAM_WITH_DROPOUT_RESNET.hdf5\n",
      "391/390 [==============================] - 97s 248ms/step - loss: 2.4675 - accuracy: 0.3624 - val_loss: 2.3108 - val_accuracy: 0.4051\n",
      "Epoch 18/80\n",
      "391/390 [==============================] - ETA: 0s - loss: 2.4186 - accuracy: 0.3741\n",
      "Epoch 00018: val_accuracy did not improve from 0.40510\n",
      "391/390 [==============================] - 95s 242ms/step - loss: 2.4186 - accuracy: 0.3741 - val_loss: 2.3332 - val_accuracy: 0.4032\n",
      "Epoch 19/80\n",
      "391/390 [==============================] - ETA: 0s - loss: 2.3810 - accuracy: 0.3808\n",
      "Epoch 00019: val_accuracy did not improve from 0.40510\n",
      "391/390 [==============================] - 95s 242ms/step - loss: 2.3810 - accuracy: 0.3808 - val_loss: 2.3446 - val_accuracy: 0.3974\n",
      "Epoch 20/80\n",
      "391/390 [==============================] - ETA: 0s - loss: 2.3559 - accuracy: 0.3877\n",
      "Epoch 00020: val_accuracy improved from 0.40510 to 0.41280, saving model to ADAM_WITH_DROPOUT_RESNET.hdf5\n",
      "391/390 [==============================] - 97s 247ms/step - loss: 2.3559 - accuracy: 0.3877 - val_loss: 2.3132 - val_accuracy: 0.4128\n",
      "Epoch 21/80\n",
      "391/390 [==============================] - ETA: 0s - loss: 2.3225 - accuracy: 0.3907\n",
      "Epoch 00021: val_accuracy improved from 0.41280 to 0.42420, saving model to ADAM_WITH_DROPOUT_RESNET.hdf5\n",
      "391/390 [==============================] - 97s 248ms/step - loss: 2.3225 - accuracy: 0.3907 - val_loss: 2.2106 - val_accuracy: 0.4242\n",
      "Epoch 22/80\n",
      "391/390 [==============================] - ETA: 0s - loss: 2.2902 - accuracy: 0.4003\n",
      "Epoch 00022: val_accuracy improved from 0.42420 to 0.42430, saving model to ADAM_WITH_DROPOUT_RESNET.hdf5\n",
      "391/390 [==============================] - 97s 247ms/step - loss: 2.2902 - accuracy: 0.4003 - val_loss: 2.2296 - val_accuracy: 0.4243\n",
      "Epoch 23/80\n",
      "391/390 [==============================] - ETA: 0s - loss: 2.2512 - accuracy: 0.4097\n",
      "Epoch 00023: val_accuracy improved from 0.42430 to 0.42840, saving model to ADAM_WITH_DROPOUT_RESNET.hdf5\n",
      "391/390 [==============================] - 97s 247ms/step - loss: 2.2512 - accuracy: 0.4097 - val_loss: 2.2237 - val_accuracy: 0.4284\n",
      "Epoch 24/80\n",
      "391/390 [==============================] - ETA: 0s - loss: 2.2300 - accuracy: 0.4107\n",
      "Epoch 00024: val_accuracy did not improve from 0.42840\n",
      "391/390 [==============================] - 95s 242ms/step - loss: 2.2300 - accuracy: 0.4107 - val_loss: 2.2673 - val_accuracy: 0.4163\n",
      "Epoch 25/80\n",
      "391/390 [==============================] - ETA: 0s - loss: 2.2006 - accuracy: 0.4185\n",
      "Epoch 00025: val_accuracy improved from 0.42840 to 0.42950, saving model to ADAM_WITH_DROPOUT_RESNET.hdf5\n",
      "391/390 [==============================] - 97s 247ms/step - loss: 2.2006 - accuracy: 0.4185 - val_loss: 2.2209 - val_accuracy: 0.4295\n",
      "Epoch 26/80\n",
      "391/390 [==============================] - ETA: 0s - loss: 2.1884 - accuracy: 0.4226\n",
      "Epoch 00026: val_accuracy improved from 0.42950 to 0.44780, saving model to ADAM_WITH_DROPOUT_RESNET.hdf5\n",
      "391/390 [==============================] - 97s 248ms/step - loss: 2.1884 - accuracy: 0.4226 - val_loss: 2.1469 - val_accuracy: 0.4478\n",
      "Epoch 27/80\n",
      "391/390 [==============================] - ETA: 0s - loss: 2.1574 - accuracy: 0.4307\n",
      "Epoch 00027: val_accuracy did not improve from 0.44780\n",
      "391/390 [==============================] - 95s 243ms/step - loss: 2.1574 - accuracy: 0.4307 - val_loss: 2.1394 - val_accuracy: 0.4446\n",
      "Epoch 28/80\n",
      "391/390 [==============================] - ETA: 0s - loss: 2.1363 - accuracy: 0.4321\n",
      "Epoch 00028: val_accuracy did not improve from 0.44780\n",
      "391/390 [==============================] - 95s 242ms/step - loss: 2.1363 - accuracy: 0.4321 - val_loss: 2.1704 - val_accuracy: 0.4365\n",
      "Epoch 29/80\n",
      "391/390 [==============================] - ETA: 0s - loss: 2.1029 - accuracy: 0.4413\n",
      "Epoch 00029: val_accuracy did not improve from 0.44780\n",
      "391/390 [==============================] - 95s 242ms/step - loss: 2.1029 - accuracy: 0.4413 - val_loss: 2.1689 - val_accuracy: 0.4415\n",
      "Epoch 30/80\n",
      "391/390 [==============================] - ETA: 0s - loss: 2.0882 - accuracy: 0.4424\n",
      "Epoch 00030: val_accuracy improved from 0.44780 to 0.45230, saving model to ADAM_WITH_DROPOUT_RESNET.hdf5\n",
      "391/390 [==============================] - 97s 248ms/step - loss: 2.0882 - accuracy: 0.4424 - val_loss: 2.1033 - val_accuracy: 0.4523\n",
      "Epoch 31/80\n",
      "391/390 [==============================] - ETA: 0s - loss: 2.0617 - accuracy: 0.4484\n",
      "Epoch 00031: val_accuracy did not improve from 0.45230\n",
      "391/390 [==============================] - 95s 242ms/step - loss: 2.0617 - accuracy: 0.4484 - val_loss: 2.1444 - val_accuracy: 0.4516\n",
      "Epoch 32/80\n",
      "391/390 [==============================] - ETA: 0s - loss: 2.0452 - accuracy: 0.4542\n",
      "Epoch 00032: val_accuracy improved from 0.45230 to 0.45610, saving model to ADAM_WITH_DROPOUT_RESNET.hdf5\n",
      "391/390 [==============================] - 97s 248ms/step - loss: 2.0452 - accuracy: 0.4542 - val_loss: 2.0931 - val_accuracy: 0.4561\n",
      "Epoch 33/80\n",
      "391/390 [==============================] - ETA: 0s - loss: 2.0287 - accuracy: 0.4563\n",
      "Epoch 00033: val_accuracy improved from 0.45610 to 0.46780, saving model to ADAM_WITH_DROPOUT_RESNET.hdf5\n",
      "391/390 [==============================] - 97s 248ms/step - loss: 2.0287 - accuracy: 0.4563 - val_loss: 2.0642 - val_accuracy: 0.4678\n",
      "Epoch 34/80\n",
      "391/390 [==============================] - ETA: 0s - loss: 2.0148 - accuracy: 0.4592\n",
      "Epoch 00034: val_accuracy did not improve from 0.46780\n",
      "391/390 [==============================] - 96s 244ms/step - loss: 2.0148 - accuracy: 0.4592 - val_loss: 2.0888 - val_accuracy: 0.4620\n",
      "Epoch 35/80\n",
      "391/390 [==============================] - ETA: 0s - loss: 1.9823 - accuracy: 0.4668\n",
      "Epoch 00035: val_accuracy did not improve from 0.46780\n",
      "391/390 [==============================] - 95s 243ms/step - loss: 1.9823 - accuracy: 0.4668 - val_loss: 2.1994 - val_accuracy: 0.4496\n",
      "Epoch 36/80\n",
      "391/390 [==============================] - ETA: 0s - loss: 1.9769 - accuracy: 0.4702\n",
      "Epoch 00036: val_accuracy did not improve from 0.46780\n",
      "391/390 [==============================] - 95s 243ms/step - loss: 1.9769 - accuracy: 0.4702 - val_loss: 2.1370 - val_accuracy: 0.4605\n",
      "Epoch 37/80\n",
      "391/390 [==============================] - ETA: 0s - loss: 1.9572 - accuracy: 0.4739\n",
      "Epoch 00037: val_accuracy did not improve from 0.46780\n",
      "391/390 [==============================] - 95s 243ms/step - loss: 1.9572 - accuracy: 0.4739 - val_loss: 2.1428 - val_accuracy: 0.4611\n",
      "Epoch 38/80\n",
      "391/390 [==============================] - ETA: 0s - loss: 1.9288 - accuracy: 0.4791Restoring model weights from the end of the best epoch.\n",
      "\n",
      "Epoch 00038: val_accuracy did not improve from 0.46780\n",
      "391/390 [==============================] - 95s 243ms/step - loss: 1.9288 - accuracy: 0.4791 - val_loss: 2.1279 - val_accuracy: 0.4640\n",
      "Epoch 00038: early stopping\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.keras.callbacks.History at 0x7f5e5230f3c8>"
      ]
     },
     "execution_count": 12,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.fit_generator(aug_data.flow(X_train,y_train,batch_size=128),steps_per_epoch=len(X_train)/128,epochs=80,validation_data=(X_test,y_test),verbose=1,callbacks=[call,checkpoint])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "id": "L-IAcVldnIyU"
   },
   "outputs": [],
   "source": [
    "from sklearn.metrics import recall_score,precision_score,classification_report,accuracy_score\n",
    "y_true = y_test.argmax(-1)\n",
    "y_pred = model.predict(X_test).argmax(-1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "id": "CnR1eIwXDRdi",
    "outputId": "a42e1280-8f64-44a4-f33a-2c2c465ded7b"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.62      0.79      0.70       100\n",
      "           1       0.64      0.52      0.57       100\n",
      "           2       0.34      0.32      0.33       100\n",
      "           3       0.33      0.20      0.25       100\n",
      "           4       0.32      0.21      0.25       100\n",
      "           5       0.40      0.48      0.44       100\n",
      "           6       0.66      0.45      0.54       100\n",
      "           7       0.65      0.37      0.47       100\n",
      "           8       0.44      0.69      0.54       100\n",
      "           9       0.37      0.65      0.47       100\n",
      "          10       0.33      0.32      0.32       100\n",
      "          11       0.35      0.33      0.34       100\n",
      "          12       0.48      0.50      0.49       100\n",
      "          13       0.44      0.32      0.37       100\n",
      "          14       0.46      0.46      0.46       100\n",
      "          15       0.44      0.33      0.38       100\n",
      "          16       0.40      0.54      0.46       100\n",
      "          17       0.57      0.64      0.60       100\n",
      "          18       0.51      0.37      0.43       100\n",
      "          19       0.47      0.30      0.37       100\n",
      "          20       0.72      0.76      0.74       100\n",
      "          21       0.52      0.68      0.59       100\n",
      "          22       0.45      0.37      0.41       100\n",
      "          23       0.76      0.60      0.67       100\n",
      "          24       0.72      0.59      0.65       100\n",
      "          25       0.30      0.20      0.24       100\n",
      "          26       0.43      0.39      0.41       100\n",
      "          27       0.32      0.27      0.29       100\n",
      "          28       0.58      0.73      0.65       100\n",
      "          29       0.55      0.41      0.47       100\n",
      "          30       0.43      0.42      0.42       100\n",
      "          31       0.45      0.40      0.42       100\n",
      "          32       0.53      0.41      0.46       100\n",
      "          33       0.37      0.48      0.42       100\n",
      "          34       0.28      0.28      0.28       100\n",
      "          35       0.25      0.23      0.24       100\n",
      "          36       0.58      0.35      0.44       100\n",
      "          37       0.37      0.53      0.44       100\n",
      "          38       0.27      0.23      0.25       100\n",
      "          39       0.49      0.61      0.54       100\n",
      "          40       0.45      0.44      0.44       100\n",
      "          41       0.68      0.79      0.73       100\n",
      "          42       0.28      0.48      0.35       100\n",
      "          43       0.47      0.42      0.44       100\n",
      "          44       0.24      0.14      0.18       100\n",
      "          45       0.40      0.31      0.35       100\n",
      "          46       0.39      0.26      0.31       100\n",
      "          47       0.52      0.61      0.56       100\n",
      "          48       0.50      0.91      0.65       100\n",
      "          49       0.58      0.62      0.60       100\n",
      "          50       0.23      0.21      0.22       100\n",
      "          51       0.43      0.39      0.41       100\n",
      "          52       0.52      0.74      0.61       100\n",
      "          53       0.68      0.76      0.72       100\n",
      "          54       0.69      0.45      0.55       100\n",
      "          55       0.16      0.07      0.10       100\n",
      "          56       0.69      0.61      0.65       100\n",
      "          57       0.52      0.50      0.51       100\n",
      "          58       0.60      0.62      0.61       100\n",
      "          59       0.57      0.31      0.40       100\n",
      "          60       0.82      0.77      0.79       100\n",
      "          61       0.50      0.72      0.59       100\n",
      "          62       0.60      0.50      0.55       100\n",
      "          63       0.59      0.35      0.44       100\n",
      "          64       0.24      0.30      0.27       100\n",
      "          65       0.35      0.27      0.30       100\n",
      "          66       0.38      0.56      0.46       100\n",
      "          67       0.34      0.40      0.37       100\n",
      "          68       0.90      0.69      0.78       100\n",
      "          69       0.58      0.60      0.59       100\n",
      "          70       0.75      0.36      0.49       100\n",
      "          71       0.63      0.63      0.63       100\n",
      "          72       0.33      0.12      0.18       100\n",
      "          73       0.41      0.35      0.38       100\n",
      "          74       0.23      0.36      0.28       100\n",
      "          75       0.76      0.68      0.72       100\n",
      "          76       0.59      0.81      0.68       100\n",
      "          77       0.32      0.27      0.29       100\n",
      "          78       0.17      0.33      0.22       100\n",
      "          79       0.40      0.48      0.43       100\n",
      "          80       0.37      0.10      0.16       100\n",
      "          81       0.42      0.66      0.52       100\n",
      "          82       0.85      0.70      0.77       100\n",
      "          83       0.57      0.30      0.39       100\n",
      "          84       0.43      0.36      0.39       100\n",
      "          85       0.73      0.54      0.62       100\n",
      "          86       0.42      0.64      0.50       100\n",
      "          87       0.32      0.71      0.44       100\n",
      "          88       0.42      0.48      0.45       100\n",
      "          89       0.49      0.58      0.53       100\n",
      "          90       0.42      0.59      0.49       100\n",
      "          91       0.66      0.58      0.62       100\n",
      "          92       0.34      0.49      0.40       100\n",
      "          93       0.45      0.17      0.25       100\n",
      "          94       0.67      0.88      0.76       100\n",
      "          95       0.50      0.55      0.53       100\n",
      "          96       0.40      0.22      0.28       100\n",
      "          97       0.43      0.41      0.42       100\n",
      "          98       0.35      0.31      0.33       100\n",
      "          99       0.53      0.59      0.56       100\n",
      "\n",
      "    accuracy                           0.47     10000\n",
      "   macro avg       0.48      0.47      0.46     10000\n",
      "weighted avg       0.48      0.47      0.46     10000\n",
      "\n",
      "0.4678\n",
      "0.4783564577467867\n",
      "0.4678\n"
     ]
    }
   ],
   "source": [
    "print(classification_report(Y_test,y_pred))\n",
    "print(accuracy_score(Y_test,y_pred))\n",
    "print(precision_score(Y_test,y_pred,average=\"weighted\"))\n",
    "print(recall_score(Y_test,y_pred,average=\"weighted\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "id": "kmwU8vMRpalj",
    "outputId": "61deb0f5-828f-4fba-9c54-956c5c0d0319"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"functional_3\"\n",
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "input_2 (InputLayer)            [(None, 32, 32, 3)]  0                                            \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_19 (Conv2D)              (None, 32, 32, 32)   896         input_2[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "dropout_17 (Dropout)            (None, 32, 32, 32)   0           conv2d_19[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_20 (Conv2D)              (None, 32, 32, 32)   9248        dropout_17[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "dropout_18 (Dropout)            (None, 32, 32, 32)   0           conv2d_20[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_21 (Conv2D)              (None, 32, 32, 32)   9248        dropout_18[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "dropout_19 (Dropout)            (None, 32, 32, 32)   0           conv2d_21[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_22 (Conv2D)              (None, 32, 32, 32)   9248        dropout_19[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "dropout_20 (Dropout)            (None, 32, 32, 32)   0           conv2d_22[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "add_7 (Add)                     (None, 32, 32, 32)   0           dropout_20[0][0]                 \n",
      "                                                                 dropout_18[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "activation_7 (Activation)       (None, 32, 32, 32)   0           add_7[0][0]                      \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_24 (Conv2D)              (None, 32, 32, 64)   18496       activation_7[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "dropout_21 (Dropout)            (None, 32, 32, 64)   0           conv2d_24[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_25 (Conv2D)              (None, 32, 32, 64)   36928       dropout_21[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_23 (Conv2D)              (None, 32, 32, 64)   18496       activation_7[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "dropout_22 (Dropout)            (None, 32, 32, 64)   0           conv2d_25[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "add_8 (Add)                     (None, 32, 32, 64)   0           conv2d_23[0][0]                  \n",
      "                                                                 dropout_22[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "activation_8 (Activation)       (None, 32, 32, 64)   0           add_8[0][0]                      \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_26 (Conv2D)              (None, 32, 32, 64)   36928       activation_8[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "dropout_23 (Dropout)            (None, 32, 32, 64)   0           conv2d_26[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_27 (Conv2D)              (None, 32, 32, 64)   36928       dropout_23[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "dropout_24 (Dropout)            (None, 32, 32, 64)   0           conv2d_27[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "add_9 (Add)                     (None, 32, 32, 64)   0           dropout_24[0][0]                 \n",
      "                                                                 activation_8[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "activation_9 (Activation)       (None, 32, 32, 64)   0           add_9[0][0]                      \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_29 (Conv2D)              (None, 32, 32, 128)  73856       activation_9[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "dropout_25 (Dropout)            (None, 32, 32, 128)  0           conv2d_29[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_30 (Conv2D)              (None, 32, 32, 128)  147584      dropout_25[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_28 (Conv2D)              (None, 32, 32, 128)  73856       activation_9[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "dropout_26 (Dropout)            (None, 32, 32, 128)  0           conv2d_30[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "add_10 (Add)                    (None, 32, 32, 128)  0           conv2d_28[0][0]                  \n",
      "                                                                 dropout_26[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "activation_10 (Activation)      (None, 32, 32, 128)  0           add_10[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_31 (Conv2D)              (None, 32, 32, 128)  147584      activation_10[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "dropout_27 (Dropout)            (None, 32, 32, 128)  0           conv2d_31[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_32 (Conv2D)              (None, 32, 32, 128)  147584      dropout_27[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "dropout_28 (Dropout)            (None, 32, 32, 128)  0           conv2d_32[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "add_11 (Add)                    (None, 32, 32, 128)  0           dropout_28[0][0]                 \n",
      "                                                                 activation_10[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "activation_11 (Activation)      (None, 32, 32, 128)  0           add_11[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_34 (Conv2D)              (None, 32, 32, 256)  295168      activation_11[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "dropout_29 (Dropout)            (None, 32, 32, 256)  0           conv2d_34[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_35 (Conv2D)              (None, 32, 32, 256)  590080      dropout_29[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_33 (Conv2D)              (None, 32, 32, 256)  295168      activation_11[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "dropout_30 (Dropout)            (None, 32, 32, 256)  0           conv2d_35[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "add_12 (Add)                    (None, 32, 32, 256)  0           conv2d_33[0][0]                  \n",
      "                                                                 dropout_30[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "activation_12 (Activation)      (None, 32, 32, 256)  0           add_12[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_36 (Conv2D)              (None, 32, 32, 256)  590080      activation_12[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "dropout_31 (Dropout)            (None, 32, 32, 256)  0           conv2d_36[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_37 (Conv2D)              (None, 32, 32, 256)  590080      dropout_31[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "dropout_32 (Dropout)            (None, 32, 32, 256)  0           conv2d_37[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "add_13 (Add)                    (None, 32, 32, 256)  0           dropout_32[0][0]                 \n",
      "                                                                 activation_12[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "activation_13 (Activation)      (None, 32, 32, 256)  0           add_13[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "average_pooling2d_1 (AveragePoo (None, 32, 32, 256)  0           activation_13[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "flatten_1 (Flatten)             (None, 262144)       0           average_pooling2d_1[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "dense_2 (Dense)                 (None, 512)          134218240   flatten_1[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "dropout_33 (Dropout)            (None, 512)          0           dense_2[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "dense_3 (Dense)                 (None, 100)          51300       dropout_33[0][0]                 \n",
      "==================================================================================================\n",
      "Total params: 137,396,996\n",
      "Trainable params: 137,396,996\n",
      "Non-trainable params: 0\n",
      "__________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From C:\\Users\\uamit\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\ops\\init_ops.py:1251: calling VarianceScaling.__init__ (from tensorflow.python.ops.init_ops) with dtype is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Call initializer instance with the dtype argument instead of passing it to the constructor\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.62      0.79      0.70       100\n",
      "           1       0.64      0.52      0.57       100\n",
      "           2       0.34      0.32      0.33       100\n",
      "           3       0.33      0.20      0.25       100\n",
      "           4       0.32      0.21      0.25       100\n",
      "           5       0.40      0.48      0.44       100\n",
      "           6       0.66      0.45      0.54       100\n",
      "           7       0.65      0.37      0.47       100\n",
      "           8       0.44      0.69      0.54       100\n",
      "           9       0.37      0.65      0.47       100\n",
      "          10       0.33      0.32      0.32       100\n",
      "          11       0.35      0.33      0.34       100\n",
      "          12       0.48      0.50      0.49       100\n",
      "          13       0.44      0.32      0.37       100\n",
      "          14       0.46      0.46      0.46       100\n",
      "          15       0.44      0.33      0.38       100\n",
      "          16       0.40      0.54      0.46       100\n",
      "          17       0.57      0.64      0.60       100\n",
      "          18       0.51      0.37      0.43       100\n",
      "          19       0.47      0.30      0.37       100\n",
      "          20       0.72      0.76      0.74       100\n",
      "          21       0.52      0.68      0.59       100\n",
      "          22       0.45      0.37      0.41       100\n",
      "          23       0.76      0.60      0.67       100\n",
      "          24       0.72      0.59      0.65       100\n",
      "          25       0.30      0.20      0.24       100\n",
      "          26       0.43      0.39      0.41       100\n",
      "          27       0.32      0.27      0.29       100\n",
      "          28       0.58      0.73      0.65       100\n",
      "          29       0.55      0.41      0.47       100\n",
      "          30       0.43      0.42      0.42       100\n",
      "          31       0.45      0.40      0.42       100\n",
      "          32       0.53      0.41      0.46       100\n",
      "          33       0.37      0.48      0.42       100\n",
      "          34       0.28      0.28      0.28       100\n",
      "          35       0.25      0.23      0.24       100\n",
      "          36       0.58      0.35      0.44       100\n",
      "          37       0.37      0.53      0.44       100\n",
      "          38       0.27      0.23      0.25       100\n",
      "          39       0.49      0.61      0.54       100\n",
      "          40       0.45      0.44      0.44       100\n",
      "          41       0.68      0.79      0.73       100\n",
      "          42       0.28      0.48      0.35       100\n",
      "          43       0.47      0.42      0.44       100\n",
      "          44       0.24      0.14      0.18       100\n",
      "          45       0.40      0.31      0.35       100\n",
      "          46       0.39      0.26      0.31       100\n",
      "          47       0.52      0.61      0.56       100\n",
      "          48       0.50      0.91      0.65       100\n",
      "          49       0.58      0.62      0.60       100\n",
      "          50       0.23      0.21      0.22       100\n",
      "          51       0.43      0.39      0.41       100\n",
      "          52       0.52      0.74      0.61       100\n",
      "          53       0.68      0.76      0.72       100\n",
      "          54       0.69      0.45      0.55       100\n",
      "          55       0.16      0.07      0.10       100\n",
      "          56       0.69      0.61      0.65       100\n",
      "          57       0.52      0.50      0.51       100\n",
      "          58       0.60      0.62      0.61       100\n",
      "          59       0.57      0.31      0.40       100\n",
      "          60       0.82      0.77      0.79       100\n",
      "          61       0.50      0.72      0.59       100\n",
      "          62       0.60      0.50      0.55       100\n",
      "          63       0.59      0.35      0.44       100\n",
      "          64       0.24      0.30      0.27       100\n",
      "          65       0.35      0.27      0.30       100\n",
      "          66       0.38      0.56      0.46       100\n",
      "          67       0.34      0.40      0.37       100\n",
      "          68       0.90      0.69      0.78       100\n",
      "          69       0.58      0.60      0.59       100\n",
      "          70       0.75      0.36      0.49       100\n",
      "          71       0.63      0.63      0.63       100\n",
      "          72       0.33      0.12      0.18       100\n",
      "          73       0.41      0.35      0.38       100\n",
      "          74       0.23      0.36      0.28       100\n",
      "          75       0.76      0.68      0.72       100\n",
      "          76       0.59      0.81      0.68       100\n",
      "          77       0.32      0.27      0.29       100\n",
      "          78       0.17      0.33      0.22       100\n",
      "          79       0.40      0.48      0.43       100\n",
      "          80       0.37      0.10      0.16       100\n",
      "          81       0.42      0.66      0.52       100\n",
      "          82       0.85      0.70      0.77       100\n",
      "          83       0.57      0.30      0.39       100\n",
      "          84       0.43      0.36      0.39       100\n",
      "          85       0.73      0.54      0.62       100\n",
      "          86       0.42      0.64      0.50       100\n",
      "          87       0.32      0.71      0.44       100\n",
      "          88       0.42      0.48      0.45       100\n",
      "          89       0.49      0.58      0.53       100\n",
      "          90       0.42      0.59      0.49       100\n",
      "          91       0.66      0.58      0.62       100\n",
      "          92       0.34      0.49      0.40       100\n",
      "          93       0.45      0.17      0.25       100\n",
      "          94       0.67      0.88      0.76       100\n",
      "          95       0.50      0.55      0.53       100\n",
      "          96       0.40      0.22      0.28       100\n",
      "          97       0.43      0.41      0.42       100\n",
      "          98       0.35      0.31      0.33       100\n",
      "          99       0.53      0.59      0.56       100\n",
      "\n",
      "    accuracy                           0.47     10000\n",
      "   macro avg       0.48      0.47      0.46     10000\n",
      "weighted avg       0.48      0.47      0.46     10000\n",
      "\n",
      "0.4678\n",
      "0.4783564577467867\n",
      "0.4678\n"
     ]
    }
   ],
   "source": [
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras.layers import Conv2D,Dense,MaxPool2D,AveragePooling2D,Flatten,Dropout\n",
    "import tensorflow.keras as keras\n",
    "from tensorflow.keras.datasets import cifar100\n",
    "from tensorflow.keras.utils import to_categorical\n",
    "from tensorflow.keras import layers\n",
    "\n",
    "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
    "from tensorflow.keras.optimizers import SGD,Adam\n",
    "\n",
    "from tensorflow.keras.callbacks import EarlyStopping,ModelCheckpoint\n",
    "\n",
    "(X_train,Y_train),(X_test,Y_test)=cifar100.load_data()\n",
    "\n",
    "X_train,X_test=X_train/255,X_test/255\n",
    "y_train,y_test=to_categorical(Y_train),to_categorical(Y_test)\n",
    "\n",
    "shape=(32,32,3)\n",
    "\n",
    "inputs = keras.Input(shape=shape)\n",
    "\n",
    "a=layers.Conv2D(32,(3,3),strides=1,padding=\"same\",activation=\"relu\")(inputs)  #1\n",
    "a=layers.Dropout(0.2)(a)\n",
    "a=layers.Conv2D(32,(3,3),strides=1,padding=\"same\",activation=\"relu\")(a) \n",
    "a=layers.Dropout(0.2)(a)  #2\n",
    "a_temp=a\n",
    "a=layers.Conv2D(32,(3,3),strides=1,padding=\"same\",activation=\"relu\")(a)#\\3\n",
    "a=layers.Dropout(0.2)(a)\n",
    "a=layers.Conv2D(32,(3,3),strides=1,padding=\"same\")(a)#4\n",
    "a=layers.Dropout(0.2)(a)\n",
    "a=layers.Add()([a,a_temp])\n",
    "a=layers.Activation(\"relu\")(a)\n",
    "\n",
    "b=layers.Conv2D(64,(3,3),strides=1,padding=\"same\",activation=\"relu\")(a) #1\n",
    "                               \n",
    "#a_temp=a\n",
    "##########################################################################\n",
    "a=layers.Conv2D(64,(3,3),strides=1,padding=\"same\",activation=\"relu\")(a) #5\n",
    "a=layers.Dropout(0.2)(a)\n",
    "a=layers.Conv2D(64,(3,3),strides=1,padding=\"same\")(a) #6\n",
    "a=layers.Dropout(0.2)(a)\n",
    "a=layers.Add()([b,a])\n",
    "a=layers.Activation(\"relu\")(a)\n",
    "a_temp=a\n",
    "a=layers.Conv2D(64,(3,3),strides=1,padding=\"same\",activation=\"relu\")(a) #7\n",
    "a=layers.Dropout(0.2)(a)\n",
    "a=layers.Conv2D(64,(3,3),strides=1,padding=\"same\")(a) #8\n",
    "a=layers.Dropout(0.2)(a)\n",
    "a=layers.Add()([a,a_temp])\n",
    "a=layers.Activation(\"relu\")(a)\n",
    "\n",
    "b=layers.Conv2D(128,(3,3),strides=1,padding=\"same\",activation=\"relu\")(a) #2\n",
    "\n",
    "###############################################################################\n",
    "\n",
    "a=layers.Conv2D(128,(3,3),strides=1,padding=\"same\",activation=\"relu\")(a) #9\n",
    "a=layers.Dropout(0.2)(a)\n",
    "a=layers.Conv2D(128,(3,3),strides=1,padding=\"same\")(a) #10\n",
    "a=layers.Dropout(0.2)(a)\n",
    "a=layers.Add()([b,a])\n",
    "a=layers.Activation(\"relu\")(a)\n",
    "a_temp=a\n",
    "a=layers.Conv2D(128,(3,3),strides=1,padding=\"same\",activation=\"relu\")(a) #11\n",
    "a=layers.Dropout(0.2)(a)\n",
    "a=layers.Conv2D(128,(3,3),strides=1,padding=\"same\")(a) #12\n",
    "a=layers.Dropout(0.2)(a)\n",
    "a=layers.Add()([a,a_temp])\n",
    "a=layers.Activation(\"relu\")(a)\n",
    "\n",
    "b=layers.Conv2D(256,(3,3),strides=1,padding=\"same\",activation=\"relu\")(a) #3\n",
    "\n",
    "#################################################################################\n",
    "\n",
    "a=layers.Conv2D(256,(3,3),strides=1,padding=\"same\",activation=\"relu\")(a) #13\n",
    "a=layers.Dropout(0.2)(a)\n",
    "a=layers.Conv2D(256,(3,3),strides=1,padding=\"same\")(a) #14\n",
    "a=layers.Dropout(0.2)(a)\n",
    "a=layers.Add()([b,a])\n",
    "a=layers.Activation(\"relu\")(a)\n",
    "a_temp=a\n",
    "a=layers.Conv2D(256,(3,3),strides=1,padding=\"same\",activation=\"relu\")(a) #15\n",
    "a=layers.Dropout(0.2)(a)\n",
    "a=layers.Conv2D(256,(3,3),strides=1,padding=\"same\")(a) #16\n",
    "a=layers.Dropout(0.2)(a)\n",
    "a=layers.Add()([a,a_temp])\n",
    "a=layers.Activation(\"relu\")(a)\n",
    "############################################################3\n",
    "a=layers.AveragePooling2D(pool_size=(2,2),strides=1,padding=\"same\")(a)\n",
    "\n",
    "a=Flatten()(a)\n",
    "\n",
    "#############################################################\n",
    "\n",
    "a=layers.Dense(512,activation=\"relu\")(a)\n",
    "a=layers.Dropout(0.2)(a)\n",
    "outputs=layers.Dense(y_train.shape[1],activation=\"softmax\")(a)\n",
    "\n",
    "\n",
    "model = keras.Model(inputs, outputs)\n",
    "model.load_weights('../weights/ADAM_WITH_DROPOUT_RESNET .hdf5')\n",
    "\n",
    "from sklearn.metrics import recall_score,precision_score,classification_report,accuracy_score\n",
    "y_true = y_test.argmax(-1)\n",
    "y_pred = model.predict(X_test).argmax(-1)\n",
    "print(classification_report(Y_test,y_pred))\n",
    "print(accuracy_score(Y_test,y_pred))\n",
    "print(precision_score(Y_test,y_pred,average=\"weighted\"))\n",
    "print(recall_score(Y_test,y_pred,average=\"weighted\"))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "name": "ADAM_WITH_DROPOUT_RESNET.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
