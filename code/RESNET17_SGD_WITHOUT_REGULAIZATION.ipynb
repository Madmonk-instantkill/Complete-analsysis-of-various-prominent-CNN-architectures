{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "id": "D13jvNutXNvw"
   },
   "outputs": [],
   "source": [
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras.layers import Conv2D,Dense,MaxPool2D,AveragePooling2D,Flatten\n",
    "import tensorflow.keras as keras\n",
    "from tensorflow.keras.datasets import cifar100\n",
    "from tensorflow.keras.utils import to_categorical\n",
    "from tensorflow.keras import layers\n",
    "\n",
    "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
    "from tensorflow.keras.optimizers import SGD\n",
    "\n",
    "from tensorflow.keras.callbacks import EarlyStopping,ModelCheckpoint\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "id": "jUc5LoTUB-qq"
   },
   "outputs": [],
   "source": [
    "(X_train,Y_train),(X_test,Y_test)=cifar100.load_data()\n",
    "\n",
    "X_train,X_test=X_train/255,X_test/255\n",
    "y_train,y_test=to_categorical(Y_train),to_categorical(Y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "id": "VT-GP-KtCEIW"
   },
   "outputs": [],
   "source": [
    "aug_data=ImageDataGenerator(rotation_range=15,horizontal_flip=True,width_shift_range=0.1,height_shift_range=0.1,zoom_range=0.3)\n",
    "aug_data.fit(X_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "id": "vfrZbxwnCKlq"
   },
   "outputs": [],
   "source": [
    "shape=(32,32,3)\n",
    "\n",
    "inputs = keras.Input(shape=shape)\n",
    "\n",
    "a=layers.Conv2D(32,(3,3),strides=1,padding=\"same\",activation=\"relu\")(inputs)  #1\n",
    "a=layers.Conv2D(32,(3,3),strides=1,padding=\"same\",activation=\"relu\")(a)   #2\n",
    "a_temp=a\n",
    "a=layers.Conv2D(32,(3,3),strides=1,padding=\"same\",activation=\"relu\")(a)#\\3\n",
    "a=layers.Conv2D(32,(3,3),strides=1,padding=\"same\")(a)#4\n",
    "a=layers.Add()([a,a_temp])\n",
    "a=layers.Activation(\"relu\")(a)\n",
    "\n",
    "b=layers.Conv2D(64,(3,3),strides=1,padding=\"same\",activation=\"relu\")(a) #1\n",
    "                               \n",
    "#a_temp=a\n",
    "##########################################################################\n",
    "a=layers.Conv2D(64,(3,3),strides=1,padding=\"same\",activation=\"relu\")(a) #5\n",
    "a=layers.Conv2D(64,(3,3),strides=1,padding=\"same\")(a) #6\n",
    "a=layers.Add()([b,a])\n",
    "a=layers.Activation(\"relu\")(a)\n",
    "a_temp=a\n",
    "a=layers.Conv2D(64,(3,3),strides=1,padding=\"same\",activation=\"relu\")(a) #7\n",
    "a=layers.Conv2D(64,(3,3),strides=1,padding=\"same\")(a) #8\n",
    "a=layers.Add()([a,a_temp])\n",
    "a=layers.Activation(\"relu\")(a)\n",
    "\n",
    "b=layers.Conv2D(128,(3,3),strides=1,padding=\"same\",activation=\"relu\")(a) #2\n",
    "\n",
    "###############################################################################\n",
    "\n",
    "a=layers.Conv2D(128,(3,3),strides=1,padding=\"same\",activation=\"relu\")(a) #9\n",
    "a=layers.Conv2D(128,(3,3),strides=1,padding=\"same\")(a) #10\n",
    "a=layers.Add()([b,a])\n",
    "a=layers.Activation(\"relu\")(a)\n",
    "a_temp=a\n",
    "a=layers.Conv2D(128,(3,3),strides=1,padding=\"same\",activation=\"relu\")(a) #11\n",
    "a=layers.Conv2D(128,(3,3),strides=1,padding=\"same\")(a) #12\n",
    "a=layers.Add()([a,a_temp])\n",
    "a=layers.Activation(\"relu\")(a)\n",
    "\n",
    "b=layers.Conv2D(256,(3,3),strides=1,padding=\"same\",activation=\"relu\")(a) #3\n",
    "\n",
    "#################################################################################\n",
    "\n",
    "a=layers.Conv2D(256,(3,3),strides=1,padding=\"same\",activation=\"relu\")(a) #13\n",
    "a=layers.Conv2D(256,(3,3),strides=1,padding=\"same\")(a) #14\n",
    "a=layers.Add()([b,a])\n",
    "a=layers.Activation(\"relu\")(a)\n",
    "a_temp=a\n",
    "a=layers.Conv2D(256,(3,3),strides=1,padding=\"same\",activation=\"relu\")(a) #15\n",
    "a=layers.Conv2D(256,(3,3),strides=1,padding=\"same\")(a) #16\n",
    "a=layers.Add()([a,a_temp])\n",
    "a=layers.Activation(\"relu\")(a)\n",
    "############################################################3\n",
    "a=layers.AveragePooling2D(pool_size=(2,2),strides=1,padding=\"same\")(a)\n",
    "\n",
    "a=Flatten()(a)\n",
    "\n",
    "#############################################################\n",
    "\n",
    "a=layers.Dense(512,activation=\"relu\")(a)\n",
    "outputs=layers.Dense(y_train.shape[1],activation=\"softmax\")(a)\n",
    "\n",
    "\n",
    "model = keras.Model(inputs, outputs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "id": "TloHSpmCCNbW",
    "outputId": "4d101785-6c55-4e74-b251-c79eb39daa7f"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:`period` argument is deprecated. Please use `save_freq` to specify the frequency in number of batches seen.\n"
     ]
    }
   ],
   "source": [
    "sgd = SGD(lr=0.01, momentum=0.91,clipvalue=0.6)\n",
    "\n",
    "\n",
    "model.compile(loss=\"categorical_crossentropy\",optimizer=sgd,metrics=[\"accuracy\"])\n",
    "call=EarlyStopping(monitor=\"val_loss\",verbose=1,mode=\"auto\",patience=5,restore_best_weights=True)\n",
    "checkpoint=ModelCheckpoint('RESNET_SGD_WITHOUT_REGULAIZATION.hdf5',monitor='val_accuracy',verbose=1,save_best_only=True,save_weights_only=True,model='auto',period=1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "id": "zEJIGgTOCQbq",
    "outputId": "df2f596c-8cdd-40ba-eb9a-ec42cc87103a"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/80\n",
      "  2/390 [..............................] - ETA: 39s - loss: 4.6043 - accuracy: 0.0117WARNING:tensorflow:Callbacks method `on_train_batch_end` is slow compared to the batch time (batch time: 0.0803s vs `on_train_batch_end` time: 0.1227s). Check your callbacks.\n",
      "391/390 [==============================] - ETA: 0s - loss: 4.3422 - accuracy: 0.0426\n",
      "Epoch 00001: val_accuracy improved from -inf to 0.09400, saving model to sgd_non_reg_VGG16_weights.hdf5\n",
      "391/390 [==============================] - 84s 214ms/step - loss: 4.3422 - accuracy: 0.0426 - val_loss: 3.9662 - val_accuracy: 0.0940\n",
      "Epoch 2/80\n",
      "391/390 [==============================] - ETA: 0s - loss: 3.8266 - accuracy: 0.1117\n",
      "Epoch 00002: val_accuracy improved from 0.09400 to 0.16640, saving model to sgd_non_reg_VGG16_weights.hdf5\n",
      "391/390 [==============================] - 83s 213ms/step - loss: 3.8266 - accuracy: 0.1117 - val_loss: 3.5412 - val_accuracy: 0.1664\n",
      "Epoch 3/80\n",
      "391/390 [==============================] - ETA: 0s - loss: 3.3938 - accuracy: 0.1902\n",
      "Epoch 00003: val_accuracy improved from 0.16640 to 0.24970, saving model to sgd_non_reg_VGG16_weights.hdf5\n",
      "391/390 [==============================] - 83s 213ms/step - loss: 3.3938 - accuracy: 0.1902 - val_loss: 3.0437 - val_accuracy: 0.2497\n",
      "Epoch 4/80\n",
      "391/390 [==============================] - ETA: 0s - loss: 3.0449 - accuracy: 0.2500\n",
      "Epoch 00004: val_accuracy improved from 0.24970 to 0.30950, saving model to sgd_non_reg_VGG16_weights.hdf5\n",
      "391/390 [==============================] - 83s 213ms/step - loss: 3.0449 - accuracy: 0.2500 - val_loss: 2.7396 - val_accuracy: 0.3095\n",
      "Epoch 5/80\n",
      "391/390 [==============================] - ETA: 0s - loss: 2.7937 - accuracy: 0.3010\n",
      "Epoch 00005: val_accuracy improved from 0.30950 to 0.36670, saving model to sgd_non_reg_VGG16_weights.hdf5\n",
      "391/390 [==============================] - 83s 213ms/step - loss: 2.7937 - accuracy: 0.3010 - val_loss: 2.4479 - val_accuracy: 0.3667\n",
      "Epoch 6/80\n",
      "391/390 [==============================] - ETA: 0s - loss: 2.5834 - accuracy: 0.3412\n",
      "Epoch 00006: val_accuracy improved from 0.36670 to 0.40530, saving model to sgd_non_reg_VGG16_weights.hdf5\n",
      "391/390 [==============================] - 83s 213ms/step - loss: 2.5834 - accuracy: 0.3412 - val_loss: 2.3135 - val_accuracy: 0.4053\n",
      "Epoch 7/80\n",
      "391/390 [==============================] - ETA: 0s - loss: 2.4292 - accuracy: 0.3739\n",
      "Epoch 00007: val_accuracy improved from 0.40530 to 0.41260, saving model to sgd_non_reg_VGG16_weights.hdf5\n",
      "391/390 [==============================] - 84s 214ms/step - loss: 2.4292 - accuracy: 0.3739 - val_loss: 2.2441 - val_accuracy: 0.4126\n",
      "Epoch 8/80\n",
      "391/390 [==============================] - ETA: 0s - loss: 2.2944 - accuracy: 0.4000\n",
      "Epoch 00008: val_accuracy improved from 0.41260 to 0.43200, saving model to sgd_non_reg_VGG16_weights.hdf5\n",
      "391/390 [==============================] - 84s 214ms/step - loss: 2.2944 - accuracy: 0.4000 - val_loss: 2.1853 - val_accuracy: 0.4320\n",
      "Epoch 9/80\n",
      "391/390 [==============================] - ETA: 0s - loss: 2.1776 - accuracy: 0.4292\n",
      "Epoch 00009: val_accuracy improved from 0.43200 to 0.45460, saving model to sgd_non_reg_VGG16_weights.hdf5\n",
      "391/390 [==============================] - 83s 214ms/step - loss: 2.1776 - accuracy: 0.4292 - val_loss: 2.0946 - val_accuracy: 0.4546\n",
      "Epoch 10/80\n",
      "391/390 [==============================] - ETA: 0s - loss: 2.0824 - accuracy: 0.4473\n",
      "Epoch 00010: val_accuracy improved from 0.45460 to 0.46350, saving model to sgd_non_reg_VGG16_weights.hdf5\n",
      "391/390 [==============================] - 83s 213ms/step - loss: 2.0824 - accuracy: 0.4473 - val_loss: 2.0358 - val_accuracy: 0.4635\n",
      "Epoch 11/80\n",
      "391/390 [==============================] - ETA: 0s - loss: 1.9715 - accuracy: 0.4746\n",
      "Epoch 00011: val_accuracy improved from 0.46350 to 0.47300, saving model to sgd_non_reg_VGG16_weights.hdf5\n",
      "391/390 [==============================] - 83s 213ms/step - loss: 1.9715 - accuracy: 0.4746 - val_loss: 2.0339 - val_accuracy: 0.4730\n",
      "Epoch 12/80\n",
      "391/390 [==============================] - ETA: 0s - loss: 1.8915 - accuracy: 0.4915\n",
      "Epoch 00012: val_accuracy improved from 0.47300 to 0.48740, saving model to sgd_non_reg_VGG16_weights.hdf5\n",
      "391/390 [==============================] - 84s 214ms/step - loss: 1.8915 - accuracy: 0.4915 - val_loss: 1.9685 - val_accuracy: 0.4874\n",
      "Epoch 13/80\n",
      "391/390 [==============================] - ETA: 0s - loss: 1.8129 - accuracy: 0.5089\n",
      "Epoch 00013: val_accuracy improved from 0.48740 to 0.49090, saving model to sgd_non_reg_VGG16_weights.hdf5\n",
      "391/390 [==============================] - 83s 213ms/step - loss: 1.8129 - accuracy: 0.5089 - val_loss: 1.9737 - val_accuracy: 0.4909\n",
      "Epoch 14/80\n",
      "391/390 [==============================] - ETA: 0s - loss: 1.7372 - accuracy: 0.5260\n",
      "Epoch 00014: val_accuracy improved from 0.49090 to 0.52140, saving model to sgd_non_reg_VGG16_weights.hdf5\n",
      "391/390 [==============================] - 84s 214ms/step - loss: 1.7372 - accuracy: 0.5260 - val_loss: 1.8427 - val_accuracy: 0.5214\n",
      "Epoch 15/80\n",
      "391/390 [==============================] - ETA: 0s - loss: 1.6863 - accuracy: 0.5389\n",
      "Epoch 00015: val_accuracy did not improve from 0.52140\n",
      "391/390 [==============================] - 81s 208ms/step - loss: 1.6863 - accuracy: 0.5389 - val_loss: 1.8519 - val_accuracy: 0.5095\n",
      "Epoch 16/80\n",
      "391/390 [==============================] - ETA: 0s - loss: 1.6172 - accuracy: 0.5552\n",
      "Epoch 00016: val_accuracy did not improve from 0.52140\n",
      "391/390 [==============================] - 81s 208ms/step - loss: 1.6172 - accuracy: 0.5552 - val_loss: 1.9084 - val_accuracy: 0.5058\n",
      "Epoch 17/80\n",
      "391/390 [==============================] - ETA: 0s - loss: 1.5430 - accuracy: 0.5727\n",
      "Epoch 00017: val_accuracy improved from 0.52140 to 0.53000, saving model to sgd_non_reg_VGG16_weights.hdf5\n",
      "391/390 [==============================] - 84s 214ms/step - loss: 1.5430 - accuracy: 0.5727 - val_loss: 1.8177 - val_accuracy: 0.5300\n",
      "Epoch 18/80\n",
      "391/390 [==============================] - ETA: 0s - loss: 1.4956 - accuracy: 0.5861\n",
      "Epoch 00018: val_accuracy improved from 0.53000 to 0.54200, saving model to sgd_non_reg_VGG16_weights.hdf5\n",
      "391/390 [==============================] - 84s 214ms/step - loss: 1.4956 - accuracy: 0.5861 - val_loss: 1.7495 - val_accuracy: 0.5420\n",
      "Epoch 19/80\n",
      "391/390 [==============================] - ETA: 0s - loss: 1.4458 - accuracy: 0.5953\n",
      "Epoch 00019: val_accuracy did not improve from 0.54200\n",
      "391/390 [==============================] - 81s 208ms/step - loss: 1.4458 - accuracy: 0.5953 - val_loss: 1.7844 - val_accuracy: 0.5358\n",
      "Epoch 20/80\n",
      "391/390 [==============================] - ETA: 0s - loss: 1.3887 - accuracy: 0.6104\n",
      "Epoch 00020: val_accuracy did not improve from 0.54200\n",
      "391/390 [==============================] - 81s 208ms/step - loss: 1.3887 - accuracy: 0.6104 - val_loss: 1.9121 - val_accuracy: 0.5254\n",
      "Epoch 21/80\n",
      "391/390 [==============================] - ETA: 0s - loss: 1.3440 - accuracy: 0.6243\n",
      "Epoch 00021: val_accuracy improved from 0.54200 to 0.56080, saving model to sgd_non_reg_VGG16_weights.hdf5\n",
      "391/390 [==============================] - 83s 213ms/step - loss: 1.3440 - accuracy: 0.6243 - val_loss: 1.7375 - val_accuracy: 0.5608\n",
      "Epoch 22/80\n",
      "391/390 [==============================] - ETA: 0s - loss: 1.2943 - accuracy: 0.6335\n",
      "Epoch 00022: val_accuracy did not improve from 0.56080\n",
      "391/390 [==============================] - 82s 209ms/step - loss: 1.2943 - accuracy: 0.6335 - val_loss: 1.9014 - val_accuracy: 0.5359\n",
      "Epoch 23/80\n",
      "391/390 [==============================] - ETA: 0s - loss: 1.2566 - accuracy: 0.6419\n",
      "Epoch 00023: val_accuracy did not improve from 0.56080\n",
      "391/390 [==============================] - 82s 209ms/step - loss: 1.2566 - accuracy: 0.6419 - val_loss: 1.7860 - val_accuracy: 0.5518\n",
      "Epoch 24/80\n",
      "391/390 [==============================] - ETA: 0s - loss: 1.1957 - accuracy: 0.6582\n",
      "Epoch 00024: val_accuracy did not improve from 0.56080\n",
      "391/390 [==============================] - 81s 208ms/step - loss: 1.1957 - accuracy: 0.6582 - val_loss: 1.8334 - val_accuracy: 0.5497\n",
      "Epoch 25/80\n",
      "391/390 [==============================] - ETA: 0s - loss: 1.1639 - accuracy: 0.6655\n",
      "Epoch 00025: val_accuracy improved from 0.56080 to 0.56750, saving model to sgd_non_reg_VGG16_weights.hdf5\n",
      "391/390 [==============================] - 84s 214ms/step - loss: 1.1639 - accuracy: 0.6655 - val_loss: 1.7272 - val_accuracy: 0.5675\n",
      "Epoch 26/80\n",
      "391/390 [==============================] - ETA: 0s - loss: 1.1366 - accuracy: 0.6732\n",
      "Epoch 00026: val_accuracy did not improve from 0.56750\n",
      "391/390 [==============================] - 81s 208ms/step - loss: 1.1366 - accuracy: 0.6732 - val_loss: 1.7433 - val_accuracy: 0.5629\n",
      "Epoch 27/80\n",
      "391/390 [==============================] - ETA: 0s - loss: 1.1024 - accuracy: 0.6820\n",
      "Epoch 00027: val_accuracy did not improve from 0.56750\n",
      "391/390 [==============================] - 82s 209ms/step - loss: 1.1024 - accuracy: 0.6820 - val_loss: 1.8367 - val_accuracy: 0.5638\n",
      "Epoch 28/80\n",
      "391/390 [==============================] - ETA: 0s - loss: 1.0714 - accuracy: 0.6888\n",
      "Epoch 00028: val_accuracy did not improve from 0.56750\n",
      "391/390 [==============================] - 82s 208ms/step - loss: 1.0714 - accuracy: 0.6888 - val_loss: 1.9752 - val_accuracy: 0.5481\n",
      "Epoch 29/80\n",
      "391/390 [==============================] - ETA: 0s - loss: 1.0381 - accuracy: 0.6996\n",
      "Epoch 00029: val_accuracy improved from 0.56750 to 0.57030, saving model to sgd_non_reg_VGG16_weights.hdf5\n",
      "391/390 [==============================] - 84s 214ms/step - loss: 1.0381 - accuracy: 0.6996 - val_loss: 1.8338 - val_accuracy: 0.5703\n",
      "Epoch 30/80\n",
      "391/390 [==============================] - ETA: 0s - loss: 0.9974 - accuracy: 0.7097Restoring model weights from the end of the best epoch.\n",
      "\n",
      "Epoch 00030: val_accuracy did not improve from 0.57030\n",
      "391/390 [==============================] - 82s 209ms/step - loss: 0.9974 - accuracy: 0.7097 - val_loss: 1.9304 - val_accuracy: 0.5625\n",
      "Epoch 00030: early stopping\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.keras.callbacks.History at 0x7f96a35bb940>"
      ]
     },
     "execution_count": 30,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.fit_generator(aug_data.flow(X_train,y_train,batch_size=128),steps_per_epoch=len(X_train)/128,epochs=80,validation_data=(X_test,y_test),verbose=1,callbacks=[call,checkpoint])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "id": "gF37F-gyQ-QT"
   },
   "outputs": [],
   "source": [
    "y_true = y_test.argmax(-1)\n",
    "y_pred = model.predict(X_test).argmax(-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "id": "ZAuJUG-1VU4c",
    "outputId": "f2be3b48-2902-4625-ca07-b698581d593d"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.86      0.77      0.81       100\n",
      "           1       0.70      0.73      0.72       100\n",
      "           2       0.53      0.46      0.49       100\n",
      "           3       0.35      0.40      0.38       100\n",
      "           4       0.51      0.31      0.39       100\n",
      "           5       0.60      0.50      0.55       100\n",
      "           6       0.68      0.69      0.69       100\n",
      "           7       0.52      0.64      0.58       100\n",
      "           8       0.57      0.79      0.66       100\n",
      "           9       0.66      0.67      0.67       100\n",
      "          10       0.42      0.37      0.39       100\n",
      "          11       0.40      0.42      0.41       100\n",
      "          12       0.50      0.74      0.59       100\n",
      "          13       0.50      0.63      0.56       100\n",
      "          14       0.38      0.63      0.48       100\n",
      "          15       0.50      0.61      0.55       100\n",
      "          16       0.68      0.64      0.66       100\n",
      "          17       0.64      0.77      0.70       100\n",
      "          18       0.58      0.45      0.51       100\n",
      "          19       0.61      0.51      0.55       100\n",
      "          20       0.68      0.86      0.76       100\n",
      "          21       0.69      0.73      0.71       100\n",
      "          22       0.52      0.57      0.54       100\n",
      "          23       0.79      0.64      0.71       100\n",
      "          24       0.85      0.77      0.81       100\n",
      "          25       0.53      0.34      0.41       100\n",
      "          26       0.32      0.60      0.42       100\n",
      "          27       0.49      0.31      0.38       100\n",
      "          28       0.69      0.73      0.71       100\n",
      "          29       0.65      0.48      0.55       100\n",
      "          30       0.48      0.63      0.55       100\n",
      "          31       0.79      0.45      0.57       100\n",
      "          32       0.44      0.52      0.48       100\n",
      "          33       0.66      0.42      0.51       100\n",
      "          34       0.55      0.57      0.56       100\n",
      "          35       0.31      0.32      0.32       100\n",
      "          36       0.87      0.52      0.65       100\n",
      "          37       0.49      0.69      0.57       100\n",
      "          38       0.45      0.37      0.41       100\n",
      "          39       0.69      0.66      0.67       100\n",
      "          40       0.43      0.54      0.48       100\n",
      "          41       0.86      0.81      0.84       100\n",
      "          42       0.31      0.66      0.42       100\n",
      "          43       0.76      0.57      0.65       100\n",
      "          44       0.26      0.31      0.28       100\n",
      "          45       0.31      0.55      0.39       100\n",
      "          46       0.46      0.38      0.42       100\n",
      "          47       0.55      0.60      0.57       100\n",
      "          48       0.71      0.90      0.79       100\n",
      "          49       0.64      0.76      0.70       100\n",
      "          50       0.52      0.23      0.32       100\n",
      "          51       0.52      0.55      0.53       100\n",
      "          52       0.52      0.67      0.59       100\n",
      "          53       0.73      0.77      0.75       100\n",
      "          54       0.62      0.70      0.66       100\n",
      "          55       0.45      0.09      0.15       100\n",
      "          56       0.73      0.77      0.75       100\n",
      "          57       0.70      0.55      0.61       100\n",
      "          58       0.80      0.63      0.70       100\n",
      "          59       0.57      0.46      0.51       100\n",
      "          60       0.78      0.80      0.79       100\n",
      "          61       0.59      0.72      0.65       100\n",
      "          62       0.65      0.65      0.65       100\n",
      "          63       0.49      0.48      0.48       100\n",
      "          64       0.51      0.37      0.43       100\n",
      "          65       0.65      0.31      0.42       100\n",
      "          66       0.66      0.41      0.51       100\n",
      "          67       0.48      0.31      0.38       100\n",
      "          68       0.91      0.82      0.86       100\n",
      "          69       0.72      0.72      0.72       100\n",
      "          70       0.71      0.55      0.62       100\n",
      "          71       0.87      0.41      0.56       100\n",
      "          72       0.28      0.28      0.28       100\n",
      "          73       0.51      0.27      0.35       100\n",
      "          74       0.32      0.35      0.33       100\n",
      "          75       0.66      0.85      0.75       100\n",
      "          76       0.88      0.71      0.78       100\n",
      "          77       0.47      0.48      0.47       100\n",
      "          78       0.37      0.43      0.40       100\n",
      "          79       0.59      0.64      0.62       100\n",
      "          80       0.66      0.31      0.42       100\n",
      "          81       0.44      0.70      0.54       100\n",
      "          82       0.78      0.80      0.79       100\n",
      "          83       0.50      0.64      0.56       100\n",
      "          84       0.53      0.62      0.57       100\n",
      "          85       0.78      0.69      0.73       100\n",
      "          86       0.49      0.67      0.57       100\n",
      "          87       0.65      0.68      0.66       100\n",
      "          88       0.58      0.75      0.66       100\n",
      "          89       0.65      0.60      0.62       100\n",
      "          90       0.59      0.60      0.60       100\n",
      "          91       0.73      0.58      0.65       100\n",
      "          92       0.61      0.43      0.51       100\n",
      "          93       0.33      0.34      0.34       100\n",
      "          94       0.87      0.84      0.85       100\n",
      "          95       0.59      0.58      0.59       100\n",
      "          96       0.51      0.39      0.44       100\n",
      "          97       0.56      0.63      0.59       100\n",
      "          98       0.40      0.33      0.36       100\n",
      "          99       0.59      0.60      0.59       100\n",
      "\n",
      "    accuracy                           0.57     10000\n",
      "   macro avg       0.59      0.57      0.56     10000\n",
      "weighted avg       0.59      0.57      0.56     10000\n",
      "\n",
      "0.5675\n",
      "0.5854920681164838\n",
      "0.5675\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import classification_report,accuracy_score,recall_score,precision_score\n",
    "print(classification_report(Y_test,y_pred))\n",
    "print(accuracy_score(Y_test,y_pred))\n",
    "print(precision_score(Y_test,y_pred,average=\"weighted\"))\n",
    "print(recall_score(Y_test,y_pred,average=\"weighted\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.76      0.79      0.77       100\n",
      "           1       0.82      0.63      0.71       100\n",
      "           2       0.46      0.40      0.43       100\n",
      "           3       0.53      0.16      0.25       100\n",
      "           4       0.51      0.18      0.27       100\n",
      "           5       0.52      0.57      0.55       100\n",
      "           6       0.55      0.73      0.63       100\n",
      "           7       0.61      0.57      0.59       100\n",
      "           8       0.57      0.80      0.67       100\n",
      "           9       0.84      0.61      0.71       100\n",
      "          10       0.46      0.31      0.37       100\n",
      "          11       0.46      0.32      0.38       100\n",
      "          12       0.62      0.66      0.64       100\n",
      "          13       0.49      0.62      0.55       100\n",
      "          14       0.53      0.59      0.56       100\n",
      "          15       0.47      0.60      0.53       100\n",
      "          16       0.49      0.76      0.60       100\n",
      "          17       0.75      0.73      0.74       100\n",
      "          18       0.47      0.40      0.43       100\n",
      "          19       0.66      0.40      0.50       100\n",
      "          20       0.82      0.78      0.80       100\n",
      "          21       0.77      0.75      0.76       100\n",
      "          22       0.51      0.62      0.56       100\n",
      "          23       0.77      0.60      0.67       100\n",
      "          24       0.94      0.65      0.77       100\n",
      "          25       0.47      0.45      0.46       100\n",
      "          26       0.27      0.61      0.37       100\n",
      "          27       0.41      0.47      0.44       100\n",
      "          28       0.72      0.76      0.74       100\n",
      "          29       0.59      0.46      0.52       100\n",
      "          30       0.58      0.56      0.57       100\n",
      "          31       0.63      0.47      0.54       100\n",
      "          32       0.69      0.47      0.56       100\n",
      "          33       0.59      0.48      0.53       100\n",
      "          34       0.47      0.64      0.54       100\n",
      "          35       0.44      0.31      0.36       100\n",
      "          36       0.85      0.55      0.67       100\n",
      "          37       0.55      0.61      0.58       100\n",
      "          38       0.68      0.34      0.45       100\n",
      "          39       0.70      0.72      0.71       100\n",
      "          40       0.64      0.41      0.50       100\n",
      "          41       0.79      0.78      0.78       100\n",
      "          42       0.37      0.72      0.49       100\n",
      "          43       0.55      0.59      0.57       100\n",
      "          44       0.28      0.26      0.27       100\n",
      "          45       0.41      0.47      0.44       100\n",
      "          46       0.36      0.50      0.42       100\n",
      "          47       0.62      0.56      0.59       100\n",
      "          48       0.54      0.94      0.69       100\n",
      "          49       0.76      0.57      0.65       100\n",
      "          50       0.32      0.43      0.37       100\n",
      "          51       0.46      0.67      0.54       100\n",
      "          52       0.52      0.69      0.59       100\n",
      "          53       0.65      0.90      0.75       100\n",
      "          54       0.67      0.72      0.70       100\n",
      "          55       0.28      0.15      0.19       100\n",
      "          56       0.83      0.71      0.76       100\n",
      "          57       0.65      0.58      0.61       100\n",
      "          58       0.66      0.80      0.72       100\n",
      "          59       0.60      0.54      0.57       100\n",
      "          60       0.81      0.77      0.79       100\n",
      "          61       0.55      0.76      0.64       100\n",
      "          62       0.72      0.57      0.64       100\n",
      "          63       0.68      0.47      0.56       100\n",
      "          64       0.43      0.35      0.38       100\n",
      "          65       0.55      0.40      0.46       100\n",
      "          66       0.64      0.58      0.61       100\n",
      "          67       0.62      0.45      0.52       100\n",
      "          68       0.90      0.77      0.83       100\n",
      "          69       0.78      0.70      0.74       100\n",
      "          70       0.71      0.60      0.65       100\n",
      "          71       0.76      0.55      0.64       100\n",
      "          72       0.28      0.19      0.23       100\n",
      "          73       0.57      0.37      0.45       100\n",
      "          74       0.37      0.40      0.39       100\n",
      "          75       0.81      0.79      0.80       100\n",
      "          76       0.89      0.75      0.82       100\n",
      "          77       0.33      0.50      0.40       100\n",
      "          78       0.24      0.57      0.34       100\n",
      "          79       0.62      0.60      0.61       100\n",
      "          80       0.58      0.26      0.36       100\n",
      "          81       0.52      0.71      0.60       100\n",
      "          82       0.92      0.77      0.84       100\n",
      "          83       0.61      0.51      0.56       100\n",
      "          84       0.60      0.54      0.57       100\n",
      "          85       0.64      0.70      0.67       100\n",
      "          86       0.64      0.58      0.61       100\n",
      "          87       0.56      0.68      0.61       100\n",
      "          88       0.53      0.76      0.62       100\n",
      "          89       0.73      0.66      0.69       100\n",
      "          90       0.66      0.59      0.62       100\n",
      "          91       0.74      0.69      0.72       100\n",
      "          92       0.61      0.59      0.60       100\n",
      "          93       0.32      0.35      0.34       100\n",
      "          94       0.87      0.78      0.82       100\n",
      "          95       0.59      0.58      0.59       100\n",
      "          96       0.49      0.45      0.47       100\n",
      "          97       0.79      0.52      0.63       100\n",
      "          98       0.43      0.43      0.43       100\n",
      "          99       0.41      0.62      0.49       100\n",
      "\n",
      "    accuracy                           0.57     10000\n",
      "   macro avg       0.59      0.57      0.57     10000\n",
      "weighted avg       0.59      0.57      0.57     10000\n",
      "\n",
      "the accuracy obtained is 0.5703\n",
      "precision is 0.5945996353811593\n",
      "the recall is 0.5703\n"
     ]
    }
   ],
   "source": [
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras.layers import Conv2D,Dense,MaxPool2D,AveragePooling2D,Flatten\n",
    "import tensorflow.keras as keras\n",
    "from tensorflow.keras.datasets import cifar100\n",
    "from tensorflow.keras.utils import to_categorical\n",
    "from tensorflow.keras import layers\n",
    "\n",
    "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
    "from tensorflow.keras.optimizers import SGD\n",
    "\n",
    "from tensorflow.keras.callbacks import EarlyStopping,ModelCheckpoint\n",
    "\n",
    "(X_train,Y_train),(X_test,Y_test)=cifar100.load_data()\n",
    "\n",
    "X_train,X_test=X_train/255,X_test/255\n",
    "y_train,y_test=to_categorical(Y_train),to_categorical(Y_test)\n",
    "\n",
    "shape=(32,32,3)\n",
    "\n",
    "inputs = keras.Input(shape=shape)\n",
    "\n",
    "a=layers.Conv2D(32,(3,3),strides=1,padding=\"same\",activation=\"relu\")(inputs)  #1\n",
    "a=layers.Conv2D(32,(3,3),strides=1,padding=\"same\",activation=\"relu\")(a)   #2\n",
    "a_temp=a\n",
    "a=layers.Conv2D(32,(3,3),strides=1,padding=\"same\",activation=\"relu\")(a)#\\3\n",
    "a=layers.Conv2D(32,(3,3),strides=1,padding=\"same\")(a)#4\n",
    "a=layers.Add()([a,a_temp])\n",
    "a=layers.Activation(\"relu\")(a)\n",
    "\n",
    "b=layers.Conv2D(64,(3,3),strides=1,padding=\"same\",activation=\"relu\")(a) #1\n",
    "                               \n",
    "#a_temp=a\n",
    "##########################################################################\n",
    "a=layers.Conv2D(64,(3,3),strides=1,padding=\"same\",activation=\"relu\")(a) #5\n",
    "a=layers.Conv2D(64,(3,3),strides=1,padding=\"same\")(a) #6\n",
    "a=layers.Add()([b,a])\n",
    "a=layers.Activation(\"relu\")(a)\n",
    "a_temp=a\n",
    "a=layers.Conv2D(64,(3,3),strides=1,padding=\"same\",activation=\"relu\")(a) #7\n",
    "a=layers.Conv2D(64,(3,3),strides=1,padding=\"same\")(a) #8\n",
    "a=layers.Add()([a,a_temp])\n",
    "a=layers.Activation(\"relu\")(a)\n",
    "\n",
    "b=layers.Conv2D(128,(3,3),strides=1,padding=\"same\",activation=\"relu\")(a) #2\n",
    "\n",
    "###############################################################################\n",
    "\n",
    "a=layers.Conv2D(128,(3,3),strides=1,padding=\"same\",activation=\"relu\")(a) #9\n",
    "a=layers.Conv2D(128,(3,3),strides=1,padding=\"same\")(a) #10\n",
    "a=layers.Add()([b,a])\n",
    "a=layers.Activation(\"relu\")(a)\n",
    "a_temp=a\n",
    "a=layers.Conv2D(128,(3,3),strides=1,padding=\"same\",activation=\"relu\")(a) #11\n",
    "a=layers.Conv2D(128,(3,3),strides=1,padding=\"same\")(a) #12\n",
    "a=layers.Add()([a,a_temp])\n",
    "a=layers.Activation(\"relu\")(a)\n",
    "\n",
    "b=layers.Conv2D(256,(3,3),strides=1,padding=\"same\",activation=\"relu\")(a) #3\n",
    "\n",
    "#################################################################################\n",
    "\n",
    "a=layers.Conv2D(256,(3,3),strides=1,padding=\"same\",activation=\"relu\")(a) #13\n",
    "a=layers.Conv2D(256,(3,3),strides=1,padding=\"same\")(a) #14\n",
    "a=layers.Add()([b,a])\n",
    "a=layers.Activation(\"relu\")(a)\n",
    "a_temp=a\n",
    "a=layers.Conv2D(256,(3,3),strides=1,padding=\"same\",activation=\"relu\")(a) #15\n",
    "a=layers.Conv2D(256,(3,3),strides=1,padding=\"same\")(a) #16\n",
    "a=layers.Add()([a,a_temp])\n",
    "a=layers.Activation(\"relu\")(a)\n",
    "############################################################3\n",
    "a=layers.AveragePooling2D(pool_size=(2,2),strides=1,padding=\"same\")(a)\n",
    "\n",
    "a=Flatten()(a)\n",
    "\n",
    "#############################################################\n",
    "\n",
    "a=layers.Dense(512,activation=\"relu\")(a)\n",
    "outputs=layers.Dense(y_train.shape[1],activation=\"softmax\")(a)\n",
    "\n",
    "\n",
    "model = keras.Model(inputs, outputs)\n",
    "\n",
    "model.load_weights('../weights/RESNET_SGD_WITHOUT_REGULAIZATION.hdf5')\n",
    "\n",
    "\n",
    "y_true = y_test.argmax(-1)\n",
    "y_pred = model.predict(X_test).argmax(-1)\n",
    "\n",
    "from sklearn.metrics import classification_report,accuracy_score,recall_score,precision_score\n",
    "print(classification_report(Y_test,y_pred))\n",
    "print(\"the accuracy obtained is {}\".format(accuracy_score(Y_test,y_pred)))\n",
    "print(\"precision is {}\".format(precision_score(Y_test,y_pred,average=\"weighted\")))\n",
    "print(\"the recall is {}\".format(recall_score(Y_test,y_pred,average=\"weighted\")))"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [],
   "name": "RESNET_SGD_WITHOUT_REGULAIZATION.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
