{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 51
    },
    "id": "uNCsHYl1s66N",
    "outputId": "74f7b556-9177-4fe1-9748-d0fa92c6d746"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading data from https://www.cs.toronto.edu/~kriz/cifar-100-python.tar.gz\n",
      "169009152/169001437 [==============================] - 4s 0us/step\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from tensorflow.keras.applications.vgg16 import VGG16\n",
    "from tensorflow.keras.models import Sequential,Model\n",
    "from tensorflow.keras.layers import Dense,Flatten,Input,Conv2D,MaxPool2D\n",
    "\n",
    "from tensorflow.keras.layers import LeakyReLU,BatchNormalization\n",
    "\n",
    "from tensorflow.keras.datasets import cifar100\n",
    "from tensorflow.keras.utils import to_categorical\n",
    "\n",
    "from tensorflow.keras.callbacks import EarlyStopping,ModelCheckpoint\n",
    "\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "\n",
    "from tensorflow.keras.optimizers import RMSprop,SGD\n",
    "\n",
    "(X_train,Y_train),(X_test,Y_test)=cifar100.load_data()\n",
    "y_train,y_test=to_categorical(Y_train),to_categorical(Y_test)\n",
    "\n",
    "X_train,X_test=X_train/255,X_test/255"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "id": "1fM3njvstNNw",
    "outputId": "aa809fc6-9272-4895-eabc-2e8fdda77209"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:`period` argument is deprecated. Please use `save_freq` to specify the frequency in number of batches seen.\n"
     ]
    }
   ],
   "source": [
    "model=Sequential()\n",
    "\n",
    "model.add(Conv2D(input_shape=(32,32,3),filters=64,kernel_size=(3,3),padding=\"same\",strides=1))\n",
    "model.add(BatchNormalization())\n",
    "model.add(LeakyReLU(alpha=0.1))\n",
    "\n",
    "model.add(Conv2D(filters=64,kernel_size=(3,3),padding=\"same\"))\n",
    "model.add(BatchNormalization())\n",
    "model.add(LeakyReLU(alpha=0.1))\n",
    "\n",
    "model.add(MaxPool2D(pool_size=(2,2),strides=(1,1)))\n",
    "\n",
    "model.add(Conv2D(filters=128, kernel_size=(3,3), padding=\"same\",strides=1))\n",
    "model.add(BatchNormalization())\n",
    "model.add(LeakyReLU(alpha=0.1))\n",
    "\n",
    "model.add(Conv2D(filters=128, kernel_size=(3,3), padding=\"same\",strides=1))\n",
    "model.add(BatchNormalization())\n",
    "model.add(LeakyReLU(alpha=0.1))\n",
    "\n",
    "model.add(MaxPool2D(pool_size=(2,2),strides=(1,1)))\n",
    "\n",
    "model.add(Conv2D(filters=256, kernel_size=(3,3), padding=\"same\",strides=1))\n",
    "model.add(BatchNormalization())\n",
    "model.add(LeakyReLU(alpha=0.1))\n",
    "\n",
    "model.add(Conv2D(filters=256, kernel_size=(3,3), padding=\"same\",strides=1))\n",
    "model.add(BatchNormalization())\n",
    "model.add(LeakyReLU(alpha=0.1))\n",
    "\n",
    "model.add(Conv2D(filters=256, kernel_size=(3,3), padding=\"same\",strides=1))\n",
    "model.add(BatchNormalization())\n",
    "model.add(LeakyReLU(alpha=0.1))\n",
    "\n",
    "model.add(MaxPool2D(pool_size=(2,2),strides=(1,1)))\n",
    "\n",
    "model.add(Conv2D(filters=512, kernel_size=(3,3), padding=\"same\",strides=1))\n",
    "model.add(BatchNormalization())\n",
    "model.add(LeakyReLU(alpha=0.1))\n",
    "\n",
    "model.add(Conv2D(filters=512, kernel_size=(3,3), padding=\"same\",strides=1))\n",
    "model.add(BatchNormalization())\n",
    "model.add(LeakyReLU(alpha=0.1))\n",
    "\n",
    "model.add(Conv2D(filters=512, kernel_size=(3,3), padding=\"same\",strides=1))\n",
    "model.add(BatchNormalization())\n",
    "model.add(LeakyReLU(alpha=0.1))\n",
    "\n",
    "model.add(MaxPool2D(pool_size=(2,2),strides=(1,1)))\n",
    "\n",
    "model.add(Conv2D(filters=512, kernel_size=(3,3), padding=\"same\",strides=1))\n",
    "model.add(BatchNormalization())\n",
    "model.add(LeakyReLU(alpha=0.1))\n",
    "\n",
    "model.add(Conv2D(filters=512, kernel_size=(3,3), padding=\"same\",strides=1))\n",
    "model.add(BatchNormalization())\n",
    "model.add(LeakyReLU(alpha=0.1))\n",
    "\n",
    "model.add(Conv2D(filters=512, kernel_size=(3,3), padding=\"same\",strides=1))\n",
    "model.add(BatchNormalization())\n",
    "model.add(LeakyReLU(alpha=0.1))\n",
    "\n",
    "model.add(MaxPool2D(pool_size=(2,2),strides=(1,1)))\n",
    "\n",
    "model.add(Flatten())\n",
    "\n",
    "model.add(Dense(512))\n",
    "model.add(BatchNormalization())\n",
    "model.add(LeakyReLU(alpha=0.1))\n",
    "\n",
    "model.add(Dense(512))\n",
    "model.add(BatchNormalization())\n",
    "model.add(LeakyReLU(alpha=0.1))\n",
    "\n",
    "#adding a new layer\n",
    "\n",
    "\n",
    "\n",
    "model.add(Dense(y_train.shape[1],activation=\"softmax\"))\n",
    "\n",
    "INIT_LEARNING_RATE = 0.01\n",
    "MOMENTUM_RATE = 0.91\n",
    "\n",
    "\n",
    "sgd = SGD(lr=INIT_LEARNING_RATE, momentum=MOMENTUM_RATE)\n",
    "\n",
    "\n",
    "#adam=Adam(learning_rate=0.001,clipnorm=1,name=\"adam\")\n",
    "\n",
    "model.compile(loss=\"categorical_crossentropy\",optimizer=sgd,metrics=[\"accuracy\"])\n",
    "\n",
    "\n",
    "call=EarlyStopping(monitor=\"val_loss\",verbose=1,mode=\"auto\",patience=5,restore_best_weights=True)\n",
    "checkpoint=ModelCheckpoint('sgd_regularized_batch_weights.hdf5',monitor='val_accuracy',verbose=1,save_best_only=True,save_weights_only=True,model='auto',period=1)\n",
    "\n",
    "#model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 989
    },
    "id": "it_FHdw3tcXj",
    "outputId": "57ced41c-ecdf-4e43-b180-fb5a24361bc7"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/50\n",
      "  2/391 [..............................] - ETA: 3:17 - loss: 5.0324 - accuracy: 0.0391WARNING:tensorflow:Callbacks method `on_train_batch_end` is slow compared to the batch time (batch time: 0.3621s vs `on_train_batch_end` time: 0.6539s). Check your callbacks.\n",
      "391/391 [==============================] - ETA: 0s - loss: 3.2259 - accuracy: 0.2272\n",
      "Epoch 00001: val_accuracy improved from -inf to 0.20840, saving model to sgd_regularized_batch_weights.hdf5\n",
      "391/391 [==============================] - 458s 1s/step - loss: 3.2259 - accuracy: 0.2272 - val_loss: 3.2945 - val_accuracy: 0.2084\n",
      "Epoch 2/50\n",
      "391/391 [==============================] - ETA: 0s - loss: 2.2373 - accuracy: 0.4228\n",
      "Epoch 00002: val_accuracy improved from 0.20840 to 0.37750, saving model to sgd_regularized_batch_weights.hdf5\n",
      "391/391 [==============================] - 464s 1s/step - loss: 2.2373 - accuracy: 0.4228 - val_loss: 2.4524 - val_accuracy: 0.3775\n",
      "Epoch 3/50\n",
      "391/391 [==============================] - ETA: 0s - loss: 1.6813 - accuracy: 0.5496\n",
      "Epoch 00003: val_accuracy improved from 0.37750 to 0.45950, saving model to sgd_regularized_batch_weights.hdf5\n",
      "391/391 [==============================] - 466s 1s/step - loss: 1.6813 - accuracy: 0.5496 - val_loss: 2.0811 - val_accuracy: 0.4595\n",
      "Epoch 4/50\n",
      "391/391 [==============================] - ETA: 0s - loss: 1.2050 - accuracy: 0.6722\n",
      "Epoch 00004: val_accuracy did not improve from 0.45950\n",
      "391/391 [==============================] - 467s 1s/step - loss: 1.2050 - accuracy: 0.6722 - val_loss: 2.2356 - val_accuracy: 0.4405\n",
      "Epoch 5/50\n",
      "391/391 [==============================] - ETA: 0s - loss: 0.7412 - accuracy: 0.8025\n",
      "Epoch 00005: val_accuracy improved from 0.45950 to 0.48490, saving model to sgd_regularized_batch_weights.hdf5\n",
      "391/391 [==============================] - 475s 1s/step - loss: 0.7412 - accuracy: 0.8025 - val_loss: 2.1239 - val_accuracy: 0.4849\n",
      "Epoch 6/50\n",
      "391/391 [==============================] - ETA: 0s - loss: 0.3343 - accuracy: 0.9255\n",
      "Epoch 00006: val_accuracy did not improve from 0.48490\n",
      "391/391 [==============================] - 465s 1s/step - loss: 0.3343 - accuracy: 0.9255 - val_loss: 2.2450 - val_accuracy: 0.4793\n",
      "Epoch 7/50\n",
      "391/391 [==============================] - ETA: 0s - loss: 0.1045 - accuracy: 0.9850\n",
      "Epoch 00007: val_accuracy improved from 0.48490 to 0.52120, saving model to sgd_regularized_batch_weights.hdf5\n",
      "391/391 [==============================] - 465s 1s/step - loss: 0.1045 - accuracy: 0.9850 - val_loss: 2.0323 - val_accuracy: 0.5212\n",
      "Epoch 8/50\n",
      "391/391 [==============================] - ETA: 0s - loss: 0.0331 - accuracy: 0.9977\n",
      "Epoch 00008: val_accuracy improved from 0.52120 to 0.54400, saving model to sgd_regularized_batch_weights.hdf5\n",
      "391/391 [==============================] - 464s 1s/step - loss: 0.0331 - accuracy: 0.9977 - val_loss: 1.9521 - val_accuracy: 0.5440\n",
      "Epoch 9/50\n",
      "391/391 [==============================] - ETA: 0s - loss: 0.0166 - accuracy: 0.9989\n",
      "Epoch 00009: val_accuracy improved from 0.54400 to 0.54960, saving model to sgd_regularized_batch_weights.hdf5\n",
      "391/391 [==============================] - 464s 1s/step - loss: 0.0166 - accuracy: 0.9989 - val_loss: 1.9584 - val_accuracy: 0.5496\n",
      "Epoch 10/50\n",
      "391/391 [==============================] - ETA: 0s - loss: 0.0106 - accuracy: 0.9994\n",
      "Epoch 00010: val_accuracy improved from 0.54960 to 0.55010, saving model to sgd_regularized_batch_weights.hdf5\n",
      "391/391 [==============================] - 476s 1s/step - loss: 0.0106 - accuracy: 0.9994 - val_loss: 1.9815 - val_accuracy: 0.5501\n",
      "Epoch 11/50\n",
      "391/391 [==============================] - ETA: 0s - loss: 0.0087 - accuracy: 0.9994\n",
      "Epoch 00011: val_accuracy did not improve from 0.55010\n",
      "391/391 [==============================] - 472s 1s/step - loss: 0.0087 - accuracy: 0.9994 - val_loss: 1.9834 - val_accuracy: 0.5478\n",
      "Epoch 12/50\n",
      "391/391 [==============================] - ETA: 0s - loss: 0.0071 - accuracy: 0.9994\n",
      "Epoch 00012: val_accuracy did not improve from 0.55010\n",
      "391/391 [==============================] - 465s 1s/step - loss: 0.0071 - accuracy: 0.9994 - val_loss: 1.9915 - val_accuracy: 0.5497\n",
      "Epoch 13/50\n",
      "391/391 [==============================] - ETA: 0s - loss: 0.0061 - accuracy: 0.9994Restoring model weights from the end of the best epoch.\n",
      "\n",
      "Epoch 00013: val_accuracy improved from 0.55010 to 0.55210, saving model to sgd_regularized_batch_weights.hdf5\n",
      "391/391 [==============================] - 466s 1s/step - loss: 0.0061 - accuracy: 0.9994 - val_loss: 1.9973 - val_accuracy: 0.5521\n",
      "Epoch 00013: early stopping\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.keras.callbacks.History at 0x7fafc0392748>"
      ]
     },
     "execution_count": 3,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.fit(X_train,y_train,batch_size=128,epochs=50,validation_data=(X_test,y_test),verbose=1,callbacks=[call,checkpoint])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "id": "vheVmLKGW1iR",
    "outputId": "943d71a7-b34c-4474-cff3-03b3f8374e5c"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From <ipython-input-4-84fbeaedd2bf>:3: Sequential.predict_classes (from tensorflow.python.keras.engine.sequential) is deprecated and will be removed after 2021-01-01.\n",
      "Instructions for updating:\n",
      "Please use instead:* `np.argmax(model.predict(x), axis=-1)`,   if your model does multi-class classification   (e.g. if it uses a `softmax` last-layer activation).* `(model.predict(x) > 0.5).astype(\"int32\")`,   if your model does binary classification   (e.g. if it uses a `sigmoid` last-layer activation).\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.80      0.75      0.77       100\n",
      "           1       0.67      0.74      0.70       100\n",
      "           2       0.42      0.41      0.41       100\n",
      "           3       0.35      0.36      0.35       100\n",
      "           4       0.37      0.33      0.35       100\n",
      "           5       0.48      0.45      0.47       100\n",
      "           6       0.52      0.56      0.54       100\n",
      "           7       0.60      0.47      0.53       100\n",
      "           8       0.76      0.61      0.68       100\n",
      "           9       0.72      0.65      0.68       100\n",
      "          10       0.47      0.45      0.46       100\n",
      "          11       0.36      0.36      0.36       100\n",
      "          12       0.54      0.56      0.55       100\n",
      "          13       0.53      0.50      0.52       100\n",
      "          14       0.58      0.45      0.51       100\n",
      "          15       0.51      0.42      0.46       100\n",
      "          16       0.57      0.54      0.55       100\n",
      "          17       0.64      0.76      0.70       100\n",
      "          18       0.48      0.53      0.50       100\n",
      "          19       0.44      0.40      0.42       100\n",
      "          20       0.84      0.76      0.80       100\n",
      "          21       0.60      0.72      0.65       100\n",
      "          22       0.59      0.55      0.57       100\n",
      "          23       0.79      0.62      0.70       100\n",
      "          24       0.67      0.75      0.71       100\n",
      "          25       0.38      0.43      0.40       100\n",
      "          26       0.51      0.52      0.52       100\n",
      "          27       0.36      0.38      0.37       100\n",
      "          28       0.67      0.76      0.71       100\n",
      "          29       0.57      0.50      0.53       100\n",
      "          30       0.54      0.50      0.52       100\n",
      "          31       0.42      0.53      0.47       100\n",
      "          32       0.61      0.41      0.49       100\n",
      "          33       0.52      0.55      0.53       100\n",
      "          34       0.48      0.61      0.54       100\n",
      "          35       0.26      0.24      0.25       100\n",
      "          36       0.74      0.61      0.67       100\n",
      "          37       0.53      0.44      0.48       100\n",
      "          38       0.31      0.45      0.37       100\n",
      "          39       0.71      0.67      0.69       100\n",
      "          40       0.50      0.48      0.49       100\n",
      "          41       0.81      0.71      0.76       100\n",
      "          42       0.46      0.57      0.51       100\n",
      "          43       0.68      0.55      0.61       100\n",
      "          44       0.22      0.25      0.23       100\n",
      "          45       0.40      0.37      0.38       100\n",
      "          46       0.32      0.42      0.36       100\n",
      "          47       0.62      0.53      0.57       100\n",
      "          48       0.75      0.80      0.77       100\n",
      "          49       0.64      0.72      0.68       100\n",
      "          50       0.32      0.30      0.31       100\n",
      "          51       0.55      0.41      0.47       100\n",
      "          52       0.54      0.64      0.58       100\n",
      "          53       0.78      0.87      0.82       100\n",
      "          54       0.53      0.75      0.62       100\n",
      "          55       0.26      0.24      0.25       100\n",
      "          56       0.72      0.73      0.72       100\n",
      "          57       0.71      0.57      0.63       100\n",
      "          58       0.68      0.68      0.68       100\n",
      "          59       0.52      0.43      0.47       100\n",
      "          60       0.73      0.83      0.78       100\n",
      "          61       0.73      0.64      0.68       100\n",
      "          62       0.60      0.58      0.59       100\n",
      "          63       0.58      0.47      0.52       100\n",
      "          64       0.39      0.32      0.35       100\n",
      "          65       0.36      0.40      0.38       100\n",
      "          66       0.54      0.55      0.54       100\n",
      "          67       0.41      0.43      0.42       100\n",
      "          68       0.79      0.85      0.82       100\n",
      "          69       0.62      0.72      0.66       100\n",
      "          70       0.52      0.56      0.54       100\n",
      "          71       0.64      0.77      0.70       100\n",
      "          72       0.28      0.26      0.27       100\n",
      "          73       0.34      0.50      0.41       100\n",
      "          74       0.39      0.46      0.42       100\n",
      "          75       0.72      0.76      0.74       100\n",
      "          76       0.65      0.81      0.72       100\n",
      "          77       0.40      0.38      0.39       100\n",
      "          78       0.34      0.34      0.34       100\n",
      "          79       0.55      0.62      0.58       100\n",
      "          80       0.36      0.33      0.34       100\n",
      "          81       0.58      0.59      0.58       100\n",
      "          82       0.78      0.78      0.78       100\n",
      "          83       0.54      0.52      0.53       100\n",
      "          84       0.49      0.46      0.47       100\n",
      "          85       0.70      0.64      0.67       100\n",
      "          86       0.71      0.58      0.64       100\n",
      "          87       0.50      0.69      0.58       100\n",
      "          88       0.56      0.58      0.57       100\n",
      "          89       0.65      0.60      0.62       100\n",
      "          90       0.56      0.57      0.56       100\n",
      "          91       0.68      0.59      0.63       100\n",
      "          92       0.54      0.43      0.48       100\n",
      "          93       0.34      0.23      0.28       100\n",
      "          94       0.83      0.83      0.83       100\n",
      "          95       0.66      0.57      0.61       100\n",
      "          96       0.48      0.43      0.45       100\n",
      "          97       0.54      0.55      0.55       100\n",
      "          98       0.28      0.24      0.26       100\n",
      "          99       0.55      0.62      0.58       100\n",
      "\n",
      "    accuracy                           0.54     10000\n",
      "   macro avg       0.55      0.54      0.54     10000\n",
      "weighted avg       0.55      0.54      0.54     10000\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import sklearn\n",
    "from sklearn.metrics import accuracy_score,precision_score,recall_score,classification_report\n",
    "y_pred=model.predict_classes(X_test)\n",
    "print(classification_report(Y_test,y_pred))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 68
    },
    "id": "A4pKc3DsXDTV",
    "outputId": "302e6ed7-1789-4b3b-a963-83e10f34a186"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.544\n",
      "54.83808167258854\n",
      "54.400000000000006\n"
     ]
    }
   ],
   "source": [
    "print(accuracy_score(Y_test,y_pred))\n",
    "print(precision_score(Y_test,y_pred,average=\"weighted\")*100)\n",
    "print(recall_score(Y_test,y_pred,average=\"weighted\")*100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "id": "8uRJgVDTj5vI",
    "outputId": "5567df6e-b566-4711-8de9-da21051d94e6"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "conv2d (Conv2D)              (None, 32, 32, 64)        1792      \n",
      "_________________________________________________________________\n",
      "batch_normalization (BatchNo (None, 32, 32, 64)        256       \n",
      "_________________________________________________________________\n",
      "leaky_re_lu (LeakyReLU)      (None, 32, 32, 64)        0         \n",
      "_________________________________________________________________\n",
      "conv2d_1 (Conv2D)            (None, 32, 32, 64)        36928     \n",
      "_________________________________________________________________\n",
      "batch_normalization_1 (Batch (None, 32, 32, 64)        256       \n",
      "_________________________________________________________________\n",
      "leaky_re_lu_1 (LeakyReLU)    (None, 32, 32, 64)        0         \n",
      "_________________________________________________________________\n",
      "max_pooling2d (MaxPooling2D) (None, 31, 31, 64)        0         \n",
      "_________________________________________________________________\n",
      "conv2d_2 (Conv2D)            (None, 31, 31, 128)       73856     \n",
      "_________________________________________________________________\n",
      "batch_normalization_2 (Batch (None, 31, 31, 128)       512       \n",
      "_________________________________________________________________\n",
      "leaky_re_lu_2 (LeakyReLU)    (None, 31, 31, 128)       0         \n",
      "_________________________________________________________________\n",
      "conv2d_3 (Conv2D)            (None, 31, 31, 128)       147584    \n",
      "_________________________________________________________________\n",
      "batch_normalization_3 (Batch (None, 31, 31, 128)       512       \n",
      "_________________________________________________________________\n",
      "leaky_re_lu_3 (LeakyReLU)    (None, 31, 31, 128)       0         \n",
      "_________________________________________________________________\n",
      "max_pooling2d_1 (MaxPooling2 (None, 30, 30, 128)       0         \n",
      "_________________________________________________________________\n",
      "conv2d_4 (Conv2D)            (None, 30, 30, 256)       295168    \n",
      "_________________________________________________________________\n",
      "batch_normalization_4 (Batch (None, 30, 30, 256)       1024      \n",
      "_________________________________________________________________\n",
      "leaky_re_lu_4 (LeakyReLU)    (None, 30, 30, 256)       0         \n",
      "_________________________________________________________________\n",
      "conv2d_5 (Conv2D)            (None, 30, 30, 256)       590080    \n",
      "_________________________________________________________________\n",
      "batch_normalization_5 (Batch (None, 30, 30, 256)       1024      \n",
      "_________________________________________________________________\n",
      "leaky_re_lu_5 (LeakyReLU)    (None, 30, 30, 256)       0         \n",
      "_________________________________________________________________\n",
      "conv2d_6 (Conv2D)            (None, 30, 30, 256)       590080    \n",
      "_________________________________________________________________\n",
      "batch_normalization_6 (Batch (None, 30, 30, 256)       1024      \n",
      "_________________________________________________________________\n",
      "leaky_re_lu_6 (LeakyReLU)    (None, 30, 30, 256)       0         \n",
      "_________________________________________________________________\n",
      "max_pooling2d_2 (MaxPooling2 (None, 29, 29, 256)       0         \n",
      "_________________________________________________________________\n",
      "conv2d_7 (Conv2D)            (None, 29, 29, 512)       1180160   \n",
      "_________________________________________________________________\n",
      "batch_normalization_7 (Batch (None, 29, 29, 512)       2048      \n",
      "_________________________________________________________________\n",
      "leaky_re_lu_7 (LeakyReLU)    (None, 29, 29, 512)       0         \n",
      "_________________________________________________________________\n",
      "conv2d_8 (Conv2D)            (None, 29, 29, 512)       2359808   \n",
      "_________________________________________________________________\n",
      "batch_normalization_8 (Batch (None, 29, 29, 512)       2048      \n",
      "_________________________________________________________________\n",
      "leaky_re_lu_8 (LeakyReLU)    (None, 29, 29, 512)       0         \n",
      "_________________________________________________________________\n",
      "conv2d_9 (Conv2D)            (None, 29, 29, 512)       2359808   \n",
      "_________________________________________________________________\n",
      "batch_normalization_9 (Batch (None, 29, 29, 512)       2048      \n",
      "_________________________________________________________________\n",
      "leaky_re_lu_9 (LeakyReLU)    (None, 29, 29, 512)       0         \n",
      "_________________________________________________________________\n",
      "max_pooling2d_3 (MaxPooling2 (None, 28, 28, 512)       0         \n",
      "_________________________________________________________________\n",
      "conv2d_10 (Conv2D)           (None, 28, 28, 512)       2359808   \n",
      "_________________________________________________________________\n",
      "batch_normalization_10 (Batc (None, 28, 28, 512)       2048      \n",
      "_________________________________________________________________\n",
      "leaky_re_lu_10 (LeakyReLU)   (None, 28, 28, 512)       0         \n",
      "_________________________________________________________________\n",
      "conv2d_11 (Conv2D)           (None, 28, 28, 512)       2359808   \n",
      "_________________________________________________________________\n",
      "batch_normalization_11 (Batc (None, 28, 28, 512)       2048      \n",
      "_________________________________________________________________\n",
      "leaky_re_lu_11 (LeakyReLU)   (None, 28, 28, 512)       0         \n",
      "_________________________________________________________________\n",
      "conv2d_12 (Conv2D)           (None, 28, 28, 512)       2359808   \n",
      "_________________________________________________________________\n",
      "batch_normalization_12 (Batc (None, 28, 28, 512)       2048      \n",
      "_________________________________________________________________\n",
      "leaky_re_lu_12 (LeakyReLU)   (None, 28, 28, 512)       0         \n",
      "_________________________________________________________________\n",
      "max_pooling2d_4 (MaxPooling2 (None, 27, 27, 512)       0         \n",
      "_________________________________________________________________\n",
      "flatten (Flatten)            (None, 373248)            0         \n",
      "_________________________________________________________________\n",
      "dense (Dense)                (None, 512)               191103488 \n",
      "_________________________________________________________________\n",
      "batch_normalization_13 (Batc (None, 512)               2048      \n",
      "_________________________________________________________________\n",
      "leaky_re_lu_13 (LeakyReLU)   (None, 512)               0         \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 512)               262656    \n",
      "_________________________________________________________________\n",
      "batch_normalization_14 (Batc (None, 512)               2048      \n",
      "_________________________________________________________________\n",
      "leaky_re_lu_14 (LeakyReLU)   (None, 512)               0         \n",
      "_________________________________________________________________\n",
      "dense_2 (Dense)              (None, 100)               51300     \n",
      "=================================================================\n",
      "Total params: 206,153,124\n",
      "Trainable params: 206,142,628\n",
      "Non-trainable params: 10,496\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from tensorflow.keras.applications.vgg16 import VGG16\n",
    "from tensorflow.keras.models import Sequential,Model\n",
    "from tensorflow.keras.layers import Dense,Flatten,Input,Conv2D,MaxPool2D\n",
    "\n",
    "from tensorflow.keras.layers import LeakyReLU,BatchNormalization\n",
    "\n",
    "from tensorflow.keras.datasets import cifar100\n",
    "from tensorflow.keras.utils import to_categorical\n",
    "\n",
    "from tensorflow.keras.callbacks import EarlyStopping,ModelCheckpoint\n",
    "\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "\n",
    "from tensorflow.keras.optimizers import RMSprop,SGD\n",
    "\n",
    "(X_train,Y_train),(X_test,Y_test)=cifar100.load_data()\n",
    "y_train,y_test=to_categorical(Y_train),to_categorical(Y_test)\n",
    "\n",
    "X_train,X_test=X_train/255,X_test/255\n",
    "\n",
    "model=Sequential()\n",
    "\n",
    "model.add(Conv2D(input_shape=(32,32,3),filters=64,kernel_size=(3,3),padding=\"same\",strides=1))\n",
    "model.add(BatchNormalization())\n",
    "model.add(LeakyReLU(alpha=0.1))\n",
    "\n",
    "model.add(Conv2D(filters=64,kernel_size=(3,3),padding=\"same\"))\n",
    "model.add(BatchNormalization())\n",
    "model.add(LeakyReLU(alpha=0.1))\n",
    "\n",
    "model.add(MaxPool2D(pool_size=(2,2),strides=(1,1)))\n",
    "\n",
    "model.add(Conv2D(filters=128, kernel_size=(3,3), padding=\"same\",strides=1))\n",
    "model.add(BatchNormalization())\n",
    "model.add(LeakyReLU(alpha=0.1))\n",
    "\n",
    "model.add(Conv2D(filters=128, kernel_size=(3,3), padding=\"same\",strides=1))\n",
    "model.add(BatchNormalization())\n",
    "model.add(LeakyReLU(alpha=0.1))\n",
    "\n",
    "model.add(MaxPool2D(pool_size=(2,2),strides=(1,1)))\n",
    "\n",
    "model.add(Conv2D(filters=256, kernel_size=(3,3), padding=\"same\",strides=1))\n",
    "model.add(BatchNormalization())\n",
    "model.add(LeakyReLU(alpha=0.1))\n",
    "\n",
    "model.add(Conv2D(filters=256, kernel_size=(3,3), padding=\"same\",strides=1))\n",
    "model.add(BatchNormalization())\n",
    "model.add(LeakyReLU(alpha=0.1))\n",
    "\n",
    "model.add(Conv2D(filters=256, kernel_size=(3,3), padding=\"same\",strides=1))\n",
    "model.add(BatchNormalization())\n",
    "model.add(LeakyReLU(alpha=0.1))\n",
    "\n",
    "model.add(MaxPool2D(pool_size=(2,2),strides=(1,1)))\n",
    "\n",
    "model.add(Conv2D(filters=512, kernel_size=(3,3), padding=\"same\",strides=1))\n",
    "model.add(BatchNormalization())\n",
    "model.add(LeakyReLU(alpha=0.1))\n",
    "\n",
    "model.add(Conv2D(filters=512, kernel_size=(3,3), padding=\"same\",strides=1))\n",
    "model.add(BatchNormalization())\n",
    "model.add(LeakyReLU(alpha=0.1))\n",
    "\n",
    "model.add(Conv2D(filters=512, kernel_size=(3,3), padding=\"same\",strides=1))\n",
    "model.add(BatchNormalization())\n",
    "model.add(LeakyReLU(alpha=0.1))\n",
    "\n",
    "model.add(MaxPool2D(pool_size=(2,2),strides=(1,1)))\n",
    "\n",
    "model.add(Conv2D(filters=512, kernel_size=(3,3), padding=\"same\",strides=1))\n",
    "model.add(BatchNormalization())\n",
    "model.add(LeakyReLU(alpha=0.1))\n",
    "\n",
    "model.add(Conv2D(filters=512, kernel_size=(3,3), padding=\"same\",strides=1))\n",
    "model.add(BatchNormalization())\n",
    "model.add(LeakyReLU(alpha=0.1))\n",
    "\n",
    "model.add(Conv2D(filters=512, kernel_size=(3,3), padding=\"same\",strides=1))\n",
    "model.add(BatchNormalization())\n",
    "model.add(LeakyReLU(alpha=0.1))\n",
    "\n",
    "model.add(MaxPool2D(pool_size=(2,2),strides=(1,1)))\n",
    "\n",
    "model.add(Flatten())\n",
    "\n",
    "model.add(Dense(512))\n",
    "model.add(BatchNormalization())\n",
    "model.add(LeakyReLU(alpha=0.1))\n",
    "\n",
    "model.add(Dense(512))\n",
    "model.add(BatchNormalization())\n",
    "model.add(LeakyReLU(alpha=0.1))\n",
    "\n",
    "#adding a new layer\n",
    "\n",
    "\n",
    "\n",
    "model.add(Dense(y_train.shape[1],activation=\"softmax\"))\n",
    "\n",
    "model.load_weights(\"../weights/sgd_regularized_batch_weights.hdf5\")\n",
    "\n",
    "\n",
    "import sklearn\n",
    "from sklearn.metrics import accuracy_score,precision_score,recall_score,classification_report\n",
    "y_pred=model.predict_classes(X_test)\n",
    "print(classification_report(Y_test,y_pred))\n",
    "\n",
    "print(accuracy_score(Y_test,y_pred))\n",
    "print(precision_score(Y_test,y_pred,average=\"weighted\")*100)\n",
    "print(recall_score(Y_test,y_pred,average=\"weighted\")*100)"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "name": "VGG16_SGD_Batchnorm.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
