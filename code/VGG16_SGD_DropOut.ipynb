{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 51
    },
    "id": "Pr0jCnvKRqA7",
    "outputId": "4a663cf9-7327-4207-fc1a-a9d45b02964e"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading data from https://www.cs.toronto.edu/~kriz/cifar-100-python.tar.gz\n",
      "169009152/169001437 [==============================] - 4s 0us/step\n"
     ]
    }
   ],
   "source": [
    "from keras.callbacks import ModelCheckpoint, EarlyStopping\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Conv2D, MaxPool2D, Flatten, Dense,BatchNormalization,Activation,Dropout,LeakyReLU\n",
    "from tensorflow.keras.optimizers import SGD\n",
    "from keras.preprocessing.image import ImageDataGenerator\n",
    "from keras.utils import to_categorical\n",
    "from tensorflow.keras.datasets import cifar100\n",
    "\n",
    "\n",
    "\n",
    "(x_train, Y_train), (x_test, Y_test) = cifar100.load_data()\n",
    "\n",
    "y_train = to_categorical(Y_train)\n",
    "y_test = to_categorical(Y_test)\n",
    "\n",
    "aug_data=ImageDataGenerator(rotation_range=20,horizontal_flip=True,width_shift_range=0.1,shear_range = 0.2,height_shift_range=0.1,zoom_range=0.2,brightness_range = (0.5, 1.5))\n",
    "aug_data.fit(x_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 54
    },
    "id": "P99BgQTASCmC",
    "outputId": "2b566eef-d4b4-4552-e557-bf872e1abcc4"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:`period` argument is deprecated. Please use `save_freq` to specify the frequency in number of batches seen.\n"
     ]
    }
   ],
   "source": [
    "\n",
    "model = Sequential()\n",
    "model.add(Conv2D(filters= 64, kernel_size= (3,3), strides= (1,1), padding='same', input_shape= (32, 32, 3)))\n",
    "model.add(LeakyReLU(alpha=0.1))\n",
    "model.add(Conv2D(filters= 64, kernel_size= (3,3), strides= (1,1), padding='same'))\n",
    "model.add(LeakyReLU(alpha=0.1))\n",
    "model.add(MaxPool2D(pool_size= (2,2), strides=(1,1)))\n",
    "model.add(Dropout(0.5))\n",
    "\n",
    "# Creating second block- (2 Convolution + 1 Max pool)\n",
    "model.add(Conv2D(filters= 128, kernel_size= (3,3), strides= (1,1), padding='same'))\n",
    "model.add(LeakyReLU(alpha=0.1))\n",
    "model.add(Conv2D(filters= 128, kernel_size= (3,3), strides= (1,1), padding='same'))\n",
    "model.add(LeakyReLU(alpha=0.1))\n",
    "model.add(MaxPool2D(pool_size= (2,2), strides=(1,1)))\n",
    "model.add(Dropout(0.5))\n",
    "\n",
    "\n",
    "# Creating third block- (3 Convolution + 1 Max pool)\n",
    "model.add(Conv2D(filters= 256, kernel_size= (3,3), strides= (1,1), padding='same'))\n",
    "model.add(LeakyReLU(alpha=0.1))\n",
    "model.add(Conv2D(filters= 256, kernel_size= (3,3), strides= (1,1), padding='same'))\n",
    "model.add(LeakyReLU(alpha=0.1))\n",
    "model.add(Conv2D(filters= 256, kernel_size= (3,3), strides= (1,1), padding='same'))\n",
    "model.add(LeakyReLU(alpha=0.1))\n",
    "model.add(MaxPool2D(pool_size= (2,2), strides=(1,1)))\n",
    "model.add(Dropout(0.5))\n",
    "\n",
    "model.add(Conv2D(filters= 512, kernel_size= (3,3), strides= (1,1), padding='same'))\n",
    "model.add(LeakyReLU(alpha=0.1))\n",
    "model.add(Conv2D(filters= 512, kernel_size= (3,3), strides= (1,1), padding='same'))\n",
    "model.add(LeakyReLU(alpha=0.1))\n",
    "model.add(Conv2D(filters= 512, kernel_size= (3,3), strides= (1,1), padding='same'))\n",
    "model.add(LeakyReLU(alpha=0.1))\n",
    "model.add(MaxPool2D(pool_size= (2,2), strides=(2,2)))\n",
    "model.add(Dropout(0.5))\n",
    "\n",
    "model.add(Conv2D(filters= 512, kernel_size= (3,3), strides= (1,1), padding='same'))\n",
    "model.add(LeakyReLU(alpha=0.1))\n",
    "model.add(Conv2D(filters= 512, kernel_size= (3,3), strides= (1,1), padding='same'))\n",
    "model.add(LeakyReLU(alpha=0.1))\n",
    "model.add(Conv2D(filters= 512, kernel_size= (3,3), strides= (1,1), padding='same'))\n",
    "model.add(LeakyReLU(alpha=0.1))\n",
    "model.add(MaxPool2D(pool_size= (2,2), strides=(1,1)))\n",
    "model.add(Dropout(0.5))\n",
    "\n",
    "# Flattening the pooled image pixels\n",
    "model.add(Flatten())\n",
    "\n",
    "model.add(Dense(units= 512))\n",
    "model.add(LeakyReLU(alpha=0.1))\n",
    "model.add(Dropout(0.5))\n",
    "\n",
    "model.add(Dropout(0.5))\n",
    "model.add(Dense(units= 100, activation='softmax'))\n",
    "'''\n",
    "optimizer = tf.train.AdamOptimizer(learning_rate=0.1)\n",
    "gvs = optimizer.compute_gradients(cost)\n",
    "capped_gvs = [(tf.clip_by_value(grad, -1., 1.), var) for grad, var in gvs]\n",
    "train_op = optimizer.apply_gradients(capped_gvs)\n",
    "'''\n",
    "sgd=SGD(learning_rate=0.01,momentum=0.9,clipnorm=1)\n",
    "model.compile(optimizer=sgd,loss='categorical_crossentropy',metrics=['accuracy'])\n",
    "\n",
    "checkpoint = ModelCheckpoint(\"VGG_DropOut_SGD_weights_amit.hdf5\", monitor='val_accuracy', verbose=1, save_best_only=True, save_weights_only=True, mode='auto', period=1)\n",
    "early = EarlyStopping(monitor='val_accuracy', min_delta=0, patience=5, verbose=1, mode='auto',restore_best_weights=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "id": "6QF9-tEYSDce",
    "outputId": "d69f0787-9fc8-45b6-9adb-4df4e99a15ae"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100\n",
      "  2/391 [..............................] - ETA: 1:08 - loss: 35.7619 - accuracy: 0.0117WARNING:tensorflow:Callbacks method `on_train_batch_end` is slow compared to the batch time (batch time: 0.1390s vs `on_train_batch_end` time: 0.2105s). Check your callbacks.\n",
      "391/391 [==============================] - ETA: 0s - loss: 4.7611 - accuracy: 0.0154\n",
      "Epoch 00001: val_accuracy improved from -inf to 0.00990, saving model to VGG_DropOut_SGD_weights_amit.hdf5\n",
      "391/391 [==============================] - 152s 388ms/step - loss: 4.7611 - accuracy: 0.0154 - val_loss: 4.6163 - val_accuracy: 0.0099\n",
      "Epoch 2/100\n",
      "391/391 [==============================] - ETA: 0s - loss: 4.4492 - accuracy: 0.0249\n",
      "Epoch 00002: val_accuracy improved from 0.00990 to 0.01510, saving model to VGG_DropOut_SGD_weights_amit.hdf5\n",
      "391/391 [==============================] - 150s 382ms/step - loss: 4.4492 - accuracy: 0.0249 - val_loss: 4.5542 - val_accuracy: 0.0151\n",
      "Epoch 3/100\n",
      "391/391 [==============================] - ETA: 0s - loss: 4.2988 - accuracy: 0.0438\n",
      "Epoch 00003: val_accuracy improved from 0.01510 to 0.04110, saving model to VGG_DropOut_SGD_weights_amit.hdf5\n",
      "391/391 [==============================] - 149s 382ms/step - loss: 4.2988 - accuracy: 0.0438 - val_loss: 4.3492 - val_accuracy: 0.0411\n",
      "Epoch 4/100\n",
      "391/391 [==============================] - ETA: 0s - loss: 4.1140 - accuracy: 0.0650\n",
      "Epoch 00004: val_accuracy improved from 0.04110 to 0.07030, saving model to VGG_DropOut_SGD_weights_amit.hdf5\n",
      "391/391 [==============================] - 150s 383ms/step - loss: 4.1140 - accuracy: 0.0650 - val_loss: 4.1299 - val_accuracy: 0.0703\n",
      "Epoch 5/100\n",
      "391/391 [==============================] - ETA: 0s - loss: 3.9558 - accuracy: 0.0879\n",
      "Epoch 00005: val_accuracy improved from 0.07030 to 0.11340, saving model to VGG_DropOut_SGD_weights_amit.hdf5\n",
      "391/391 [==============================] - 149s 382ms/step - loss: 3.9558 - accuracy: 0.0879 - val_loss: 3.8659 - val_accuracy: 0.1134\n",
      "Epoch 6/100\n",
      "391/391 [==============================] - ETA: 0s - loss: 3.8137 - accuracy: 0.1098\n",
      "Epoch 00006: val_accuracy improved from 0.11340 to 0.15190, saving model to VGG_DropOut_SGD_weights_amit.hdf5\n",
      "391/391 [==============================] - 149s 382ms/step - loss: 3.8137 - accuracy: 0.1098 - val_loss: 3.5961 - val_accuracy: 0.1519\n",
      "Epoch 7/100\n",
      "391/391 [==============================] - ETA: 0s - loss: 3.6745 - accuracy: 0.1328\n",
      "Epoch 00007: val_accuracy improved from 0.15190 to 0.19290, saving model to VGG_DropOut_SGD_weights_amit.hdf5\n",
      "391/391 [==============================] - 149s 382ms/step - loss: 3.6745 - accuracy: 0.1328 - val_loss: 3.4130 - val_accuracy: 0.1929\n",
      "Epoch 8/100\n",
      "391/391 [==============================] - ETA: 0s - loss: 3.5659 - accuracy: 0.1528\n",
      "Epoch 00008: val_accuracy improved from 0.19290 to 0.22390, saving model to VGG_DropOut_SGD_weights_amit.hdf5\n",
      "391/391 [==============================] - 150s 383ms/step - loss: 3.5659 - accuracy: 0.1528 - val_loss: 3.2787 - val_accuracy: 0.2239\n",
      "Epoch 9/100\n",
      "391/391 [==============================] - ETA: 0s - loss: 3.4625 - accuracy: 0.1702\n",
      "Epoch 00009: val_accuracy improved from 0.22390 to 0.24340, saving model to VGG_DropOut_SGD_weights_amit.hdf5\n",
      "391/391 [==============================] - 149s 381ms/step - loss: 3.4625 - accuracy: 0.1702 - val_loss: 3.1321 - val_accuracy: 0.2434\n",
      "Epoch 10/100\n",
      "391/391 [==============================] - ETA: 0s - loss: 3.3693 - accuracy: 0.1869\n",
      "Epoch 00010: val_accuracy improved from 0.24340 to 0.26030, saving model to VGG_DropOut_SGD_weights_amit.hdf5\n",
      "391/391 [==============================] - 149s 382ms/step - loss: 3.3693 - accuracy: 0.1869 - val_loss: 3.0153 - val_accuracy: 0.2603\n",
      "Epoch 11/100\n",
      "391/391 [==============================] - ETA: 0s - loss: 3.2878 - accuracy: 0.2008\n",
      "Epoch 00011: val_accuracy improved from 0.26030 to 0.27920, saving model to VGG_DropOut_SGD_weights_amit.hdf5\n",
      "391/391 [==============================] - 150s 383ms/step - loss: 3.2878 - accuracy: 0.2008 - val_loss: 2.9370 - val_accuracy: 0.2792\n",
      "Epoch 12/100\n",
      "391/391 [==============================] - ETA: 0s - loss: 3.1963 - accuracy: 0.2198\n",
      "Epoch 00012: val_accuracy improved from 0.27920 to 0.30640, saving model to VGG_DropOut_SGD_weights_amit.hdf5\n",
      "391/391 [==============================] - 149s 382ms/step - loss: 3.1963 - accuracy: 0.2198 - val_loss: 2.8016 - val_accuracy: 0.3064\n",
      "Epoch 13/100\n",
      "391/391 [==============================] - ETA: 0s - loss: 3.1219 - accuracy: 0.2342\n",
      "Epoch 00013: val_accuracy improved from 0.30640 to 0.32130, saving model to VGG_DropOut_SGD_weights_amit.hdf5\n",
      "391/391 [==============================] - 149s 381ms/step - loss: 3.1219 - accuracy: 0.2342 - val_loss: 2.7061 - val_accuracy: 0.3213\n",
      "Epoch 14/100\n",
      "391/391 [==============================] - ETA: 0s - loss: 3.0467 - accuracy: 0.2455\n",
      "Epoch 00014: val_accuracy improved from 0.32130 to 0.32530, saving model to VGG_DropOut_SGD_weights_amit.hdf5\n",
      "391/391 [==============================] - 149s 381ms/step - loss: 3.0467 - accuracy: 0.2455 - val_loss: 2.6811 - val_accuracy: 0.3253\n",
      "Epoch 15/100\n",
      "391/391 [==============================] - ETA: 0s - loss: 2.9592 - accuracy: 0.2635\n",
      "Epoch 00015: val_accuracy improved from 0.32530 to 0.36590, saving model to VGG_DropOut_SGD_weights_amit.hdf5\n",
      "391/391 [==============================] - 149s 381ms/step - loss: 2.9592 - accuracy: 0.2635 - val_loss: 2.4537 - val_accuracy: 0.3659\n",
      "Epoch 16/100\n",
      "391/391 [==============================] - ETA: 0s - loss: 2.8952 - accuracy: 0.2749\n",
      "Epoch 00016: val_accuracy did not improve from 0.36590\n",
      "391/391 [==============================] - 149s 382ms/step - loss: 2.8952 - accuracy: 0.2749 - val_loss: 2.5134 - val_accuracy: 0.3584\n",
      "Epoch 17/100\n",
      "391/391 [==============================] - ETA: 0s - loss: 2.8376 - accuracy: 0.2847\n",
      "Epoch 00017: val_accuracy improved from 0.36590 to 0.36730, saving model to VGG_DropOut_SGD_weights_amit.hdf5\n",
      "391/391 [==============================] - 149s 382ms/step - loss: 2.8376 - accuracy: 0.2847 - val_loss: 2.4328 - val_accuracy: 0.3673\n",
      "Epoch 18/100\n",
      "391/391 [==============================] - ETA: 0s - loss: 2.7804 - accuracy: 0.2970\n",
      "Epoch 00018: val_accuracy improved from 0.36730 to 0.40020, saving model to VGG_DropOut_SGD_weights_amit.hdf5\n",
      "391/391 [==============================] - 149s 382ms/step - loss: 2.7804 - accuracy: 0.2970 - val_loss: 2.3048 - val_accuracy: 0.4002\n",
      "Epoch 19/100\n",
      "391/391 [==============================] - ETA: 0s - loss: 2.7104 - accuracy: 0.3088\n",
      "Epoch 00019: val_accuracy improved from 0.40020 to 0.40840, saving model to VGG_DropOut_SGD_weights_amit.hdf5\n",
      "391/391 [==============================] - 150s 383ms/step - loss: 2.7104 - accuracy: 0.3088 - val_loss: 2.2551 - val_accuracy: 0.4084\n",
      "Epoch 20/100\n",
      "391/391 [==============================] - ETA: 0s - loss: 2.6630 - accuracy: 0.3193\n",
      "Epoch 00020: val_accuracy did not improve from 0.40840\n",
      "391/391 [==============================] - 149s 382ms/step - loss: 2.6630 - accuracy: 0.3193 - val_loss: 2.2724 - val_accuracy: 0.4052\n",
      "Epoch 21/100\n",
      "391/391 [==============================] - ETA: 0s - loss: 2.6086 - accuracy: 0.3307\n",
      "Epoch 00021: val_accuracy improved from 0.40840 to 0.41120, saving model to VGG_DropOut_SGD_weights_amit.hdf5\n",
      "391/391 [==============================] - 150s 383ms/step - loss: 2.6086 - accuracy: 0.3307 - val_loss: 2.2474 - val_accuracy: 0.4112\n",
      "Epoch 22/100\n",
      "391/391 [==============================] - ETA: 0s - loss: 2.5696 - accuracy: 0.3407\n",
      "Epoch 00022: val_accuracy did not improve from 0.41120\n",
      "391/391 [==============================] - 149s 381ms/step - loss: 2.5696 - accuracy: 0.3407 - val_loss: 2.2621 - val_accuracy: 0.4048\n",
      "Epoch 23/100\n",
      "391/391 [==============================] - ETA: 0s - loss: 2.4924 - accuracy: 0.3517\n",
      "Epoch 00023: val_accuracy improved from 0.41120 to 0.43790, saving model to VGG_DropOut_SGD_weights_amit.hdf5\n",
      "391/391 [==============================] - 150s 383ms/step - loss: 2.4924 - accuracy: 0.3517 - val_loss: 2.1323 - val_accuracy: 0.4379\n",
      "Epoch 24/100\n",
      "391/391 [==============================] - ETA: 0s - loss: 2.4667 - accuracy: 0.3595\n",
      "Epoch 00024: val_accuracy did not improve from 0.43790\n",
      "391/391 [==============================] - 149s 381ms/step - loss: 2.4667 - accuracy: 0.3595 - val_loss: 2.1798 - val_accuracy: 0.4360\n",
      "Epoch 25/100\n",
      "391/391 [==============================] - ETA: 0s - loss: 2.4284 - accuracy: 0.3667\n",
      "Epoch 00025: val_accuracy improved from 0.43790 to 0.45270, saving model to VGG_DropOut_SGD_weights_amit.hdf5\n",
      "391/391 [==============================] - 150s 383ms/step - loss: 2.4284 - accuracy: 0.3667 - val_loss: 2.0493 - val_accuracy: 0.4527\n",
      "Epoch 26/100\n",
      "391/391 [==============================] - ETA: 0s - loss: 2.3820 - accuracy: 0.3754\n",
      "Epoch 00026: val_accuracy did not improve from 0.45270\n",
      "391/391 [==============================] - 149s 380ms/step - loss: 2.3820 - accuracy: 0.3754 - val_loss: 2.1615 - val_accuracy: 0.4385\n",
      "Epoch 27/100\n",
      "391/391 [==============================] - ETA: 0s - loss: 2.3480 - accuracy: 0.3843\n",
      "Epoch 00027: val_accuracy did not improve from 0.45270\n",
      "391/391 [==============================] - 149s 381ms/step - loss: 2.3480 - accuracy: 0.3843 - val_loss: 2.4602 - val_accuracy: 0.4126\n",
      "Epoch 28/100\n",
      "391/391 [==============================] - ETA: 0s - loss: 2.3265 - accuracy: 0.3907\n",
      "Epoch 00028: val_accuracy improved from 0.45270 to 0.46960, saving model to VGG_DropOut_SGD_weights_amit.hdf5\n",
      "391/391 [==============================] - 149s 382ms/step - loss: 2.3265 - accuracy: 0.3907 - val_loss: 2.0061 - val_accuracy: 0.4696\n",
      "Epoch 29/100\n",
      "391/391 [==============================] - ETA: 0s - loss: 2.2874 - accuracy: 0.3969\n",
      "Epoch 00029: val_accuracy improved from 0.46960 to 0.49500, saving model to VGG_DropOut_SGD_weights_amit.hdf5\n",
      "391/391 [==============================] - 150s 382ms/step - loss: 2.2874 - accuracy: 0.3969 - val_loss: 1.8438 - val_accuracy: 0.4950\n",
      "Epoch 30/100\n",
      "391/391 [==============================] - ETA: 0s - loss: 2.2474 - accuracy: 0.4065\n",
      "Epoch 00030: val_accuracy did not improve from 0.49500\n",
      "391/391 [==============================] - 149s 381ms/step - loss: 2.2474 - accuracy: 0.4065 - val_loss: 2.1641 - val_accuracy: 0.4617\n",
      "Epoch 31/100\n",
      "391/391 [==============================] - ETA: 0s - loss: 2.2126 - accuracy: 0.4146\n",
      "Epoch 00031: val_accuracy did not improve from 0.49500\n",
      "391/391 [==============================] - 150s 382ms/step - loss: 2.2126 - accuracy: 0.4146 - val_loss: 2.2324 - val_accuracy: 0.4448\n",
      "Epoch 32/100\n",
      "391/391 [==============================] - ETA: 0s - loss: 2.2046 - accuracy: 0.4155\n",
      "Epoch 00032: val_accuracy did not improve from 0.49500\n",
      "391/391 [==============================] - 149s 381ms/step - loss: 2.2046 - accuracy: 0.4155 - val_loss: 2.1762 - val_accuracy: 0.4431\n",
      "Epoch 33/100\n",
      "391/391 [==============================] - ETA: 0s - loss: 2.1722 - accuracy: 0.4215\n",
      "Epoch 00033: val_accuracy did not improve from 0.49500\n",
      "391/391 [==============================] - 149s 381ms/step - loss: 2.1722 - accuracy: 0.4215 - val_loss: 2.1686 - val_accuracy: 0.4551\n",
      "Epoch 34/100\n",
      "391/391 [==============================] - ETA: 0s - loss: 2.1432 - accuracy: 0.4327\n",
      "Epoch 00034: val_accuracy did not improve from 0.49500\n",
      "Restoring model weights from the end of the best epoch.\n",
      "391/391 [==============================] - 149s 382ms/step - loss: 2.1432 - accuracy: 0.4327 - val_loss: 2.1262 - val_accuracy: 0.4599\n",
      "Epoch 00034: early stopping\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.keras.callbacks.History at 0x7fe73de4e5f8>"
      ]
     },
     "execution_count": 3,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.fit(aug_data.flow(x_train, y_train, batch_size=128), batch_size=128, epochs=100, verbose=1, validation_data=(x_test, y_test),callbacks=[checkpoint,early])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 88
    },
    "id": "WDE9I1QvSJ7h",
    "outputId": "a583d25a-02b8-4bb3-a980-fc5f8f71531f"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From <ipython-input-5-7e95b17fe9a2>:1: Sequential.predict_classes (from tensorflow.python.keras.engine.sequential) is deprecated and will be removed after 2021-01-01.\n",
      "Instructions for updating:\n",
      "Please use instead:* `np.argmax(model.predict(x), axis=-1)`,   if your model does multi-class classification   (e.g. if it uses a `softmax` last-layer activation).* `(model.predict(x) > 0.5).astype(\"int32\")`,   if your model does binary classification   (e.g. if it uses a `sigmoid` last-layer activation).\n"
     ]
    }
   ],
   "source": [
    "y_pred=model.predict_classes(x_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "id": "ZyA634v4pUMi",
    "outputId": "a1936580-d20c-4ea8-e2fa-2335cdb73bd3"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.82      0.72      0.77       100\n",
      "           1       0.77      0.55      0.64       100\n",
      "           2       0.50      0.34      0.40       100\n",
      "           3       0.42      0.15      0.22       100\n",
      "           4       0.28      0.16      0.20       100\n",
      "           5       0.63      0.31      0.42       100\n",
      "           6       0.60      0.52      0.56       100\n",
      "           7       0.46      0.60      0.52       100\n",
      "           8       0.75      0.57      0.65       100\n",
      "           9       0.65      0.59      0.62       100\n",
      "          10       0.50      0.26      0.34       100\n",
      "          11       0.54      0.29      0.38       100\n",
      "          12       0.45      0.60      0.52       100\n",
      "          13       0.50      0.36      0.42       100\n",
      "          14       0.37      0.37      0.37       100\n",
      "          15       0.47      0.47      0.47       100\n",
      "          16       0.58      0.54      0.56       100\n",
      "          17       0.62      0.74      0.68       100\n",
      "          18       0.43      0.43      0.43       100\n",
      "          19       0.54      0.37      0.44       100\n",
      "          20       0.64      0.86      0.73       100\n",
      "          21       0.65      0.54      0.59       100\n",
      "          22       0.49      0.46      0.47       100\n",
      "          23       0.68      0.58      0.63       100\n",
      "          24       0.81      0.60      0.69       100\n",
      "          25       0.51      0.34      0.41       100\n",
      "          26       0.54      0.36      0.43       100\n",
      "          27       0.30      0.36      0.33       100\n",
      "          28       0.76      0.71      0.74       100\n",
      "          29       0.48      0.59      0.53       100\n",
      "          30       0.49      0.35      0.41       100\n",
      "          31       0.54      0.53      0.54       100\n",
      "          32       0.64      0.43      0.51       100\n",
      "          33       0.31      0.54      0.39       100\n",
      "          34       0.48      0.30      0.37       100\n",
      "          35       0.35      0.39      0.37       100\n",
      "          36       0.54      0.70      0.61       100\n",
      "          37       0.57      0.47      0.52       100\n",
      "          38       0.37      0.31      0.34       100\n",
      "          39       0.34      0.65      0.44       100\n",
      "          40       0.49      0.34      0.40       100\n",
      "          41       0.57      0.73      0.64       100\n",
      "          42       0.26      0.63      0.37       100\n",
      "          43       0.29      0.63      0.39       100\n",
      "          44       0.33      0.18      0.23       100\n",
      "          45       0.33      0.17      0.22       100\n",
      "          46       0.37      0.25      0.30       100\n",
      "          47       0.63      0.45      0.53       100\n",
      "          48       0.53      0.94      0.67       100\n",
      "          49       0.66      0.59      0.62       100\n",
      "          50       0.27      0.23      0.25       100\n",
      "          51       0.39      0.37      0.38       100\n",
      "          52       0.45      0.90      0.60       100\n",
      "          53       0.69      0.80      0.74       100\n",
      "          54       0.87      0.41      0.56       100\n",
      "          55       0.13      0.04      0.06       100\n",
      "          56       0.75      0.74      0.74       100\n",
      "          57       0.66      0.53      0.59       100\n",
      "          58       0.65      0.71      0.68       100\n",
      "          59       0.54      0.32      0.40       100\n",
      "          60       0.76      0.78      0.77       100\n",
      "          61       0.46      0.61      0.52       100\n",
      "          62       0.53      0.66      0.59       100\n",
      "          63       0.27      0.47      0.35       100\n",
      "          64       0.28      0.29      0.28       100\n",
      "          65       0.39      0.23      0.29       100\n",
      "          66       0.35      0.68      0.46       100\n",
      "          67       0.38      0.41      0.39       100\n",
      "          68       0.82      0.77      0.79       100\n",
      "          69       0.70      0.62      0.66       100\n",
      "          70       0.78      0.47      0.59       100\n",
      "          71       0.60      0.64      0.62       100\n",
      "          72       0.20      0.03      0.05       100\n",
      "          73       0.39      0.27      0.32       100\n",
      "          74       0.28      0.42      0.34       100\n",
      "          75       0.56      0.88      0.69       100\n",
      "          76       0.65      0.77      0.70       100\n",
      "          77       0.47      0.47      0.47       100\n",
      "          78       0.26      0.46      0.33       100\n",
      "          79       0.51      0.50      0.50       100\n",
      "          80       0.33      0.08      0.13       100\n",
      "          81       0.46      0.54      0.50       100\n",
      "          82       0.78      0.80      0.79       100\n",
      "          83       0.29      0.45      0.36       100\n",
      "          84       0.54      0.43      0.48       100\n",
      "          85       0.60      0.63      0.61       100\n",
      "          86       0.52      0.44      0.48       100\n",
      "          87       0.47      0.62      0.53       100\n",
      "          88       0.57      0.54      0.55       100\n",
      "          89       0.49      0.65      0.56       100\n",
      "          90       0.56      0.49      0.52       100\n",
      "          91       0.74      0.68      0.71       100\n",
      "          92       0.44      0.19      0.27       100\n",
      "          93       0.43      0.23      0.30       100\n",
      "          94       0.58      0.88      0.70       100\n",
      "          95       0.47      0.56      0.51       100\n",
      "          96       0.31      0.40      0.35       100\n",
      "          97       0.58      0.46      0.51       100\n",
      "          98       0.36      0.36      0.36       100\n",
      "          99       0.47      0.67      0.55       100\n",
      "\n",
      "    accuracy                           0.49     10000\n",
      "   macro avg       0.51      0.49      0.49     10000\n",
      "weighted avg       0.51      0.49      0.49     10000\n",
      "\n",
      "Accuracy is 0.495\n",
      "Precision is 0.5089995476200098\n",
      "Recall is 0.495\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import classification_report,accuracy_score,recall_score,precision_score\n",
    "print(classification_report(Y_test,y_pred))\n",
    "print(\"Accuracy is {}\".format(accuracy_score(Y_test,y_pred)))\n",
    "print(\"Precision is {}\".format(precision_score(Y_test,y_pred,average=\"weighted\")))\n",
    "print(\"Recall is {}\".format(recall_score(Y_test,y_pred,average=\"weighted\")))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From C:\\Users\\uamit\\Anaconda3\\lib\\site-packages\\keras\\backend\\tensorflow_backend.py:4070: The name tf.nn.max_pool is deprecated. Please use tf.nn.max_pool2d instead.\n",
      "\n",
      "WARNING:tensorflow:From C:\\Users\\uamit\\Anaconda3\\lib\\site-packages\\keras\\backend\\tensorflow_backend.py:422: The name tf.global_variables is deprecated. Please use tf.compat.v1.global_variables instead.\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.82      0.72      0.77       100\n",
      "           1       0.77      0.55      0.64       100\n",
      "           2       0.50      0.34      0.40       100\n",
      "           3       0.42      0.15      0.22       100\n",
      "           4       0.28      0.16      0.20       100\n",
      "           5       0.63      0.31      0.42       100\n",
      "           6       0.60      0.52      0.56       100\n",
      "           7       0.46      0.60      0.52       100\n",
      "           8       0.75      0.57      0.65       100\n",
      "           9       0.65      0.59      0.62       100\n",
      "          10       0.50      0.26      0.34       100\n",
      "          11       0.54      0.29      0.38       100\n",
      "          12       0.45      0.60      0.52       100\n",
      "          13       0.50      0.36      0.42       100\n",
      "          14       0.37      0.37      0.37       100\n",
      "          15       0.47      0.47      0.47       100\n",
      "          16       0.58      0.54      0.56       100\n",
      "          17       0.62      0.74      0.68       100\n",
      "          18       0.43      0.43      0.43       100\n",
      "          19       0.54      0.37      0.44       100\n",
      "          20       0.64      0.86      0.73       100\n",
      "          21       0.65      0.54      0.59       100\n",
      "          22       0.49      0.46      0.47       100\n",
      "          23       0.68      0.58      0.63       100\n",
      "          24       0.81      0.60      0.69       100\n",
      "          25       0.51      0.34      0.41       100\n",
      "          26       0.54      0.36      0.43       100\n",
      "          27       0.30      0.36      0.33       100\n",
      "          28       0.76      0.71      0.74       100\n",
      "          29       0.48      0.59      0.53       100\n",
      "          30       0.49      0.35      0.41       100\n",
      "          31       0.54      0.53      0.54       100\n",
      "          32       0.64      0.43      0.51       100\n",
      "          33       0.31      0.54      0.39       100\n",
      "          34       0.48      0.30      0.37       100\n",
      "          35       0.35      0.39      0.37       100\n",
      "          36       0.54      0.70      0.61       100\n",
      "          37       0.57      0.47      0.52       100\n",
      "          38       0.37      0.31      0.34       100\n",
      "          39       0.34      0.65      0.44       100\n",
      "          40       0.49      0.34      0.40       100\n",
      "          41       0.57      0.73      0.64       100\n",
      "          42       0.26      0.63      0.37       100\n",
      "          43       0.29      0.63      0.39       100\n",
      "          44       0.33      0.18      0.23       100\n",
      "          45       0.33      0.17      0.22       100\n",
      "          46       0.37      0.25      0.30       100\n",
      "          47       0.63      0.45      0.53       100\n",
      "          48       0.53      0.94      0.67       100\n",
      "          49       0.66      0.59      0.62       100\n",
      "          50       0.27      0.23      0.25       100\n",
      "          51       0.39      0.37      0.38       100\n",
      "          52       0.45      0.90      0.60       100\n",
      "          53       0.69      0.80      0.74       100\n",
      "          54       0.87      0.41      0.56       100\n",
      "          55       0.13      0.04      0.06       100\n",
      "          56       0.75      0.74      0.74       100\n",
      "          57       0.66      0.53      0.59       100\n",
      "          58       0.65      0.71      0.68       100\n",
      "          59       0.54      0.32      0.40       100\n",
      "          60       0.76      0.78      0.77       100\n",
      "          61       0.46      0.61      0.52       100\n",
      "          62       0.53      0.66      0.59       100\n",
      "          63       0.27      0.47      0.35       100\n",
      "          64       0.28      0.29      0.28       100\n",
      "          65       0.39      0.23      0.29       100\n",
      "          66       0.35      0.68      0.46       100\n",
      "          67       0.38      0.41      0.39       100\n",
      "          68       0.82      0.77      0.79       100\n",
      "          69       0.70      0.62      0.66       100\n",
      "          70       0.78      0.47      0.59       100\n",
      "          71       0.60      0.64      0.62       100\n",
      "          72       0.20      0.03      0.05       100\n",
      "          73       0.39      0.27      0.32       100\n",
      "          74       0.28      0.42      0.34       100\n",
      "          75       0.56      0.88      0.69       100\n",
      "          76       0.65      0.77      0.70       100\n",
      "          77       0.47      0.47      0.47       100\n",
      "          78       0.26      0.46      0.33       100\n",
      "          79       0.51      0.50      0.50       100\n",
      "          80       0.33      0.08      0.13       100\n",
      "          81       0.46      0.54      0.50       100\n",
      "          82       0.78      0.80      0.79       100\n",
      "          83       0.29      0.45      0.36       100\n",
      "          84       0.54      0.43      0.48       100\n",
      "          85       0.60      0.63      0.61       100\n",
      "          86       0.52      0.44      0.48       100\n",
      "          87       0.47      0.62      0.53       100\n",
      "          88       0.57      0.54      0.55       100\n",
      "          89       0.49      0.65      0.56       100\n",
      "          90       0.56      0.49      0.52       100\n",
      "          91       0.74      0.68      0.71       100\n",
      "          92       0.44      0.19      0.27       100\n",
      "          93       0.43      0.23      0.30       100\n",
      "          94       0.58      0.88      0.70       100\n",
      "          95       0.47      0.56      0.51       100\n",
      "          96       0.31      0.40      0.35       100\n",
      "          97       0.58      0.46      0.51       100\n",
      "          98       0.36      0.36      0.36       100\n",
      "          99       0.47      0.67      0.55       100\n",
      "\n",
      "    accuracy                           0.49     10000\n",
      "   macro avg       0.51      0.49      0.49     10000\n",
      "weighted avg       0.51      0.49      0.49     10000\n",
      "\n",
      "Accuracy is 0.495\n",
      "Precision is 0.5089995476200098\n",
      "Recall is 0.495\n"
     ]
    }
   ],
   "source": [
    "from keras.callbacks import ModelCheckpoint, EarlyStopping\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Conv2D, MaxPool2D, Flatten, Dense,BatchNormalization,Activation,Dropout,LeakyReLU\n",
    "from tensorflow.keras.optimizers import SGD\n",
    "from keras.preprocessing.image import ImageDataGenerator\n",
    "from keras.utils import to_categorical\n",
    "from tensorflow.keras.datasets import cifar100\n",
    "\n",
    "\n",
    "\n",
    "(x_train, Y_train), (x_test, Y_test) = cifar100.load_data()\n",
    "\n",
    "y_train = to_categorical(Y_train)\n",
    "y_test = to_categorical(Y_test)\n",
    "\n",
    "\n",
    "model = Sequential()\n",
    "model.add(Conv2D(filters= 64, kernel_size= (3,3), strides= (1,1), padding='same', input_shape= (32, 32, 3)))\n",
    "model.add(LeakyReLU(alpha=0.1))\n",
    "model.add(Conv2D(filters= 64, kernel_size= (3,3), strides= (1,1), padding='same'))\n",
    "model.add(LeakyReLU(alpha=0.1))\n",
    "model.add(MaxPool2D(pool_size= (2,2), strides=(1,1)))\n",
    "model.add(Dropout(0.5))\n",
    "\n",
    "# Creating second block- (2 Convolution + 1 Max pool)\n",
    "model.add(Conv2D(filters= 128, kernel_size= (3,3), strides= (1,1), padding='same'))\n",
    "model.add(LeakyReLU(alpha=0.1))\n",
    "model.add(Conv2D(filters= 128, kernel_size= (3,3), strides= (1,1), padding='same'))\n",
    "model.add(LeakyReLU(alpha=0.1))\n",
    "model.add(MaxPool2D(pool_size= (2,2), strides=(1,1)))\n",
    "model.add(Dropout(0.5))\n",
    "\n",
    "\n",
    "# Creating third block- (3 Convolution + 1 Max pool)\n",
    "model.add(Conv2D(filters= 256, kernel_size= (3,3), strides= (1,1), padding='same'))\n",
    "model.add(LeakyReLU(alpha=0.1))\n",
    "model.add(Conv2D(filters= 256, kernel_size= (3,3), strides= (1,1), padding='same'))\n",
    "model.add(LeakyReLU(alpha=0.1))\n",
    "model.add(Conv2D(filters= 256, kernel_size= (3,3), strides= (1,1), padding='same'))\n",
    "model.add(LeakyReLU(alpha=0.1))\n",
    "model.add(MaxPool2D(pool_size= (2,2), strides=(1,1)))\n",
    "model.add(Dropout(0.5))\n",
    "\n",
    "model.add(Conv2D(filters= 512, kernel_size= (3,3), strides= (1,1), padding='same'))\n",
    "model.add(LeakyReLU(alpha=0.1))\n",
    "model.add(Conv2D(filters= 512, kernel_size= (3,3), strides= (1,1), padding='same'))\n",
    "model.add(LeakyReLU(alpha=0.1))\n",
    "model.add(Conv2D(filters= 512, kernel_size= (3,3), strides= (1,1), padding='same'))\n",
    "model.add(LeakyReLU(alpha=0.1))\n",
    "model.add(MaxPool2D(pool_size= (2,2), strides=(2,2)))\n",
    "model.add(Dropout(0.5))\n",
    "\n",
    "model.add(Conv2D(filters= 512, kernel_size= (3,3), strides= (1,1), padding='same'))\n",
    "model.add(LeakyReLU(alpha=0.1))\n",
    "model.add(Conv2D(filters= 512, kernel_size= (3,3), strides= (1,1), padding='same'))\n",
    "model.add(LeakyReLU(alpha=0.1))\n",
    "model.add(Conv2D(filters= 512, kernel_size= (3,3), strides= (1,1), padding='same'))\n",
    "model.add(LeakyReLU(alpha=0.1))\n",
    "model.add(MaxPool2D(pool_size= (2,2), strides=(1,1)))\n",
    "model.add(Dropout(0.5))\n",
    "\n",
    "# Flattening the pooled image pixels\n",
    "model.add(Flatten())\n",
    "\n",
    "model.add(Dense(units= 512))\n",
    "model.add(LeakyReLU(alpha=0.1))\n",
    "model.add(Dropout(0.5))\n",
    "\n",
    "model.add(Dropout(0.5))\n",
    "model.add(Dense(units= 100, activation='softmax'))\n",
    "\n",
    "model.load_weights(\"VGG_DropOut_SGD_weights_amit.hdf5\")\n",
    "\n",
    "y_pred=model.predict_classes(x_test)\n",
    "\n",
    "from sklearn.metrics import classification_report,accuracy_score,recall_score,precision_score\n",
    "print(classification_report(Y_test,y_pred))\n",
    "print(\"Accuracy is {}\".format(accuracy_score(Y_test,y_pred)))\n",
    "print(\"Precision is {}\".format(precision_score(Y_test,y_pred,average=\"weighted\")))\n",
    "print(\"Recall is {}\".format(recall_score(Y_test,y_pred,average=\"weighted\")))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From C:\\Users\\uamit\\Anaconda3\\lib\\site-packages\\keras\\backend\\tensorflow_backend.py:422: The name tf.global_variables is deprecated. Please use tf.compat.v1.global_variables instead.\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.82      0.72      0.77       100\n",
      "           1       0.77      0.55      0.64       100\n",
      "           2       0.50      0.34      0.40       100\n",
      "           3       0.42      0.15      0.22       100\n",
      "           4       0.28      0.16      0.20       100\n",
      "           5       0.63      0.31      0.42       100\n",
      "           6       0.60      0.52      0.56       100\n",
      "           7       0.46      0.60      0.52       100\n",
      "           8       0.75      0.57      0.65       100\n",
      "           9       0.65      0.59      0.62       100\n",
      "          10       0.50      0.26      0.34       100\n",
      "          11       0.54      0.29      0.38       100\n",
      "          12       0.45      0.60      0.52       100\n",
      "          13       0.50      0.36      0.42       100\n",
      "          14       0.37      0.37      0.37       100\n",
      "          15       0.47      0.47      0.47       100\n",
      "          16       0.58      0.54      0.56       100\n",
      "          17       0.62      0.74      0.68       100\n",
      "          18       0.43      0.43      0.43       100\n",
      "          19       0.54      0.37      0.44       100\n",
      "          20       0.64      0.86      0.73       100\n",
      "          21       0.65      0.54      0.59       100\n",
      "          22       0.49      0.46      0.47       100\n",
      "          23       0.68      0.58      0.63       100\n",
      "          24       0.81      0.60      0.69       100\n",
      "          25       0.51      0.34      0.41       100\n",
      "          26       0.54      0.36      0.43       100\n",
      "          27       0.30      0.36      0.33       100\n",
      "          28       0.76      0.71      0.74       100\n",
      "          29       0.48      0.59      0.53       100\n",
      "          30       0.49      0.35      0.41       100\n",
      "          31       0.54      0.53      0.54       100\n",
      "          32       0.64      0.43      0.51       100\n",
      "          33       0.31      0.54      0.39       100\n",
      "          34       0.48      0.30      0.37       100\n",
      "          35       0.35      0.39      0.37       100\n",
      "          36       0.54      0.70      0.61       100\n",
      "          37       0.57      0.47      0.52       100\n",
      "          38       0.37      0.31      0.34       100\n",
      "          39       0.34      0.65      0.44       100\n",
      "          40       0.49      0.34      0.40       100\n",
      "          41       0.57      0.73      0.64       100\n",
      "          42       0.26      0.63      0.37       100\n",
      "          43       0.29      0.63      0.39       100\n",
      "          44       0.33      0.18      0.23       100\n",
      "          45       0.33      0.17      0.22       100\n",
      "          46       0.37      0.25      0.30       100\n",
      "          47       0.63      0.45      0.53       100\n",
      "          48       0.53      0.94      0.67       100\n",
      "          49       0.66      0.59      0.62       100\n",
      "          50       0.27      0.23      0.25       100\n",
      "          51       0.39      0.37      0.38       100\n",
      "          52       0.45      0.90      0.60       100\n",
      "          53       0.69      0.80      0.74       100\n",
      "          54       0.87      0.41      0.56       100\n",
      "          55       0.13      0.04      0.06       100\n",
      "          56       0.75      0.74      0.74       100\n",
      "          57       0.66      0.53      0.59       100\n",
      "          58       0.65      0.71      0.68       100\n",
      "          59       0.54      0.32      0.40       100\n",
      "          60       0.76      0.78      0.77       100\n",
      "          61       0.46      0.61      0.52       100\n",
      "          62       0.53      0.66      0.59       100\n",
      "          63       0.27      0.47      0.35       100\n",
      "          64       0.28      0.29      0.28       100\n",
      "          65       0.39      0.23      0.29       100\n",
      "          66       0.35      0.68      0.46       100\n",
      "          67       0.38      0.41      0.39       100\n",
      "          68       0.82      0.77      0.79       100\n",
      "          69       0.70      0.62      0.66       100\n",
      "          70       0.78      0.47      0.59       100\n",
      "          71       0.60      0.64      0.62       100\n",
      "          72       0.20      0.03      0.05       100\n",
      "          73       0.39      0.27      0.32       100\n",
      "          74       0.28      0.42      0.34       100\n",
      "          75       0.56      0.88      0.69       100\n",
      "          76       0.65      0.77      0.70       100\n",
      "          77       0.47      0.47      0.47       100\n",
      "          78       0.26      0.46      0.33       100\n",
      "          79       0.51      0.50      0.50       100\n",
      "          80       0.33      0.08      0.13       100\n",
      "          81       0.46      0.54      0.50       100\n",
      "          82       0.78      0.80      0.79       100\n",
      "          83       0.29      0.45      0.36       100\n",
      "          84       0.54      0.43      0.48       100\n",
      "          85       0.60      0.63      0.61       100\n",
      "          86       0.52      0.44      0.48       100\n",
      "          87       0.47      0.62      0.53       100\n",
      "          88       0.57      0.54      0.55       100\n",
      "          89       0.49      0.65      0.56       100\n",
      "          90       0.56      0.49      0.52       100\n",
      "          91       0.74      0.68      0.71       100\n",
      "          92       0.44      0.19      0.27       100\n",
      "          93       0.43      0.23      0.30       100\n",
      "          94       0.58      0.88      0.70       100\n",
      "          95       0.47      0.56      0.51       100\n",
      "          96       0.31      0.40      0.35       100\n",
      "          97       0.58      0.46      0.51       100\n",
      "          98       0.36      0.36      0.36       100\n",
      "          99       0.47      0.67      0.55       100\n",
      "\n",
      "    accuracy                           0.49     10000\n",
      "   macro avg       0.51      0.49      0.49     10000\n",
      "weighted avg       0.51      0.49      0.49     10000\n",
      "\n",
      "Accuracy is 0.495\n",
      "Precision is 0.5089995476200098\n",
      "Recall is 0.495\n"
     ]
    }
   ],
   "source": [
    "from keras.callbacks import ModelCheckpoint, EarlyStopping\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Conv2D, MaxPool2D, Flatten, Dense,BatchNormalization,Activation,Dropout,LeakyReLU\n",
    "from tensorflow.keras.optimizers import SGD\n",
    "from keras.preprocessing.image import ImageDataGenerator\n",
    "from keras.utils import to_categorical\n",
    "from tensorflow.keras.datasets import cifar100\n",
    "\n",
    "\n",
    "\n",
    "(x_train, Y_train), (x_test, Y_test) = cifar100.load_data()\n",
    "\n",
    "y_train = to_categorical(Y_train)\n",
    "y_test = to_categorical(Y_test)\n",
    "\n",
    "\n",
    "model = Sequential()\n",
    "model.add(Conv2D(filters= 64, kernel_size= (3,3), strides= (1,1), padding='same', input_shape= (32, 32, 3)))\n",
    "model.add(LeakyReLU(alpha=0.1))\n",
    "model.add(Conv2D(filters= 64, kernel_size= (3,3), strides= (1,1), padding='same'))\n",
    "model.add(LeakyReLU(alpha=0.1))\n",
    "model.add(MaxPool2D(pool_size= (2,2), strides=(1,1)))\n",
    "model.add(Dropout(0.5))\n",
    "\n",
    "# Creating second block- (2 Convolution + 1 Max pool)\n",
    "model.add(Conv2D(filters= 128, kernel_size= (3,3), strides= (1,1), padding='same'))\n",
    "model.add(LeakyReLU(alpha=0.1))\n",
    "model.add(Conv2D(filters= 128, kernel_size= (3,3), strides= (1,1), padding='same'))\n",
    "model.add(LeakyReLU(alpha=0.1))\n",
    "model.add(MaxPool2D(pool_size= (2,2), strides=(1,1)))\n",
    "model.add(Dropout(0.5))\n",
    "\n",
    "\n",
    "# Creating third block- (3 Convolution + 1 Max pool)\n",
    "model.add(Conv2D(filters= 256, kernel_size= (3,3), strides= (1,1), padding='same'))\n",
    "model.add(LeakyReLU(alpha=0.1))\n",
    "model.add(Conv2D(filters= 256, kernel_size= (3,3), strides= (1,1), padding='same'))\n",
    "model.add(LeakyReLU(alpha=0.1))\n",
    "model.add(Conv2D(filters= 256, kernel_size= (3,3), strides= (1,1), padding='same'))\n",
    "model.add(LeakyReLU(alpha=0.1))\n",
    "model.add(MaxPool2D(pool_size= (2,2), strides=(1,1)))\n",
    "model.add(Dropout(0.5))\n",
    "\n",
    "model.add(Conv2D(filters= 512, kernel_size= (3,3), strides= (1,1), padding='same'))\n",
    "model.add(LeakyReLU(alpha=0.1))\n",
    "model.add(Conv2D(filters= 512, kernel_size= (3,3), strides= (1,1), padding='same'))\n",
    "model.add(LeakyReLU(alpha=0.1))\n",
    "model.add(Conv2D(filters= 512, kernel_size= (3,3), strides= (1,1), padding='same'))\n",
    "model.add(LeakyReLU(alpha=0.1))\n",
    "model.add(MaxPool2D(pool_size= (2,2), strides=(2,2)))\n",
    "model.add(Dropout(0.5))\n",
    "\n",
    "model.add(Conv2D(filters= 512, kernel_size= (3,3), strides= (1,1), padding='same'))\n",
    "model.add(LeakyReLU(alpha=0.1))\n",
    "model.add(Conv2D(filters= 512, kernel_size= (3,3), strides= (1,1), padding='same'))\n",
    "model.add(LeakyReLU(alpha=0.1))\n",
    "model.add(Conv2D(filters= 512, kernel_size= (3,3), strides= (1,1), padding='same'))\n",
    "model.add(LeakyReLU(alpha=0.1))\n",
    "model.add(MaxPool2D(pool_size= (2,2), strides=(1,1)))\n",
    "model.add(Dropout(0.5))\n",
    "\n",
    "# Flattening the pooled image pixels\n",
    "model.add(Flatten())\n",
    "\n",
    "model.add(Dense(units= 512))\n",
    "model.add(LeakyReLU(alpha=0.1))\n",
    "model.add(Dropout(0.5))\n",
    "\n",
    "model.add(Dropout(0.5))\n",
    "model.add(Dense(units= 100, activation='softmax'))\n",
    "\n",
    "model.load_weights(\"../weights/VGG_DropOut_SGD_weights_amit.hdf5\")\n",
    "\n",
    "y_true = y_test.argmax(-1)\n",
    "y_pred = model.predict(x_test).argmax(-1)\n",
    "\n",
    "from sklearn.metrics import classification_report,accuracy_score,recall_score,precision_score\n",
    "print(classification_report(Y_test,y_pred))\n",
    "print(\"Accuracy is {}\".format(accuracy_score(Y_test,y_pred)))\n",
    "print(\"Precision is {}\".format(precision_score(Y_test,y_pred,average=\"weighted\")))\n",
    "print(\"Recall is {}\".format(recall_score(Y_test,y_pred,average=\"weighted\")))"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "name": "VGG_DropOut_SGD_weights_amit.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
