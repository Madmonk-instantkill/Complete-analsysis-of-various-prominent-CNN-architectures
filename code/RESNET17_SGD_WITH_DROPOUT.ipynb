{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "FB6LG2oJHKs4"
   },
   "outputs": [],
   "source": [
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras.layers import Conv2D,Dense,MaxPool2D,AveragePooling2D,Flatten,Dropout\n",
    "import tensorflow.keras as keras\n",
    "from tensorflow.keras.datasets import cifar100\n",
    "from tensorflow.keras.utils import to_categorical\n",
    "from tensorflow.keras import layers\n",
    "\n",
    "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
    "from tensorflow.keras.optimizers import SGD\n",
    "\n",
    "from tensorflow.keras.callbacks import EarlyStopping,ModelCheckpoint\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 51
    },
    "id": "SZMyBKb9HaIp",
    "outputId": "9c06ace3-77a6-4fa6-9cb2-a47feb2a6665"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading data from https://www.cs.toronto.edu/~kriz/cifar-100-python.tar.gz\n",
      "169009152/169001437 [==============================] - 12s 0us/step\n"
     ]
    }
   ],
   "source": [
    "(X_train,Y_train),(X_test,Y_test)=cifar100.load_data()\n",
    "\n",
    "X_train,X_test=X_train/255,X_test/255\n",
    "y_train,y_test=to_categorical(Y_train),to_categorical(Y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "pGRJHApkHfr5"
   },
   "outputs": [],
   "source": [
    "aug_data=ImageDataGenerator(rotation_range=15,horizontal_flip=True,width_shift_range=0.1,height_shift_range=0.1,zoom_range=0.3)\n",
    "aug_data.fit(X_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "PlNTlQ0PHn55"
   },
   "outputs": [],
   "source": [
    "shape=(32,32,3)\n",
    "\n",
    "inputs = keras.Input(shape=shape)\n",
    "\n",
    "a=layers.Conv2D(32,(3,3),strides=1,padding=\"same\",activation=\"relu\")(inputs)  #1\n",
    "a=layers.Dropout(0.2)(a)\n",
    "a=layers.Conv2D(32,(3,3),strides=1,padding=\"same\",activation=\"relu\")(a) \n",
    "a=layers.Dropout(0.2)(a)  #2\n",
    "a_temp=a\n",
    "a=layers.Conv2D(32,(3,3),strides=1,padding=\"same\",activation=\"relu\")(a)#\\3\n",
    "a=layers.Dropout(0.2)(a)\n",
    "a=layers.Conv2D(32,(3,3),strides=1,padding=\"same\")(a)#4\n",
    "a=layers.Dropout(0.2)(a)\n",
    "a=layers.Add()([a,a_temp])\n",
    "a=layers.Activation(\"relu\")(a)\n",
    "\n",
    "b=layers.Conv2D(64,(3,3),strides=1,padding=\"same\",activation=\"relu\")(a) #1\n",
    "                               \n",
    "#a_temp=a\n",
    "##########################################################################\n",
    "a=layers.Conv2D(64,(3,3),strides=1,padding=\"same\",activation=\"relu\")(a) #5\n",
    "a=layers.Dropout(0.2)(a)\n",
    "a=layers.Conv2D(64,(3,3),strides=1,padding=\"same\")(a) #6\n",
    "a=layers.Dropout(0.2)(a)\n",
    "a=layers.Add()([b,a])\n",
    "a=layers.Activation(\"relu\")(a)\n",
    "a_temp=a\n",
    "a=layers.Conv2D(64,(3,3),strides=1,padding=\"same\",activation=\"relu\")(a) #7\n",
    "a=layers.Dropout(0.2)(a)\n",
    "a=layers.Conv2D(64,(3,3),strides=1,padding=\"same\")(a) #8\n",
    "a=layers.Dropout(0.2)(a)\n",
    "a=layers.Add()([a,a_temp])\n",
    "a=layers.Activation(\"relu\")(a)\n",
    "\n",
    "b=layers.Conv2D(128,(3,3),strides=1,padding=\"same\",activation=\"relu\")(a) #2\n",
    "\n",
    "###############################################################################\n",
    "\n",
    "a=layers.Conv2D(128,(3,3),strides=1,padding=\"same\",activation=\"relu\")(a) #9\n",
    "a=layers.Dropout(0.2)(a)\n",
    "a=layers.Conv2D(128,(3,3),strides=1,padding=\"same\")(a) #10\n",
    "a=layers.Dropout(0.2)(a)\n",
    "a=layers.Add()([b,a])\n",
    "a=layers.Activation(\"relu\")(a)\n",
    "a_temp=a\n",
    "a=layers.Conv2D(128,(3,3),strides=1,padding=\"same\",activation=\"relu\")(a) #11\n",
    "a=layers.Dropout(0.2)(a)\n",
    "a=layers.Conv2D(128,(3,3),strides=1,padding=\"same\")(a) #12\n",
    "a=layers.Dropout(0.2)(a)\n",
    "a=layers.Add()([a,a_temp])\n",
    "a=layers.Activation(\"relu\")(a)\n",
    "\n",
    "b=layers.Conv2D(256,(3,3),strides=1,padding=\"same\",activation=\"relu\")(a) #3\n",
    "\n",
    "#################################################################################\n",
    "\n",
    "a=layers.Conv2D(256,(3,3),strides=1,padding=\"same\",activation=\"relu\")(a) #13\n",
    "a=layers.Dropout(0.2)(a)\n",
    "a=layers.Conv2D(256,(3,3),strides=1,padding=\"same\")(a) #14\n",
    "a=layers.Dropout(0.2)(a)\n",
    "a=layers.Add()([b,a])\n",
    "a=layers.Activation(\"relu\")(a)\n",
    "a_temp=a\n",
    "a=layers.Conv2D(256,(3,3),strides=1,padding=\"same\",activation=\"relu\")(a) #15\n",
    "a=layers.Dropout(0.2)(a)\n",
    "a=layers.Conv2D(256,(3,3),strides=1,padding=\"same\")(a) #16\n",
    "a=layers.Dropout(0.2)(a)\n",
    "a=layers.Add()([a,a_temp])\n",
    "a=layers.Activation(\"relu\")(a)\n",
    "############################################################3\n",
    "a=layers.AveragePooling2D(pool_size=(2,2),strides=1,padding=\"same\")(a)\n",
    "\n",
    "a=Flatten()(a)\n",
    "\n",
    "#############################################################\n",
    "\n",
    "a=layers.Dense(512,activation=\"relu\")(a)\n",
    "a=layers.Dropout(0.2)(a)\n",
    "outputs=layers.Dense(y_train.shape[1],activation=\"softmax\")(a)\n",
    "\n",
    "\n",
    "model = keras.Model(inputs, outputs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "id": "_fQt9HADHzfV",
    "outputId": "ff63414b-dafe-4823-dafa-b8e9a619ceda"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:`period` argument is deprecated. Please use `save_freq` to specify the frequency in number of batches seen.\n"
     ]
    }
   ],
   "source": [
    "sgd = SGD(lr=0.01, momentum=0.91,clipvalue=0.6)\n",
    "\n",
    "\n",
    "model.compile(loss=\"categorical_crossentropy\",optimizer=sgd,metrics=[\"accuracy\"])\n",
    "call=EarlyStopping(monitor=\"val_loss\",verbose=1,mode=\"auto\",patience=5,restore_best_weights=True)\n",
    "checkpoint=ModelCheckpoint('RESNET_SGD_WITH_DROPOUT_REGULAIZATION.hdf5',monitor='val_accuracy',verbose=1,save_best_only=True,save_weights_only=True,model='auto',period=1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "background_save": true,
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "id": "m6xfr9oSH0IC",
    "outputId": "8b645f78-6cd7-4892-9494-1ce0698bff1c"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From <ipython-input-7-c77f76be1324>:1: Model.fit_generator (from tensorflow.python.keras.engine.training) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please use Model.fit, which supports generators.\n",
      "Epoch 1/80\n",
      "  2/390 [..............................] - ETA: 2:47 - loss: 4.6180 - accuracy: 0.0039WARNING:tensorflow:Callbacks method `on_train_batch_end` is slow compared to the batch time (batch time: 0.3387s vs `on_train_batch_end` time: 0.5213s). Check your callbacks.\n",
      "391/390 [==============================] - ETA: 0s - loss: 4.3695 - accuracy: 0.0395\n",
      "Epoch 00001: val_accuracy improved from -inf to 0.07770, saving model to RESNET_SGD_WITH_DROPOUT_REGULAIZATION.hdf5\n",
      "391/390 [==============================] - 371s 950ms/step - loss: 4.3695 - accuracy: 0.0395 - val_loss: 4.1180 - val_accuracy: 0.0777\n",
      "Epoch 2/80\n",
      "391/390 [==============================] - ETA: 0s - loss: 4.0191 - accuracy: 0.0837\n",
      "Epoch 00002: val_accuracy improved from 0.07770 to 0.13920, saving model to RESNET_SGD_WITH_DROPOUT_REGULAIZATION.hdf5\n",
      "391/390 [==============================] - 367s 939ms/step - loss: 4.0191 - accuracy: 0.0837 - val_loss: 3.6849 - val_accuracy: 0.1392\n",
      "Epoch 3/80\n",
      "391/390 [==============================] - ETA: 0s - loss: 3.7202 - accuracy: 0.1286\n",
      "Epoch 00003: val_accuracy improved from 0.13920 to 0.15950, saving model to RESNET_SGD_WITH_DROPOUT_REGULAIZATION.hdf5\n",
      "391/390 [==============================] - 366s 936ms/step - loss: 3.7202 - accuracy: 0.1286 - val_loss: 3.5793 - val_accuracy: 0.1595\n",
      "Epoch 4/80\n",
      "391/390 [==============================] - ETA: 0s - loss: 3.5056 - accuracy: 0.1652\n",
      "Epoch 00004: val_accuracy improved from 0.15950 to 0.22180, saving model to RESNET_SGD_WITH_DROPOUT_REGULAIZATION.hdf5\n",
      "391/390 [==============================] - 366s 936ms/step - loss: 3.5056 - accuracy: 0.1652 - val_loss: 3.2645 - val_accuracy: 0.2218\n",
      "Epoch 5/80\n",
      "391/390 [==============================] - ETA: 0s - loss: 3.3119 - accuracy: 0.1972\n",
      "Epoch 00005: val_accuracy improved from 0.22180 to 0.24400, saving model to RESNET_SGD_WITH_DROPOUT_REGULAIZATION.hdf5\n",
      "391/390 [==============================] - 366s 937ms/step - loss: 3.3119 - accuracy: 0.1972 - val_loss: 3.1256 - val_accuracy: 0.2440\n",
      "Epoch 6/80\n",
      "391/390 [==============================] - ETA: 0s - loss: 3.1334 - accuracy: 0.2330\n",
      "Epoch 00006: val_accuracy improved from 0.24400 to 0.28430, saving model to RESNET_SGD_WITH_DROPOUT_REGULAIZATION.hdf5\n",
      "391/390 [==============================] - 362s 925ms/step - loss: 3.1334 - accuracy: 0.2330 - val_loss: 2.8779 - val_accuracy: 0.2843\n",
      "Epoch 7/80\n",
      "391/390 [==============================] - ETA: 0s - loss: 2.9850 - accuracy: 0.2620\n",
      "Epoch 00007: val_accuracy improved from 0.28430 to 0.32350, saving model to RESNET_SGD_WITH_DROPOUT_REGULAIZATION.hdf5\n",
      "391/390 [==============================] - 362s 926ms/step - loss: 2.9850 - accuracy: 0.2620 - val_loss: 2.6725 - val_accuracy: 0.3235\n",
      "Epoch 8/80\n",
      "391/390 [==============================] - ETA: 0s - loss: 2.8505 - accuracy: 0.2889\n",
      "Epoch 00008: val_accuracy improved from 0.32350 to 0.35100, saving model to RESNET_SGD_WITH_DROPOUT_REGULAIZATION.hdf5\n",
      "391/390 [==============================] - 365s 933ms/step - loss: 2.8505 - accuracy: 0.2889 - val_loss: 2.5350 - val_accuracy: 0.3510\n",
      "Epoch 9/80\n",
      "391/390 [==============================] - ETA: 0s - loss: 2.7523 - accuracy: 0.3085\n",
      "Epoch 00009: val_accuracy improved from 0.35100 to 0.36980, saving model to RESNET_SGD_WITH_DROPOUT_REGULAIZATION.hdf5\n",
      "391/390 [==============================] - 364s 931ms/step - loss: 2.7523 - accuracy: 0.3085 - val_loss: 2.4807 - val_accuracy: 0.3698\n",
      "Epoch 10/80\n",
      "391/390 [==============================] - ETA: 0s - loss: 2.6393 - accuracy: 0.3305\n",
      "Epoch 00010: val_accuracy improved from 0.36980 to 0.38490, saving model to RESNET_SGD_WITH_DROPOUT_REGULAIZATION.hdf5\n",
      "391/390 [==============================] - 360s 920ms/step - loss: 2.6393 - accuracy: 0.3305 - val_loss: 2.4049 - val_accuracy: 0.3849\n",
      "Epoch 11/80\n",
      "391/390 [==============================] - ETA: 0s - loss: 2.5603 - accuracy: 0.3475\n",
      "Epoch 00011: val_accuracy improved from 0.38490 to 0.40580, saving model to RESNET_SGD_WITH_DROPOUT_REGULAIZATION.hdf5\n",
      "391/390 [==============================] - 361s 923ms/step - loss: 2.5603 - accuracy: 0.3475 - val_loss: 2.2904 - val_accuracy: 0.4058\n",
      "Epoch 12/80\n",
      "391/390 [==============================] - ETA: 0s - loss: 2.4673 - accuracy: 0.3669\n",
      "Epoch 00012: val_accuracy improved from 0.40580 to 0.42990, saving model to RESNET_SGD_WITH_DROPOUT_REGULAIZATION.hdf5\n",
      "391/390 [==============================] - 362s 925ms/step - loss: 2.4673 - accuracy: 0.3669 - val_loss: 2.2145 - val_accuracy: 0.4299\n",
      "Epoch 13/80\n",
      "391/390 [==============================] - ETA: 0s - loss: 2.4041 - accuracy: 0.3767\n",
      "Epoch 00013: val_accuracy improved from 0.42990 to 0.43230, saving model to RESNET_SGD_WITH_DROPOUT_REGULAIZATION.hdf5\n",
      "391/390 [==============================] - 362s 926ms/step - loss: 2.4041 - accuracy: 0.3767 - val_loss: 2.1494 - val_accuracy: 0.4323\n",
      "Epoch 14/80\n",
      "391/390 [==============================] - ETA: 0s - loss: 2.3353 - accuracy: 0.3899\n",
      "Epoch 00014: val_accuracy improved from 0.43230 to 0.45780, saving model to RESNET_SGD_WITH_DROPOUT_REGULAIZATION.hdf5\n",
      "391/390 [==============================] - 363s 928ms/step - loss: 2.3353 - accuracy: 0.3899 - val_loss: 2.0805 - val_accuracy: 0.4578\n",
      "Epoch 15/80\n",
      "391/390 [==============================] - ETA: 0s - loss: 2.2573 - accuracy: 0.4080\n",
      "Epoch 00015: val_accuracy did not improve from 0.45780\n",
      "391/390 [==============================] - 358s 916ms/step - loss: 2.2573 - accuracy: 0.4080 - val_loss: 2.0817 - val_accuracy: 0.4563\n",
      "Epoch 16/80\n",
      "391/390 [==============================] - ETA: 0s - loss: 2.2083 - accuracy: 0.4195\n",
      "Epoch 00016: val_accuracy improved from 0.45780 to 0.46510, saving model to RESNET_SGD_WITH_DROPOUT_REGULAIZATION.hdf5\n",
      "391/390 [==============================] - 364s 931ms/step - loss: 2.2083 - accuracy: 0.4195 - val_loss: 2.0220 - val_accuracy: 0.4651\n",
      "Epoch 17/80\n",
      "391/390 [==============================] - ETA: 0s - loss: 2.1534 - accuracy: 0.4294\n",
      "Epoch 00017: val_accuracy improved from 0.46510 to 0.47560, saving model to RESNET_SGD_WITH_DROPOUT_REGULAIZATION.hdf5\n",
      "391/390 [==============================] - 363s 928ms/step - loss: 2.1534 - accuracy: 0.4294 - val_loss: 1.9663 - val_accuracy: 0.4756\n",
      "Epoch 18/80\n",
      "391/390 [==============================] - ETA: 0s - loss: 2.0964 - accuracy: 0.4425\n",
      "Epoch 00018: val_accuracy improved from 0.47560 to 0.48460, saving model to RESNET_SGD_WITH_DROPOUT_REGULAIZATION.hdf5\n",
      "391/390 [==============================] - 364s 930ms/step - loss: 2.0964 - accuracy: 0.4425 - val_loss: 1.9653 - val_accuracy: 0.4846\n",
      "Epoch 19/80\n",
      "391/390 [==============================] - ETA: 0s - loss: 2.0452 - accuracy: 0.4574\n",
      "Epoch 00019: val_accuracy improved from 0.48460 to 0.48800, saving model to RESNET_SGD_WITH_DROPOUT_REGULAIZATION.hdf5\n",
      "391/390 [==============================] - 363s 930ms/step - loss: 2.0452 - accuracy: 0.4574 - val_loss: 1.9192 - val_accuracy: 0.4880\n",
      "Epoch 20/80\n",
      "391/390 [==============================] - ETA: 0s - loss: 1.9997 - accuracy: 0.4652\n",
      "Epoch 00020: val_accuracy improved from 0.48800 to 0.50000, saving model to RESNET_SGD_WITH_DROPOUT_REGULAIZATION.hdf5\n",
      "391/390 [==============================] - 364s 931ms/step - loss: 1.9997 - accuracy: 0.4652 - val_loss: 1.8736 - val_accuracy: 0.5000\n",
      "Epoch 21/80\n",
      "391/390 [==============================] - ETA: 0s - loss: 1.9518 - accuracy: 0.4767\n",
      "Epoch 00021: val_accuracy did not improve from 0.50000\n",
      "391/390 [==============================] - 362s 925ms/step - loss: 1.9518 - accuracy: 0.4767 - val_loss: 1.9109 - val_accuracy: 0.4885\n",
      "Epoch 22/80\n",
      "391/390 [==============================] - ETA: 0s - loss: 1.9113 - accuracy: 0.4850\n",
      "Epoch 00022: val_accuracy did not improve from 0.50000\n",
      "391/390 [==============================] - 361s 924ms/step - loss: 1.9113 - accuracy: 0.4850 - val_loss: 1.9304 - val_accuracy: 0.4904\n",
      "Epoch 23/80\n",
      "391/390 [==============================] - ETA: 0s - loss: 1.8714 - accuracy: 0.4946\n",
      "Epoch 00023: val_accuracy did not improve from 0.50000\n",
      "391/390 [==============================] - 361s 924ms/step - loss: 1.8714 - accuracy: 0.4946 - val_loss: 1.9270 - val_accuracy: 0.4913\n",
      "Epoch 24/80\n",
      "391/390 [==============================] - ETA: 0s - loss: 1.8245 - accuracy: 0.5045\n",
      "Epoch 00024: val_accuracy improved from 0.50000 to 0.51550, saving model to RESNET_SGD_WITH_DROPOUT_REGULAIZATION.hdf5\n",
      "391/390 [==============================] - 363s 928ms/step - loss: 1.8245 - accuracy: 0.5045 - val_loss: 1.8138 - val_accuracy: 0.5155\n",
      "Epoch 25/80\n",
      "391/390 [==============================] - ETA: 0s - loss: 1.7925 - accuracy: 0.5134\n",
      "Epoch 00025: val_accuracy improved from 0.51550 to 0.51630, saving model to RESNET_SGD_WITH_DROPOUT_REGULAIZATION.hdf5\n",
      "391/390 [==============================] - 363s 929ms/step - loss: 1.7925 - accuracy: 0.5134 - val_loss: 1.8235 - val_accuracy: 0.5163\n",
      "Epoch 26/80\n",
      "391/390 [==============================] - ETA: 0s - loss: 1.7630 - accuracy: 0.5197\n",
      "Epoch 00026: val_accuracy improved from 0.51630 to 0.51850, saving model to RESNET_SGD_WITH_DROPOUT_REGULAIZATION.hdf5\n",
      "391/390 [==============================] - 363s 929ms/step - loss: 1.7630 - accuracy: 0.5197 - val_loss: 1.7975 - val_accuracy: 0.5185\n",
      "Epoch 27/80\n",
      "391/390 [==============================] - ETA: 0s - loss: 1.7162 - accuracy: 0.5309\n",
      "Epoch 00027: val_accuracy improved from 0.51850 to 0.52260, saving model to RESNET_SGD_WITH_DROPOUT_REGULAIZATION.hdf5\n",
      "391/390 [==============================] - 363s 929ms/step - loss: 1.7162 - accuracy: 0.5309 - val_loss: 1.7867 - val_accuracy: 0.5226\n",
      "Epoch 28/80\n",
      "391/390 [==============================] - ETA: 0s - loss: 1.6928 - accuracy: 0.5371\n",
      "Epoch 00028: val_accuracy improved from 0.52260 to 0.53590, saving model to RESNET_SGD_WITH_DROPOUT_REGULAIZATION.hdf5\n",
      "391/390 [==============================] - 363s 929ms/step - loss: 1.6928 - accuracy: 0.5371 - val_loss: 1.7374 - val_accuracy: 0.5359\n",
      "Epoch 29/80\n",
      "391/390 [==============================] - ETA: 0s - loss: 1.6569 - accuracy: 0.5431\n",
      "Epoch 00029: val_accuracy did not improve from 0.53590\n",
      "391/390 [==============================] - 361s 923ms/step - loss: 1.6569 - accuracy: 0.5431 - val_loss: 1.7599 - val_accuracy: 0.5294\n",
      "Epoch 30/80\n",
      "391/390 [==============================] - ETA: 0s - loss: 1.6284 - accuracy: 0.5497\n",
      "Epoch 00030: val_accuracy did not improve from 0.53590\n",
      "391/390 [==============================] - 361s 923ms/step - loss: 1.6284 - accuracy: 0.5497 - val_loss: 1.7584 - val_accuracy: 0.5342\n",
      "Epoch 31/80\n",
      "391/390 [==============================] - ETA: 0s - loss: 1.5958 - accuracy: 0.5592\n",
      "Epoch 00031: val_accuracy improved from 0.53590 to 0.54750, saving model to RESNET_SGD_WITH_DROPOUT_REGULAIZATION.hdf5\n",
      "391/390 [==============================] - 363s 927ms/step - loss: 1.5958 - accuracy: 0.5592 - val_loss: 1.7040 - val_accuracy: 0.5475\n",
      "Epoch 32/80\n",
      "391/390 [==============================] - ETA: 0s - loss: 1.5740 - accuracy: 0.5674\n",
      "Epoch 00032: val_accuracy improved from 0.54750 to 0.54960, saving model to RESNET_SGD_WITH_DROPOUT_REGULAIZATION.hdf5\n",
      "391/390 [==============================] - 363s 927ms/step - loss: 1.5740 - accuracy: 0.5674 - val_loss: 1.7074 - val_accuracy: 0.5496\n",
      "Epoch 33/80\n",
      "391/390 [==============================] - ETA: 0s - loss: 1.5341 - accuracy: 0.5718\n",
      "Epoch 00033: val_accuracy improved from 0.54960 to 0.55370, saving model to RESNET_SGD_WITH_DROPOUT_REGULAIZATION.hdf5\n",
      "391/390 [==============================] - 363s 928ms/step - loss: 1.5341 - accuracy: 0.5718 - val_loss: 1.6721 - val_accuracy: 0.5537\n",
      "Epoch 34/80\n",
      "391/390 [==============================] - ETA: 0s - loss: 1.5129 - accuracy: 0.5806\n",
      "Epoch 00034: val_accuracy did not improve from 0.55370\n",
      "391/390 [==============================] - 360s 920ms/step - loss: 1.5129 - accuracy: 0.5806 - val_loss: 1.7284 - val_accuracy: 0.5518\n",
      "Epoch 35/80\n",
      "391/390 [==============================] - ETA: 0s - loss: 1.4771 - accuracy: 0.5877\n",
      "Epoch 00035: val_accuracy did not improve from 0.55370\n",
      "391/390 [==============================] - 360s 922ms/step - loss: 1.4771 - accuracy: 0.5877 - val_loss: 1.7289 - val_accuracy: 0.5523\n",
      "Epoch 36/80\n",
      "391/390 [==============================] - ETA: 0s - loss: 1.4529 - accuracy: 0.5919\n",
      "Epoch 00036: val_accuracy did not improve from 0.55370\n",
      "391/390 [==============================] - 361s 922ms/step - loss: 1.4529 - accuracy: 0.5919 - val_loss: 1.7444 - val_accuracy: 0.5474\n",
      "Epoch 37/80\n",
      "391/390 [==============================] - ETA: 0s - loss: 1.4282 - accuracy: 0.5987\n",
      "Epoch 00037: val_accuracy did not improve from 0.55370\n",
      "391/390 [==============================] - 360s 921ms/step - loss: 1.4282 - accuracy: 0.5987 - val_loss: 1.7018 - val_accuracy: 0.5522\n",
      "Epoch 38/80\n",
      "391/390 [==============================] - ETA: 0s - loss: 1.4075 - accuracy: 0.6048Restoring model weights from the end of the best epoch.\n",
      "\n",
      "Epoch 00038: val_accuracy did not improve from 0.55370\n",
      "391/390 [==============================] - 360s 921ms/step - loss: 1.4075 - accuracy: 0.6048 - val_loss: 1.7309 - val_accuracy: 0.5518\n",
      "Epoch 00038: early stopping\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.keras.callbacks.History at 0x7fb17b51a898>"
      ]
     },
     "metadata": {
      "tags": []
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "model.fit_generator(aug_data.flow(X_train,y_train,batch_size=128),steps_per_epoch=len(X_train)/128,epochs=80,validation_data=(X_test,y_test),verbose=1,callbacks=[call,checkpoint])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "id": "b3QLSvnZ9cVx"
   },
   "outputs": [],
   "source": [
    "y_true = y_test.argmax(-1)\n",
    "y_pred = model.predict(X_test).argmax(-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "id": "69F8Yaqn9spC"
   },
   "outputs": [],
   "source": [
    "from sklearn.metrics import classification_report,accuracy_score,recall_score,precision_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "id": "pfzMVDFj98uf",
    "outputId": "8340f56d-5bb8-4e5a-9289-a6ebf33c0a0a"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.61      0.89      0.72       100\n",
      "           1       0.65      0.71      0.68       100\n",
      "           2       0.49      0.41      0.45       100\n",
      "           3       0.38      0.30      0.34       100\n",
      "           4       0.45      0.45      0.45       100\n",
      "           5       0.56      0.42      0.48       100\n",
      "           6       0.75      0.56      0.64       100\n",
      "           7       0.57      0.65      0.60       100\n",
      "           8       0.58      0.73      0.65       100\n",
      "           9       0.75      0.70      0.73       100\n",
      "          10       0.39      0.36      0.37       100\n",
      "          11       0.45      0.47      0.46       100\n",
      "          12       0.61      0.62      0.62       100\n",
      "          13       0.50      0.56      0.53       100\n",
      "          14       0.53      0.45      0.49       100\n",
      "          15       0.43      0.57      0.49       100\n",
      "          16       0.56      0.53      0.54       100\n",
      "          17       0.65      0.77      0.71       100\n",
      "          18       0.52      0.29      0.37       100\n",
      "          19       0.55      0.47      0.51       100\n",
      "          20       0.66      0.84      0.74       100\n",
      "          21       0.70      0.71      0.70       100\n",
      "          22       0.50      0.55      0.52       100\n",
      "          23       0.78      0.54      0.64       100\n",
      "          24       0.80      0.63      0.70       100\n",
      "          25       0.41      0.43      0.42       100\n",
      "          26       0.35      0.61      0.44       100\n",
      "          27       0.45      0.24      0.31       100\n",
      "          28       0.72      0.76      0.74       100\n",
      "          29       0.53      0.50      0.51       100\n",
      "          30       0.49      0.59      0.53       100\n",
      "          31       0.61      0.56      0.58       100\n",
      "          32       0.66      0.52      0.58       100\n",
      "          33       0.45      0.54      0.49       100\n",
      "          34       0.42      0.52      0.46       100\n",
      "          35       0.46      0.31      0.37       100\n",
      "          36       0.64      0.54      0.58       100\n",
      "          37       0.48      0.64      0.55       100\n",
      "          38       0.45      0.34      0.39       100\n",
      "          39       0.63      0.74      0.68       100\n",
      "          40       0.50      0.45      0.47       100\n",
      "          41       0.82      0.75      0.78       100\n",
      "          42       0.36      0.68      0.47       100\n",
      "          43       0.56      0.51      0.53       100\n",
      "          44       0.30      0.24      0.27       100\n",
      "          45       0.32      0.38      0.35       100\n",
      "          46       0.34      0.50      0.40       100\n",
      "          47       0.54      0.68      0.60       100\n",
      "          48       0.70      0.86      0.77       100\n",
      "          49       0.70      0.66      0.68       100\n",
      "          50       0.26      0.35      0.30       100\n",
      "          51       0.60      0.58      0.59       100\n",
      "          52       0.55      0.68      0.61       100\n",
      "          53       0.80      0.71      0.75       100\n",
      "          54       0.73      0.66      0.69       100\n",
      "          55       0.29      0.11      0.16       100\n",
      "          56       0.68      0.81      0.74       100\n",
      "          57       0.67      0.65      0.66       100\n",
      "          58       0.63      0.64      0.64       100\n",
      "          59       0.61      0.47      0.53       100\n",
      "          60       0.76      0.84      0.80       100\n",
      "          61       0.54      0.67      0.60       100\n",
      "          62       0.72      0.52      0.60       100\n",
      "          63       0.68      0.48      0.56       100\n",
      "          64       0.47      0.34      0.39       100\n",
      "          65       0.41      0.29      0.34       100\n",
      "          66       0.47      0.62      0.53       100\n",
      "          67       0.57      0.33      0.42       100\n",
      "          68       0.85      0.82      0.83       100\n",
      "          69       0.73      0.62      0.67       100\n",
      "          70       0.69      0.55      0.61       100\n",
      "          71       0.70      0.74      0.72       100\n",
      "          72       0.56      0.10      0.17       100\n",
      "          73       0.53      0.30      0.38       100\n",
      "          74       0.26      0.47      0.33       100\n",
      "          75       0.63      0.84      0.72       100\n",
      "          76       0.83      0.74      0.78       100\n",
      "          77       0.56      0.44      0.49       100\n",
      "          78       0.27      0.41      0.33       100\n",
      "          79       0.61      0.59      0.60       100\n",
      "          80       0.44      0.27      0.33       100\n",
      "          81       0.47      0.75      0.58       100\n",
      "          82       0.91      0.77      0.83       100\n",
      "          83       0.68      0.38      0.49       100\n",
      "          84       0.59      0.50      0.54       100\n",
      "          85       0.70      0.62      0.66       100\n",
      "          86       0.55      0.58      0.56       100\n",
      "          87       0.58      0.71      0.64       100\n",
      "          88       0.51      0.83      0.63       100\n",
      "          89       0.50      0.69      0.58       100\n",
      "          90       0.67      0.46      0.54       100\n",
      "          91       0.67      0.71      0.69       100\n",
      "          92       0.54      0.56      0.55       100\n",
      "          93       0.36      0.30      0.33       100\n",
      "          94       0.68      0.88      0.77       100\n",
      "          95       0.70      0.54      0.61       100\n",
      "          96       0.53      0.31      0.39       100\n",
      "          97       0.69      0.54      0.61       100\n",
      "          98       0.32      0.29      0.31       100\n",
      "          99       0.53      0.58      0.55       100\n",
      "\n",
      "    accuracy                           0.55     10000\n",
      "   macro avg       0.56      0.55      0.55     10000\n",
      "weighted avg       0.56      0.55      0.55     10000\n",
      "\n",
      "0.5537\n",
      "0.5649005080835992\n",
      "0.5537\n"
     ]
    }
   ],
   "source": [
    "print(classification_report(Y_test,y_pred))\n",
    "print(accuracy_score(Y_test,y_pred))\n",
    "print(precision_score(Y_test,y_pred,average=\"weighted\"))\n",
    "print(recall_score(Y_test,y_pred,average=\"weighted\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From C:\\Users\\uamit\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\ops\\init_ops.py:1251: calling VarianceScaling.__init__ (from tensorflow.python.ops.init_ops) with dtype is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Call initializer instance with the dtype argument instead of passing it to the constructor\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.78      0.77      0.77       100\n",
      "           1       0.94      0.48      0.64       100\n",
      "           2       0.50      0.52      0.51       100\n",
      "           3       0.44      0.21      0.28       100\n",
      "           4       0.50      0.37      0.43       100\n",
      "           5       0.45      0.55      0.50       100\n",
      "           6       0.75      0.70      0.73       100\n",
      "           7       0.54      0.54      0.54       100\n",
      "           8       0.57      0.80      0.66       100\n",
      "           9       0.79      0.64      0.71       100\n",
      "          10       0.34      0.39      0.36       100\n",
      "          11       0.47      0.47      0.47       100\n",
      "          12       0.64      0.61      0.63       100\n",
      "          13       0.65      0.47      0.55       100\n",
      "          14       0.51      0.55      0.53       100\n",
      "          15       0.44      0.63      0.52       100\n",
      "          16       0.67      0.58      0.62       100\n",
      "          17       0.69      0.79      0.74       100\n",
      "          18       0.68      0.44      0.53       100\n",
      "          19       0.50      0.64      0.56       100\n",
      "          20       0.81      0.80      0.80       100\n",
      "          21       0.80      0.65      0.72       100\n",
      "          22       0.41      0.71      0.52       100\n",
      "          23       0.74      0.72      0.73       100\n",
      "          24       0.70      0.68      0.69       100\n",
      "          25       0.37      0.58      0.45       100\n",
      "          26       0.41      0.59      0.48       100\n",
      "          27       0.40      0.33      0.36       100\n",
      "          28       0.60      0.78      0.68       100\n",
      "          29       0.54      0.53      0.53       100\n",
      "          30       0.45      0.35      0.40       100\n",
      "          31       0.81      0.43      0.56       100\n",
      "          32       0.53      0.52      0.53       100\n",
      "          33       0.60      0.61      0.60       100\n",
      "          34       0.54      0.58      0.56       100\n",
      "          35       0.46      0.25      0.32       100\n",
      "          36       0.68      0.59      0.63       100\n",
      "          37       0.59      0.54      0.56       100\n",
      "          38       0.51      0.28      0.36       100\n",
      "          39       0.45      0.83      0.58       100\n",
      "          40       0.48      0.50      0.49       100\n",
      "          41       0.68      0.80      0.73       100\n",
      "          42       0.42      0.69      0.52       100\n",
      "          43       0.75      0.50      0.60       100\n",
      "          44       0.28      0.26      0.27       100\n",
      "          45       0.48      0.44      0.46       100\n",
      "          46       0.38      0.41      0.39       100\n",
      "          47       0.63      0.60      0.62       100\n",
      "          48       0.64      0.91      0.75       100\n",
      "          49       0.80      0.55      0.65       100\n",
      "          50       0.48      0.30      0.37       100\n",
      "          51       0.55      0.54      0.55       100\n",
      "          52       0.53      0.77      0.63       100\n",
      "          53       0.68      0.89      0.77       100\n",
      "          54       0.65      0.72      0.68       100\n",
      "          55       0.37      0.21      0.27       100\n",
      "          56       0.76      0.77      0.77       100\n",
      "          57       0.51      0.57      0.54       100\n",
      "          58       0.77      0.76      0.76       100\n",
      "          59       0.63      0.38      0.48       100\n",
      "          60       0.81      0.75      0.78       100\n",
      "          61       0.61      0.59      0.60       100\n",
      "          62       0.63      0.61      0.62       100\n",
      "          63       0.72      0.57      0.64       100\n",
      "          64       0.46      0.38      0.42       100\n",
      "          65       0.44      0.22      0.29       100\n",
      "          66       0.63      0.66      0.64       100\n",
      "          67       0.50      0.38      0.43       100\n",
      "          68       0.83      0.88      0.85       100\n",
      "          69       0.74      0.61      0.67       100\n",
      "          70       0.63      0.63      0.63       100\n",
      "          71       0.62      0.73      0.67       100\n",
      "          72       0.33      0.29      0.31       100\n",
      "          73       0.55      0.42      0.48       100\n",
      "          74       0.38      0.42      0.40       100\n",
      "          75       0.83      0.85      0.84       100\n",
      "          76       0.78      0.67      0.72       100\n",
      "          77       0.39      0.44      0.41       100\n",
      "          78       0.30      0.49      0.37       100\n",
      "          79       0.61      0.61      0.61       100\n",
      "          80       0.38      0.36      0.37       100\n",
      "          81       0.48      0.76      0.59       100\n",
      "          82       0.92      0.73      0.82       100\n",
      "          83       0.59      0.46      0.52       100\n",
      "          84       0.48      0.54      0.51       100\n",
      "          85       0.68      0.75      0.71       100\n",
      "          86       0.42      0.67      0.52       100\n",
      "          87       0.68      0.58      0.63       100\n",
      "          88       0.53      0.77      0.63       100\n",
      "          89       0.68      0.68      0.68       100\n",
      "          90       0.67      0.55      0.60       100\n",
      "          91       0.64      0.67      0.65       100\n",
      "          92       0.62      0.42      0.50       100\n",
      "          93       0.48      0.34      0.40       100\n",
      "          94       0.74      0.88      0.80       100\n",
      "          95       0.57      0.64      0.60       100\n",
      "          96       0.60      0.36      0.45       100\n",
      "          97       0.63      0.52      0.57       100\n",
      "          98       0.44      0.44      0.44       100\n",
      "          99       0.50      0.57      0.54       100\n",
      "\n",
      "    accuracy                           0.57     10000\n",
      "   macro avg       0.58      0.57      0.57     10000\n",
      "weighted avg       0.58      0.57      0.57     10000\n",
      "\n",
      "0.5696\n",
      "0.582388291354551\n",
      "0.5696\n"
     ]
    }
   ],
   "source": [
    "#RESNET_SGD_WITH_DROPOUT_REGULAIZATION.hdf5\n",
    "\n",
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras.layers import Conv2D,Dense,MaxPool2D,AveragePooling2D,Flatten,Dropout\n",
    "import tensorflow.keras as keras\n",
    "from tensorflow.keras.datasets import cifar100\n",
    "from tensorflow.keras.utils import to_categorical\n",
    "from tensorflow.keras import layers\n",
    "\n",
    "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
    "from tensorflow.keras.optimizers import SGD\n",
    "\n",
    "from tensorflow.keras.callbacks import EarlyStopping,ModelCheckpoint\n",
    "\n",
    "(X_train,Y_train),(X_test,Y_test)=cifar100.load_data()\n",
    "\n",
    "X_train,X_test=X_train/255,X_test/255\n",
    "y_train,y_test=to_categorical(Y_train),to_categorical(Y_test)\n",
    "\n",
    "shape=(32,32,3)\n",
    "\n",
    "inputs = keras.Input(shape=shape)\n",
    "\n",
    "a=layers.Conv2D(32,(3,3),strides=1,padding=\"same\",activation=\"relu\")(inputs)  #1\n",
    "a=layers.Dropout(0.2)(a)\n",
    "a=layers.Conv2D(32,(3,3),strides=1,padding=\"same\",activation=\"relu\")(a) \n",
    "a=layers.Dropout(0.2)(a)  #2\n",
    "a_temp=a\n",
    "a=layers.Conv2D(32,(3,3),strides=1,padding=\"same\",activation=\"relu\")(a)#\\3\n",
    "a=layers.Dropout(0.2)(a)\n",
    "a=layers.Conv2D(32,(3,3),strides=1,padding=\"same\")(a)#4\n",
    "a=layers.Dropout(0.2)(a)\n",
    "a=layers.Add()([a,a_temp])\n",
    "a=layers.Activation(\"relu\")(a)\n",
    "\n",
    "b=layers.Conv2D(64,(3,3),strides=1,padding=\"same\",activation=\"relu\")(a) #1\n",
    "                               \n",
    "#a_temp=a\n",
    "##########################################################################\n",
    "a=layers.Conv2D(64,(3,3),strides=1,padding=\"same\",activation=\"relu\")(a) #5\n",
    "a=layers.Dropout(0.2)(a)\n",
    "a=layers.Conv2D(64,(3,3),strides=1,padding=\"same\")(a) #6\n",
    "a=layers.Dropout(0.2)(a)\n",
    "a=layers.Add()([b,a])\n",
    "a=layers.Activation(\"relu\")(a)\n",
    "a_temp=a\n",
    "a=layers.Conv2D(64,(3,3),strides=1,padding=\"same\",activation=\"relu\")(a) #7\n",
    "a=layers.Dropout(0.2)(a)\n",
    "a=layers.Conv2D(64,(3,3),strides=1,padding=\"same\")(a) #8\n",
    "a=layers.Dropout(0.2)(a)\n",
    "a=layers.Add()([a,a_temp])\n",
    "a=layers.Activation(\"relu\")(a)\n",
    "\n",
    "b=layers.Conv2D(128,(3,3),strides=1,padding=\"same\",activation=\"relu\")(a) #2\n",
    "\n",
    "###############################################################################\n",
    "\n",
    "a=layers.Conv2D(128,(3,3),strides=1,padding=\"same\",activation=\"relu\")(a) #9\n",
    "a=layers.Dropout(0.2)(a)\n",
    "a=layers.Conv2D(128,(3,3),strides=1,padding=\"same\")(a) #10\n",
    "a=layers.Dropout(0.2)(a)\n",
    "a=layers.Add()([b,a])\n",
    "a=layers.Activation(\"relu\")(a)\n",
    "a_temp=a\n",
    "a=layers.Conv2D(128,(3,3),strides=1,padding=\"same\",activation=\"relu\")(a) #11\n",
    "a=layers.Dropout(0.2)(a)\n",
    "a=layers.Conv2D(128,(3,3),strides=1,padding=\"same\")(a) #12\n",
    "a=layers.Dropout(0.2)(a)\n",
    "a=layers.Add()([a,a_temp])\n",
    "a=layers.Activation(\"relu\")(a)\n",
    "\n",
    "b=layers.Conv2D(256,(3,3),strides=1,padding=\"same\",activation=\"relu\")(a) #3\n",
    "\n",
    "#################################################################################\n",
    "\n",
    "a=layers.Conv2D(256,(3,3),strides=1,padding=\"same\",activation=\"relu\")(a) #13\n",
    "a=layers.Dropout(0.2)(a)\n",
    "a=layers.Conv2D(256,(3,3),strides=1,padding=\"same\")(a) #14\n",
    "a=layers.Dropout(0.2)(a)\n",
    "a=layers.Add()([b,a])\n",
    "a=layers.Activation(\"relu\")(a)\n",
    "a_temp=a\n",
    "a=layers.Conv2D(256,(3,3),strides=1,padding=\"same\",activation=\"relu\")(a) #15\n",
    "a=layers.Dropout(0.2)(a)\n",
    "a=layers.Conv2D(256,(3,3),strides=1,padding=\"same\")(a) #16\n",
    "a=layers.Dropout(0.2)(a)\n",
    "a=layers.Add()([a,a_temp])\n",
    "a=layers.Activation(\"relu\")(a)\n",
    "############################################################3\n",
    "a=layers.AveragePooling2D(pool_size=(2,2),strides=1,padding=\"same\")(a)\n",
    "\n",
    "a=Flatten()(a)\n",
    "\n",
    "#############################################################\n",
    "\n",
    "a=layers.Dense(512,activation=\"relu\")(a)\n",
    "a=layers.Dropout(0.2)(a)\n",
    "outputs=layers.Dense(y_train.shape[1],activation=\"softmax\")(a)\n",
    "\n",
    "\n",
    "model = keras.Model(inputs, outputs)\n",
    "\n",
    "model.load_weights('../weights/RESNET_SGD_WITH_DROPOUT_REGULAIZATION.hdf5')\n",
    "\n",
    "\n",
    "y_true = y_test.argmax(-1)\n",
    "y_pred = model.predict(X_test).argmax(-1)\n",
    "\n",
    "from sklearn.metrics import classification_report,accuracy_score,recall_score,precision_score\n",
    "\n",
    "print(classification_report(Y_test,y_pred))\n",
    "print(accuracy_score(Y_test,y_pred))\n",
    "print(precision_score(Y_test,y_pred,average=\"weighted\"))\n",
    "print(recall_score(Y_test,y_pred,average=\"weighted\"))"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [],
   "name": "RESNET_SGD_WITH_DROPOUT_REGULAIZATION.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
